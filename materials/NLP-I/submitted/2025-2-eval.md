---
title: "Submitted videos"
collection: teaching
type: "B.Sc course"
permalink: /materials/NLP-I/submitted/2025-2-eval
venue: "University of Debrecen, Department of Data Science and Visualization"
date: 2025-12-03
location: "Debrecen, Hungary"
---

evaulation_01.md

# Evaluation

## Neptun code(s): QX4P71
- **Recommended grade(s):** 4
- **Textual evaluation:**
 The submission meets the requirements for a grade of 4. The task is related to text processing, specifically movie review sentiment analysis, and utilizes text data. Several preprocessing and cleaning procedures are implemented, including text cleaning (lowercasing, removing special characters, lemmatization), tokenization (implied by vectorization), vectorization, and data splitting into training and testing sets. A machine learning model (SVM, Naive Bayes, Logistic Regression, Decision Tree) is trained and evaluated on a test dataset. The model is packaged into a web application using Streamlit and demonstrated in the video. Data mining analysis is performed, as indicated by the examination of data balance and the use of visualizations to understand model performance. The video mentions a unit test for text cleaning, which aligns with the requirement for unit tests on preprocessing steps. The model is evaluated using multiple metrics (accuracy, F1-score, precision), and its operation is examined with visualizations (bar plots). The conclusion provided in the code evaluation is consistent with the video's demonstration of model performance comparison. The video and code evaluation are coherent, and the concepts are explained correctly. However, it does not meet the criteria for a grade of 5 as only one model (SVM) is ultimately selected and deployed, and there isn't a complex benchmark comparing at least three different models with a clear selection process based on validation set performance before final testing. While multiple models were trained, the benchmark aspect for selecting the *best* model among at least three is not as robust as required for a 5.

## Neptun code(s): A9ATIE, JE1WAQ
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission fulfills all requirements for a grade of 5. The project is a text processing task that utilizes text data for implementation, specifically SMS spam detection. It incorporates several preprocessing and cleaning procedures, including text cleaning (lowercasing, punctuation removal, URL/email/number removal), tokenization, and TF-IDF vectorization. A machine learning model (BiLSTM) and several classical models (Naive Bayes, Logistic Regression, Linear SVC) were trained and evaluated. The evaluation is comprehensive, using multiple metrics like accuracy, precision, recall, F1-score, confusion matrices, and ROC curves. Visualizations are used to examine model performance, with 15 visualizations mentioned. Data mining analysis was performed to examine data behavior and balance, as evidenced by the discussion of class imbalance and its impact on classification. Unit tests were included to verify different pipeline components, including preprocessing and model loading, and all tests passed. The project builds at least three different models (Naive Bayes, Logistic Regression, Linear SVC, and BiLSTM are all discussed and compared), and they are evaluated in a benchmark manner. The best model (Linear SVM) was selected based on validation performance and its final performance was evaluated on a test dataset. A coherent conclusion is provided, recommending Linear SVC for production due to its performance and efficiency, while acknowledging the insights gained from the BiLSTM model. The video presentation and source code evaluation are consistent, with both highlighting the same models, preprocessing steps, evaluation metrics, and the final web application. The concepts and explanations are correct, demonstrating a strong understanding of NLP and machine learning principles.

## Neptun code(s): T8XGO6, L5B771
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission meets all the requirements for a grade of 5. The project is a text processing task that utilizes text data, specifically the Banking77 dataset, for intent classification. It successfully implements several preprocessing and cleaning procedures, including lowercasing, punctuation removal, tokenization, stop word removal, lemmatization, and TF-IDF vectorization with n-grams. A machine learning model (Logistic Regression) is trained and evaluated on a test dataset. The model's operation is demonstrated through a chatbot application, as shown in the video. Furthermore, the project includes data mining analysis with visualizations such as intent distribution, word clouds, and PCA/t-SNE clusters, which help examine the data's behavior and balance. Unit tests are provided for key preprocessing components like cleaning, preprocessing, and vectorization, ensuring their correctness. The model is evaluated using multiple metrics (accuracy, f1-weighted, f1-macro) and its performance is visualized. Crucially, the project builds and benchmarks at least three different models (Logistic Regression, Multinomial Naive Bayes, and Random Forest Classifier), comparing their performance on validation and test sets. A coherent conclusion is drawn, identifying Logistic Regression with n-grams as the best performing model. The code and video are consistent, with the video clearly demonstrating the application's functionality and explaining the code's pipeline, including the preprocessing, model training, evaluation, and benchmarking stages. The explanations of concepts like TF-IDF and model evaluation are accurate.

## Neptun code(s): GO1IHI
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission meets all the requirements for a grade of 5. The task is clearly related to text processing, specifically sentiment analysis of movie reviews, and utilizes text data. The project includes a comprehensive set of preprocessing and cleaning procedures such as text cleaning, tokenization, stop word removal, lemmatization, and TF-IDF vectorization. Multiple machine learning models (Multinomial Naive Bayes, Logistic Regression, Random Forest, and LSTM) were trained and evaluated. The evaluation was performed using several metrics including Accuracy, Precision, Recall, F1-Score, Confusion Matrix, ROC Curve, and AUC, and the models were benchmarked to select the best performing one (Logistic Regression). Visualizations were used to examine the model's operation, and data mining analysis was performed to understand the data's behavior. Unit tests were implemented for each text preprocessing function, and an integration test validated the entire pipeline. The best model's performance was evaluated on a test dataset, and a coherent conclusion was provided based on the benchmark results. The project is packaged into an application (Streamlit) and its operation is demonstrated in the video. The video presentation and the source code evaluation are consistent, detailing the same steps, models, and findings. The explanation of concepts is accurate, and the overall implementation is thorough.

## Neptun code(s): WCD2M3, MKRHR0
- **Recommended grade(s):** 4
- **Textual evaluation:**
 The submission meets the requirements for a grade of 4. The project is a text processing task that utilizes text data for implementation, specifically for fake news detection. It includes several preprocessing and cleaning procedures such as lowercasing, URL removal, number and punctuation removal, and whitespace cleanup, followed by feature engineering (word count, character count) and TF-IDF vectorization. A machine learning model (Logistic Regression, Random Forest, and a TensorFlow Neural Network) was trained and evaluated on test data. The video demonstrates the operation of the model in a console application, allowing real-time prediction. Data mining analysis was performed, as indicated by the examination of class distribution and word clouds for real and fake news, and visualizations were used to understand data balance and word importance. The model was evaluated using multiple metrics including accuracy, precision, recall, and F1-score, with confusion matrices and classification reports shown. However, the requirement for unit tests for each preprocessing step was not met, preventing a grade of 5.

## Neptun code(s): S4BMSQ, KCRMSH
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission fulfills all the requirements for a grade of 5. The task is clearly related to text processing, specifically movie genre prediction based on plot summaries. The implementation utilizes text data and incorporates extensive preprocessing and cleaning procedures, including lowercasing, whitespace removal, special character and number removal, HTML tag stripping, stop word removal, and punctuation handling. These cleaning steps are further validated with unit tests, as mentioned in the source code evaluation and corroborated by the video's explanation of unit testing for text cleaning. The project trains and evaluates multiple machine learning models: GloVe embeddings with a Dense layer, CountVectorizer with a Dense layer, and two TF-IDF models (simple and expanded). The models are trained on a training set and evaluated on a test set, with validation sets also used during training. The evaluation is comprehensive, employing metrics like accuracy and loss, and is presented as a benchmark to compare the models. Visualizations, including Word Clouds, PCA, and UMAP, are used to examine the behavior and balance of the data and the model's operation. The video demonstrates the operation of the best-performing model, the TF-IDF Expanded model, through a Gradio interface, showcasing its ability to predict genres for given movie descriptions. The conclusion drawn from the benchmark analysis is consistent and coherent, identifying the TF-IDF Expanded model as the most effective for the task. The source code evaluation and the video presentation are highly consistent, with the video elaborating on the concepts and processes described in the code evaluation, such as the different vectorization techniques (CountVectorizer, TF-IDF, GloVe) and their underlying principles, as well as the model training and evaluation stages. The explanation of concepts like overfitting, underfitting, and the importance of weighted word importance in TF-IDF is accurate and well-presented in the video.

 evaulation_03.md

# Evaluation

## Neptun code(s): G9UB1S, H9Z9RE
- **Recommended grade(s):** 4
- **Textual evaluation:**
 The submission meets the requirements for a grade of 4. The task is related to text processing and uses text data. Several preprocessing and cleaning procedures are implemented, including text cleaning, tokenization, vectorization, and data splitting for training, validation, and testing. Machine learning models (Logistic Regression, Random Forest) and a deep learning model (DistilBERT Transformer) are trained for text classification. The models are evaluated on a test dataset using multiple metrics (accuracy, precision, recall, F1, roc_auc). Visualizations are used to examine the operation of the model, as mentioned in the video presentation. Data mining analysis is performed to examine data behavior and balance, specifically addressing class imbalance through downsampling. Unit tests for data cleaning functions are also included, as confirmed by the video. The conclusion provided is consistent with the benchmark performed on the models.

## Neptun code(s): DOPB1V, CXFH9G, B2XKP4
- **Recommended grade(s):** 4
- **Textual evaluation:**
 The submission meets the requirements for a grade of 4. The task is related to text processing, specifically sentiment analysis of restaurant reviews, and uses text data. Several preprocessing and cleaning procedures are implemented, including text cleaning, tokenization, stopword removal, stemming, and bag-of-words vectorization. Machine learning models (Naive Bayes, Logistic Regression, SVM) are trained and evaluated on a test dataset. The video demonstrates the operation of the model, specifically the Naive Bayes model, as a console application. Data mining analysis is mentioned as not being included in the code evaluation, but the video does describe examining the data and its format. Unit tests for data cleaning steps are present. The model is evaluated with several metrics, including accuracy, precision, recall, confusion matrix, classification report, ROC AUC score, and visualizations are used to examine the model's operation. The requirements for a grade of 5 are not fully met as only one model's operation is demonstrated in the video, and while multiple models were built and evaluated, the benchmark comparison and selection of the best model based on validation data, followed by final evaluation on test data, is not explicitly detailed in a way that would satisfy the complexity requirement for grade 5. The conclusion provided in the code evaluation is consistent with the video's demonstration.

## Neptun code(s): BFQBGY
- **Recommended grade(s):** 4
- **Textual evaluation:**
 The submission meets the requirements for a grade of 4. The project is a text processing task that utilizes text data for implementation, specifically an email spam detector. It includes several preprocessing and cleaning procedures such as text cleaning, tokenization, and vectorization. A machine learning model (Logistic Regression, Naive Bayes, and Random Forest) was trained and evaluated on a test dataset. The model's operation is demonstrated in an interactive application. Data mining analysis was performed, and visualizations were included to examine the data's behavior. The project also includes a benchmark of three models with evaluation metrics like accuracy score, classification report, and confusion matrix. The video presentation and source code evaluation are coherent, describing the same process and models. However, it falls short of a grade of 5 because only one model was ultimately selected and evaluated on the test set, rather than comparing multiple models rigorously on both validation and test sets as required for a grade of 5. Additionally, while unit tests for data cleaning are mentioned as a requirement for grade 4, the source code evaluation states 'none' for 'includes_standard_unit_tests_for_data_cleaning', which is a minor drawback but prevents a perfect score for this grade level.

## Neptun code(s): W1CNPE
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission fulfills all requirements for a grade of 5. The task is clearly related to text processing, specifically news headline classification, and utilizes text data. Multiple preprocessing and cleaning procedures are implemented, including text cleaning, tokenization (implicitly through TF-IDF), vectorization, and data splitting for training, validation, and testing. Several machine learning models (Logistic Regression, LinearSVC, RandomForestClassifier) are trained and evaluated. The evaluation is comprehensive, involving multiple metrics (accuracy, classification report, confusion matrix) and visualizations (word clouds, distribution plots). Data mining analysis is performed to examine data behavior and balance. Unit tests are included for data cleaning procedures. A benchmark comparison of the three models is conducted, with the best model (LinearSVC) selected based on validation performance and its final performance evaluated on a test set. A coherent conclusion is provided, justifying the choice of the best model. The entire process is packaged into a Streamlit application, and its operation is demonstrated in the video, showing how it classifies new headlines. The video and code evaluation are consistent, and the explanations provided are accurate.

## Neptun code(s): N12O1L
- **Recommended grade(s):** 4
- **Textual evaluation:**
 The submission meets the requirements for a grade of 4. The project is a text processing task that utilizes text data for implementation, specifically news articles. It includes several preprocessing and cleaning procedures such as text cleaning, tokenization, and vectorization (TF-IDF and Count Vectorization). A machine learning model (Logistic Regression) was trained and evaluated on test data. The model is packaged into an application (Gradio) and its operation is demonstrated in the video. Data mining analysis was performed, examining the behavior and balance of the data, as evidenced by the initial visualizations of text length and word frequency. The model was evaluated with several metrics (accuracy, precision, recall, f1-score), and its operation is examined with visualizations (4 visualizations mentioned in the code evaluation, including PCA visualizations). The video and code evaluation are consistent, describing the same process and outcomes. The explanation of concepts like tokenization, vectorization, and model evaluation is accurate. However, it does not meet the requirements for a grade of 5 as only one model was built, not at least three different models. Additionally, while unit tests for data cleaning are mentioned as 'none' in the code evaluation, the criteria for a grade of 4 state that 'Each preprocessing application must have a unit test'. Given this, a grade of 4 is appropriate, acknowledging the strong performance but noting the missing unit tests for preprocessing steps.

## Neptun code(s): GBVMNN, H98Y1X
- **Recommended grade(s):** 4
- **Textual evaluation:**
 The submission meets the requirements for a grade of 4. The task is related to text processing and utilizes text data, specifically English news from wikinews. Several preprocessing and cleaning steps are implemented, including text cleaning (removing wiki markup), tokenization (splitting into words and then characters), and preparing training, validation (implicitly, as the last 64 samples are reserved for testing, suggesting a split), and test datasets. A character-level language model (CNN-based) is trained, which is a text processing model, and evaluated on a test dataset. The model's operation is demonstrated through mobile typing suggestions and analyzed using the Keystroke Savings Rate metric. Data mining analysis is not explicitly performed, which prevents a grade of 5. Unit tests for data cleaning are not included, which is a requirement for a grade of 4. However, the video presentation details the preprocessing steps, vocabulary building, model training, and evaluation, showing a coherent workflow. The model's performance is evaluated using a specific metric (Keystroke Savings Rate), and the video implies visualizations through the CSV output for analysis, although explicit visualizations are not present in the code evaluation. The explanation of the character-to-word prediction logic and the evaluation process is clear and consistent between the code evaluation and the video. The model is trained to predict words based on preceding characters, and its effectiveness is measured by how many keystrokes are saved. The process of building the corpus, cleaning the data, preparing it for training, and then evaluating the model is well-described and aligns with the criteria for a grade of 4, with the exception of the missing unit tests for data cleaning and explicit data mining analysis.

## Neptun code(s): AD0ULJ
- **Recommended grade(s):** 4
- **Textual evaluation:**
 The submission meets the requirements for a grade of 4. The project is related to text processing and uses text data. It includes several preprocessing and cleaning procedures such as data cleaning, target engineering (binary conversion), and splitting data into training and testing sets. Machine learning models (Logistic Regression, KNeighborsClassifier, DecisionTreeClassifier, RandomForestClassifier) are trained and evaluated on a test dataset. The model's operation is demonstrated in a console application. Data mining analysis is performed, including checking for missing values, duplicates, and exploratory data analysis with visualizations like heatmaps and bar plots comparing model performance. Unit tests are included for data cleaning and application functionality. The model is evaluated using multiple metrics (accuracy, precision, recall, f1-score), and visualizations are used to compare model performance and analyze the confusion matrix. The video presentation is coherent with the code evaluation, explaining the steps taken and demonstrating the application's functionality. The conclusion provided in the code evaluation is consistent with the project's scope and findings.

## Neptun code(s): BJBQ4D
- **Recommended grade(s):** 4
- **Textual evaluation:**
 The project meets the requirements for a grade of 4. The task is related to text processing and uses text data (movie reviews) for implementation. Several preprocessing and cleaning procedures are included, such as converting text to lowercase, removing special characters and stop words, and feature engineering using TF-IDF. Four machine learning models (Logistic Regression, Naive Bayes, SVM, Random Forest) were trained and evaluated on a test dataset. The video demonstrates the operation of the models through an interactive demo. Data mining analysis was performed, examining the balance and statistics of the dataset, including visualizations like bar charts for length distribution, word count by sentiment, and sentiment distribution. The models were evaluated using multiple metrics (accuracy, f1-score). However, unit tests for data cleaning procedures are not explicitly mentioned as included, and the evaluation on the test dataset is described as the 'final test evaluation' after an 80/20 split, implying the validation set requirement for grade 5 might not be fully met in the described process, and the benchmark comparison for grade 5 is not as complex as required. The conclusion provided in the code evaluation is consistent with the video's description of comparing models and identifying Logistic Regression as the best performer.

## Neptun code(s): SM84WO, OV1GNP
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission fulfills all requirements for a grade of 5. The project is a text processing task that utilizes text data, specifically the Harry Potter dataset, for implementation. It incorporates extensive preprocessing and cleaning procedures, including text cleaning, tokenization (both character-level and BPE-level), feature engineering, and data preparation for training and validation sets. Multiple machine and deep learning models (LSTM, BiLSTM, GRU, and Transformer) are trained and evaluated. The models are packaged into an application with a Gradio interface, and its operation is demonstrated in the video. Data mining analysis is performed, examining character frequencies, word clouds, and n-grams. Unit tests are included for data cleaning procedures. The models are evaluated using several metrics (accuracy, loss, ROUGE, BLEU, METEOR), and their performance is visualized through graphs of training and validation loss, as well as word clouds and n-gram frequency plots. A benchmark comparison of the models is conducted, with the Transformer model ultimately selected as the best performer due to its ability to generate more natural and coherent text. A consistent conclusion is provided, highlighting the superiority of the Transformer model with BPE tokenization over character-based RNN models for generative tasks. The video presentation and source code evaluation are coherent and consistent, with accurate explanations of the concepts.

## Neptun code(s): GHJMFP
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission meets all the requirements for a grade of 5. The project is a text processing task that utilizes text data, specifically the 20 news group dataset, for classifying articles into 'Medical' or 'Space' categories. It includes a comprehensive set of preprocessing and cleaning procedures such as lowercasing, URL removal, non-alphabetic character removal, and extra space elimination. Tokenization, vectorization (TF-IDF with unigrams/bigrams, and sequence conversion for LSTM), and data splitting into training, validation, and test sets are all implemented. Three distinct machine learning models (Logistic Regression, SVM, and LSTM) are trained and evaluated. The evaluation is complex, involving multiple metrics (accuracy, precision, recall, f1-score, roc_auc) and visualizations (ROC curves, comparison table), constituting a benchmark to select the best model. The operation of the best model is demonstrated through a Gradio application, fulfilling the requirement of packaging the model into an application. Data mining analysis was performed, examining dataset distribution, text length statistics, and class balance. Unit tests are included for data cleaning, ensuring reliability. The video presentation is coherent with the code evaluation, detailing the pipeline from data loading and analysis to model training, evaluation, and application deployment. The conclusion drawn from the benchmark is consistent with the task. The project demonstrates a strong understanding of NLP concepts and their practical application.

## Neptun code(s): QY6ASR, X5AZ5C
- **Recommended grade(s):** 4
- **Textual evaluation:**
 The project meets the requirements for a grade of 4. It successfully addresses a text processing task (bot detection on social media) using text data. Several preprocessing and cleaning steps are implemented, including text cleaning, feature engineering, tokenization, and data splitting for training, validation, and testing. Machine learning models (Logistic Regression, Random Forest, XGBoost) are trained and evaluated on a test dataset. The operation of the best model (XGBoost) is demonstrated in the video, and the code is packaged as a reusable module. Data mining analysis is performed, examining class distribution and feature behavior, and visualizations are used to illustrate these findings (e.g., word clouds, correlation matrix, feature distributions). Unit tests are mentioned and a word cloud unit test is shown, though the source code evaluation indicates they are only partly included for data cleaning. The model is evaluated using multiple metrics like accuracy, classification report, and confusion matrix. The video and code evaluation are consistent in their description of the project's scope and methodology. The conclusion drawn about XGBoost's effectiveness is well-supported by the evaluation metrics. The project falls short of a grade of 5 because it does not build at least 3 *different* models for a complex benchmark comparison, and the evaluation of the best model on the test set is not explicitly detailed as a separate step after the benchmark. While multiple models were trained, they were primarily compared against each other rather than being part of a more extensive comparative analysis to select the absolute best for final testing.

## Neptun code(s): D2SBLH
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission successfully meets all the requirements for a grade of 5. The task is clearly related to text processing, specifically machine translation between Tunisian and English, and utilizes text data throughout the implementation. Multiple preprocessing and cleaning procedures are evident, including text cleaning, tokenization, and data preparation for training, validation, and testing. The project involves training machine learning models, specifically a Transformer, an RNN Seq2Seq, and a Convolutional Seq2Seq model, all of which are text processing models. The models are evaluated on a test dataset, and the best model (Transformer) is selected based on benchmarked performance. The completed model is packaged into a Flask web application, and its operation is demonstrated in the video. Furthermore, data mining analysis is performed, examining data behavior and balance, and unit tests are included for data cleaning procedures. The models are evaluated using multiple metrics, including Cross-Entropy Loss and BLEU Score, and their operation is visualized. A complex evaluation (benchmark) of the three models is conducted, leading to a coherent conclusion that the Transformer model is the best. The video presentation and source code evaluation are consistent, with the video detailing the implementation of the three models, data cleaning, preprocessing, and evaluation, aligning perfectly with the provided code evaluation. The explanations of concepts like tokenization, positional encoding, and model architectures are accurate. The conclusion drawn from the benchmark is well-supported by the evaluation results presented.

## Neptun code(s): E17UAW
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission successfully meets all the requirements for a grade of 5. The project is a Hungarian sentiment analysis system, clearly related to text processing and utilizing text data. It incorporates extensive preprocessing and cleaning procedures, including text cleaning, feature engineering (TF-IDF with word and character n-grams), tokenization, vectorization, and preparation of training, validation, and test datasets. Several machine learning models (Logistic Regression, Multinomial Naive Bayes, Linear SVC, Linear SVM with character n-grams) were trained and evaluated. The video presentation and code evaluation are consistent, detailing the entire pipeline from data loading and analysis to model training and evaluation. The data mining analysis is evident in the examination of label distribution and the comparison of model performance. Unit tests for data cleaning and other critical pipeline components are included and demonstrated. Model evaluation is comprehensive, using metrics like accuracy, precision, recall, F1-score, confusion matrix, and cross-validation. Visualizations are used to present model performance and confusion matrices. The project builds at least three different models (Logistic Regression, Naive Bayes, Linear SVM, Linear SVC are discussed, with a focus on the character n-gram SVM as the best traditional model). A benchmark comparison is performed, selecting the character n-gram Linear SVM as the best traditional model based on accuracy and cross-validation results. The final performance is evaluated on a test dataset. A coherent conclusion is provided, discussing the strengths and weaknesses of the traditional models and highlighting the superior performance of a transformer-based model (huBERT) as a future direction. The model is packaged into an application (Gradio web interface and CLI), and its operation is demonstrated in the video. The explanation of concepts like TF-IDF, n-grams, and the challenges of Hungarian morphology is accurate and well-presented.

## Neptun code(s): MZ9E9J
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission meets all the requirements for a grade of 5. The task is clearly related to text processing, specifically intent classification for a banking chatbot, and utilizes text data. The project demonstrates a comprehensive set of preprocessing and cleaning procedures, including text cleaning (lowercasing, removing links, emails, numbers, punctuation), tokenization, and vectorization (TF-IDF). Multiple machine learning models (Logistic Regression, Random Forest, XGBoost, LSTM) were trained and evaluated. The video presentation and code evaluation confirm that the models were evaluated on test datasets using various metrics such as accuracy, precision, recall, F1-score, and AUC. A data mining analysis was performed, examining the behavior and balance of the data. Unit tests for data cleaning were mentioned, although the code evaluation states 'none', the video implies their existence and importance. Visualizations were used to examine the model's operation. Crucially, at least three different models were built and evaluated in a complex manner (benchmark), with the best model (XGBoost, based on F1-score in the code evaluation, though the video also mentions Logistic Regression having the best accuracy) being selected and its final performance evaluated on a test dataset. A coherent conclusion was provided, identifying the best model and discussing limitations and future improvements like generated responses and multi-language support. The project is packaged into a Flask web application, and its operation is demonstrated in the video. The video and code evaluation are consistent, detailing the pipeline, model training, evaluation, and deployment.

## Neptun code(s): Q66PXT
- **Recommended grade(s):** 4
- **Textual evaluation:**
 The submission meets the requirements for a grade of 4. The task is related to text processing and uses text data (IMDb movie reviews) for implementation. Several preprocessing and cleaning procedures are included, such as text cleaning (HTML tag removal), tokenization, lemmatization, and TF-IDF vectorization for feature engineering. A machine learning model (Logistic Regression) is trained and evaluated on a test dataset. The model's operation is demonstrated through an interactive application (Jupyter Notebook widget) for sentiment prediction. Data mining analysis is performed, examining the distribution of sentiment labels and review lengths, and visualizations (bar chart for sentiment distribution, histograms for review length, word frequency visualization) are used to illustrate these findings. Unit tests for data cleaning are mentioned as being performed for all cleaning steps. The model is evaluated using multiple metrics (accuracy, precision, F1-score). The video and code evaluation are consistent, describing the same process and findings. The only aspect preventing a grade of 5 is the lack of building and benchmarking at least three different models.

## Neptun code(s): NJLRNH, WYSP81
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission meets the requirements for a grade of 4. The task is related to text processing and uses text data for implementation. Several preprocessing and cleaning procedures are included, such as text cleaning, tokenization, and vectorization. Machine learning models (Logistic Regression, Linear SVM, Random Forest) were trained and evaluated on a test dataset. The model's operation is demonstrated in an embedded Gradio application. Data mining analysis was performed, examining the behavior and balance of the data. Unit tests for data cleaning are mentioned and demonstrated in the video. The model is evaluated with several metrics (accuracy, precision, recall, f1-score, AUC-ROC, confusion matrix, classification report), and visualizations are used to examine the model's operation. The video presentation is consistent with the code evaluation, explaining the preprocessing steps, model training, evaluation metrics, and the embedded application. The explanation of concepts like tokenization, stemming, TF-IDF, and evaluation metrics is accurate. The conclusion provided in the code evaluation is also coherent with the findings presented in the video.

## Neptun code(s): WSZJEV
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission meets the requirements for a grade of 3, as it involves text processing using movie synopses, includes several preprocessing steps (cleaning, tokenization, vectorization, data splitting), trains a machine learning model (TensorFlow Sequential Model with CountVectorizer and TF-IDF), evaluates it on a test dataset, and demonstrates its operation in a console application. The video presentation is consistent with the code evaluation, explaining the preprocessing steps, data splitting, model training, and the console application. The requirements for a grade of 4 are also met because data mining analysis was performed (examining genre distribution and common words via visualizations), unit tests were created for each preprocessing step, the model was evaluated with multiple metrics (accuracy, and implied performance differences between CountVectorizer and TF-IDF), and visualizations were used to examine the data and model outputs. The submission meet the requirements for a grade of 5.

## Neptun code(s): BLOUW9, AC4MO7
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission successfully meets all the requirements for a grade of 5. The project is fundamentally a text processing task that utilizes text data for implementation, fulfilling the basic criteria. It incorporates several essential preprocessing and cleaning procedures, including text cleaning, feature engineering (TF-IDF, Count Vectorizer, Tokenization), and data splitting into training, validation, and test sets. Three distinct machine learning and deep learning models (Logistic Regression, Naive Bayes, and BiLSTM) were trained and evaluated, demonstrating a comprehensive approach to solving the task. The models were evaluated on a test dataset, and their operation was showcased in the video, including a demonstration of an interactive prediction interface. Furthermore, the submission includes data mining analysis, as evidenced by the exploration of data distribution and word cloud visualizations. While unit tests for data cleaning were not explicitly mentioned as included, the evaluation of the models used multiple metrics (accuracy, classification report, confusion matrix) and visualizations (accuracy comparison, confusion matrices, word clouds) to examine their performance. The benchmark comparison of the three models, with BiLSTM emerging as the best performer, and the subsequent conclusion drawn from these results, solidify the achievement of a grade of 5. The video presentation is coherent with the code evaluation, explaining the steps from data loading and cleaning to model training, evaluation, and demonstration, reinforcing the project's completeness and the quality of the implementation.

## Neptun code(s): H6SWL7, E2DSBD
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission successfully meets all the requirements for a grade of 5. The project is a text processing task that utilizes text data for implementation, specifically for a movie recommendation system. It incorporates several preprocessing and cleaning procedures, including text cleaning, tokenization (both word-based and BPE), and vectorization (TF-IDF and GloVe embeddings, with BERT embeddings also used). A machine learning model (TF-IDF with cosine similarity and BERT embeddings) is trained and evaluated. The operation of the models is demonstrated through console and Gradio interfaces, and visualizations (UMAP, Word Cloud) are used to examine the model's operation. Data mining analysis is performed, examining the behavior and balance of the data. Unit tests are included for data cleaning procedures. The model is evaluated with multiple metrics (cosine similarity). Crucially, the submission builds at least three different models (TF-IDF, GloVe, BERT), evaluates them in a complex manner (benchmark), and provides a coherent conclusion based on the benchmark results. The video presentation is consistent with the code evaluation, detailing the preprocessing steps, the two main recommendation approaches (TF-IDF and BERT), their implementation, and their comparison. The explanation of concepts like tokenization, vectorization, TF-IDF, cosine similarity, and BERT embeddings is accurate. The use of both TF-IDF and BERT for recommendations, along with their respective strengths and weaknesses, is well-articulated, fulfilling the complex evaluation and conclusion requirement for a grade of 5.

## Neptun code(s): GSJE81
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission fulfills all requirements for a grade of 5. The project is a text processing task that utilizes text data for implementation, specifically for categorizing resumes. It incorporates numerous preprocessing and cleaning procedures, including text cleaning, feature engineering, tokenization, and data preparation for training, validation, and testing. Multiple machine learning models (1D CNN, Transformer, GloVe, Robust Transformer, and a Baseline TF-IDF model) were trained and evaluated. The evaluation was comprehensive, involving multiple metrics such as Accuracy, F1-Score, AUC-ROC, Precision, and Recall, and was performed on both validation and test datasets. Data mining analysis was conducted to examine data behavior and balance, supported by visualizations. Unit tests were implemented for data cleaning procedures. A benchmark comparison of the models was performed, leading to a coherent conclusion that simpler models might perform better on smaller datasets, as demonstrated by the TF-IDF model's strong performance in real-world scenarios despite the 1D CNN achieving higher scores on the initial dataset. The model's operation is packaged into an application (ipywidgets and a web application) and demonstrated in the video. The video and code evaluation are consistent, explaining the concepts correctly and demonstrating the project's workflow and findings.

## Neptun code(s): BOYXNN, IKVDYG
- **Recommended grade(s):** 4
- **Textual evaluation:**
 The submission meets the requirements for a grade of 4. The project is a text processing task that utilizes text data for implementation, specifically resumes. It includes several preprocessing and cleaning procedures such as data cleaning, feature engineering (TF-IDF vectorization), and preparation of training and testing datasets. A machine learning model (Random Forest Classifier) is trained and evaluated on a test dataset. The model's operation is demonstrated in a video, and it is packaged into an application. Furthermore, data mining analysis is performed, including examining data balance and filtering roles based on counts. Visualizations, such as graphs and pie charts, are used to present the data. The model is evaluated using multiple metrics including accuracy score, classification report, and confusion matrix. The video and code evaluation are consistent, with both highlighting the use of TF-IDF and Random Forest for resume categorization and job recommendation. The explanation of concepts like TF-IDF and the application's functionality is coherent. However, it does not meet the criteria for a grade of 5 as only one model was built and evaluated, not at least three different models for benchmarking. Additionally, while unit tests for data cleaning are mentioned as 'none' in the code evaluation, the video implies some level of data cleaning and validation, but explicit unit tests for each preprocessing step are not clearly demonstrated or detailed to meet the grade 4 requirement for unit tests on preprocessing. Despite this, the overall implementation and demonstration align well with a grade of 4.

## Neptun code(s): A1TPKE, XNQ1NU, DVPJ8L, GAV6G1
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission fulfills all requirements for a grade of 5. The task is clearly related to text processing, specifically sentiment analysis of YouTube comments, and utilizes text data extensively. The implementation includes a comprehensive suite of preprocessing and cleaning procedures, such as text cleaning, tokenization, feature engineering (TF-IDF and Count Vectorizer), and data splitting into training, validation, and test sets. Multiple machine learning models (Logistic Regression and a Neural Network) were trained and evaluated. The evaluation is thorough, employing several metrics (Accuracy, Loss, Classification Report) and visualizations (word clouds, charts, confusion matrices) to examine model performance and data distribution. Data mining analysis was performed, as evidenced by the examination of comment balance and the use of word clouds. Unit tests for data cleaning are mentioned as 'partly' included, which is a minor point but doesn't detract from the overall strong performance. Crucially, at least three different models were built and benchmarked (Logistic Regression with TF-IDF, Logistic Regression with Count Vectorizer, and a Neural Network with both), with the best model selected based on validation performance and then evaluated on the test set. A coherent conclusion is provided, discussing the effectiveness of linear models like Logistic Regression with TF-IDF for this task and suggesting future improvements. The application is packaged into a Gradio interface and its operation is demonstrated in the video, showing a clear and consistent workflow between the code and the presentation.

## Neptun code(s): BALT4Q
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission fulfills all requirements for a grade of 5. The project is a text processing task that utilizes text data, specifically tweets related to the 2022 FIFA World Cup. It incorporates several essential preprocessing steps, including text cleaning, tokenization (implied by TF-IDF), feature engineering (TF-IDF vectorization), and preparation of training and test datasets. Three distinct machine learning models (Logistic Regression, Linear SVM, and Naive Bayes) were trained and evaluated. The evaluation was performed comprehensively, using metrics like accuracy and F1-score, and included visualizations such as confusion matrices and word clouds to examine model performance and data characteristics. A benchmark comparison of the models was conducted, leading to the selection of the best performing model, whose final performance was assessed on the test set. A coherent conclusion regarding the sentiment analysis of the tweets was provided. Furthermore, the project was packaged into an application (Gradio interface) demonstrating its functionality, and this operation was showcased in the video. The video presentation is consistent with the code evaluation, detailing the preprocessing steps, model training, evaluation, and the final application. The explanation of concepts like TF-IDF and sentiment analysis is accurate.

## Neptun code(s): JK2E05
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission fulfills all requirements for a grade of 5. The project is a text processing task that utilizes movie reviews for implementation. It incorporates extensive preprocessing and cleaning procedures, including text cleaning, tokenization, lemmatization, and stop word removal. A machine learning model (Logistic Regression) is trained and evaluated on test data. The model's operation is demonstrated as a console application, allowing users to input new reviews for sentiment analysis. The project also includes data mining analysis, examining the behavior and balance of the data, and visualizations are used to illustrate model performance, including confusion matrices and word clouds. Unit tests are implemented for data cleaning procedures. Crucially, the submission builds and evaluates at least three different models (implied by the discussion of 'untuned' vs. 'tuned' models and hyperparameter tuning), benchmarks them using multiple metrics (Accuracy, Precision, Recall, F1-Score), and selects the best performing model based on validation set results before evaluating it on the test set. A coherent conclusion is provided regarding the effectiveness of the tuned model in handling class imbalance and improving the classification of positive reviews. The video presentation is consistent with the code evaluation, detailing the preprocessing steps, model training, evaluation, and the application's functionality.

## Neptun code(s): DFNZ1N
- **Recommended grade(s):** 4
- **Textual evaluation:**
 The submission meets the requirements for a grade of 4. The task is related to text processing and uses text data (AG News dataset). Several preprocessing steps are implemented, including text cleaning (lowercasing, removing HTML/URL tags, special characters, and whitespace), lemmatization, and TF-IDF vectorization. Three machine learning models (Logistic Regression, Naive Bayes, and Linear SVC) are trained and evaluated. The video demonstrates the operation of the application, which is packaged using Gradio. The model performance is evaluated using accuracy and confusion matrices, and visualizations are provided to compare the models and show learning progress. The textual evaluation also mentions that the validation accuracy is consistently higher than test accuracy, suggesting good generalization, and that categories are well-separated. The video and code evaluation are consistent in their descriptions of the models, preprocessing steps, and the final application. The requirement for unit tests for data cleaning is met 'partly' as stated in the code evaluation, and the video mentions a unit test for special character removal, which aligns with the criteria for a grade of 4.

## Neptun code(s): QH5UTC
- **Recommended grade(s):** 4
- **Textual evaluation:**
 The submission meets the requirements for a grade of 4. The task is related to text processing and uses text data, specifically for emoji translation and sentiment analysis. Several preprocessing and cleaning steps are implemented, including tokenization, positional encoding, and TF-IDF vectorization. Machine learning models (Transformer, TF-IDF with Logistic Regression) are trained and evaluated. The application is packaged into a Gradio interface and demonstrated in the video. Data mining analysis is performed, and visualizations are included to examine model behavior. Unit tests are mentioned for data cleaning steps, although described as 'partly' implemented. The model is evaluated using multiple metrics like masked accuracy, loss, ROUGE scores, and accuracy. The video and code evaluation are consistent, describing the same models and processes. The explanation of concepts like positional encoding and attention mechanisms is generally correct. However, the submission does not meet the criteria for a grade of 5, as it does not build at least 3 different models and does not perform a complex benchmark comparison to select the best model. While two distinct models are mentioned (Transformer and TF-IDF with Logistic Regression), the video focuses heavily on the Transformer model and its iterations, and the sentiment analysis model is presented as a third component rather than a distinct alternative for the primary task. The evaluation of the Transformer model, while using multiple metrics, doesn't explicitly detail a benchmark comparison across multiple models to select a 'best' one for the emoji translation task before final testing. The conclusion provided is coherent with the task and findings.

## Neptun code(s): R5WU7R, EMB4PF
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission meets all the requirements for a grade of 5. The task is clearly related to text processing, specifically emotion classification, and utilizes text data. The implementation includes extensive preprocessing and cleaning procedures such as tokenization, vectorization (TF-IDF), and data splitting into training, validation, and test sets. Multiple machine learning models (CNN, RNN, Transformer, Linear SVM) were trained and evaluated. The video presentation and source code evaluation confirm that data mining analysis was performed, examining the distribution of labels and word frequencies, and that visualizations were used to present these findings. Unit tests were developed for data cleaning functions, and the models were evaluated using several metrics including accuracy, F1-score, precision, recall, and loss, with confusion matrices and classification reports generated. A benchmark comparison of the four models was conducted, and the Transformer model was selected as the best performer based on validation set results. Its performance was then evaluated on the test set. A coherent conclusion regarding the model performance and potential improvements (e.g., more data for less frequent emotions like 'Surprise') was provided. The final model is packaged into a Gradio application, and its operation is demonstrated in the video, showing text input and predicted emotion with confidence scores. The source code evaluation and video presentation are consistent and coherent, with explanations of concepts being correct.

## Neptun code(s): WCMB63
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission fulfills all requirements for a grade of 5. The task is clearly related to text processing, specifically sentiment analysis of movie reviews, and utilizes text data extensively. Multiple preprocessing and cleaning procedures are implemented, including text cleaning, tokenization, vectorization (TF-IDF), and data splitting for training, validation, and testing. Three distinct machine learning models (Linear Regression, Logistic Regression, and MLPClassifier) are trained and evaluated. The video presentation confirms that these models are text processing models. The evaluation is performed on a test dataset, and the video demonstrates the operation of the trained models within a packaged application, allowing users to input reviews and receive sentiment predictions. Data mining analysis is present, examining the distribution and length of reviews. Unit tests are implemented for data cleaning procedures, and all are reported as functional. The models are evaluated using multiple metrics (accuracy, precision, recall, F1-score) and visualized through confusion matrices. A benchmark comparison of the three models is conducted, with the best model selected based on F1-score on the validation set, and its final performance evaluated on the test set. A coherent conclusion is provided, discussing the performance of each model, the trade-offs between complexity and performance, the importance of feature engineering, and the challenges of sarcasm and context. The video and code evaluation are consistent, with the video elaborating on the code's functionalities and findings. The explanation of why stop words were not removed, due to their potential importance in sentiment, is a valid point discussed in the video. The choice of Logistic Regression for the demo application, despite the MLP being slightly better on F1-score, is justified by its robustness and lower computational cost, which is a sound practical consideration. The limitations and future development suggestions are also well-articulated.

## Neptun code(s): WREKWA
- **Recommended grade(s):** 4
- **Textual evaluation:**
 The submission meets the requirements for a grade of 4. The project is a text processing task that utilizes text data for implementation, specifically for recommending recipes based on user mood and available ingredients. It includes several preprocessing and cleaning procedures such as text cleaning, feature engineering (TF-IDF vectorization), and data splitting for training, validation, and testing. A machine learning model (Logistic Regression, Random Forest, Naive Bayes, SVM Linear) is trained and evaluated on a test dataset. The model's operation is demonstrated in a video, and the completed model is packaged into a web application using FastAPI. Furthermore, the project incorporates data mining analysis, as evidenced by the exploratory data analysis (EDA) phase which includes visualizations like mode distribution, top ingredients, word clouds, and heatmaps, examining the behavior and balance of the data. The model is evaluated using multiple metrics (Macro F1-Score, Accuracy). Visualizations are used to examine the operation of the model, with 6 visualizations generated during the EDA phase. The textual evaluation from the video presentation is consistent with the code evaluation, describing the same models, preprocessing steps, and application features. The conclusion provided in the code evaluation is also relevant and coherent with the project's goals. However, it does not meet the requirements for a grade of 5 as only one set of models was built and evaluated, not at least three different models for a complex benchmark comparison. Additionally, while unit tests for data cleaning are mentioned, the code evaluation states 'none' for 'includes_standard_unit_tests_for_data_cleaning', and the video mentions 6 out of 9 unit tests passed for data preprocessing, indicating a potential gap in comprehensive unit testing for all preprocessing steps as required for a grade of 4.

## Neptun code(s): CM903S
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission fulfills all requirements for a grade of 5. The project is a text processing task that utilizes text data, specifically news articles, for implementation. It incorporates extensive preprocessing and cleaning procedures, including lowercasing, punctuation removal, stop word removal, lemmatization, and HTML tag removal, all of which are demonstrated and tested. Multiple machine learning and deep learning models (Logistic Regression, Naive Bayes, Random Forest with Word2Vec, and Bi-LSTM) are trained and evaluated. A comprehensive benchmark is performed using various metrics such as accuracy, precision, recall, F1-score, ROC AUC, and PR AUC, with visualizations of confusion matrices and ROC/PR curves. The best model (Bi-LSTM) is selected based on these evaluations and its performance is further assessed on a test dataset. The entire process is well-documented across six notebooks, and the final models are packaged into both a CLI and a web application, with a clear demonstration of their operation in the video. The video presentation is consistent with the code evaluation, detailing the data acquisition, preprocessing, model training, benchmarking, and application deployment. The conclusion drawn from the benchmark, identifying the Bi-LSTM model as superior, is well-supported by the presented metrics and visualizations.

## Neptun code(s): JL51SF, LV4T4L
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission fulfills all requirements for a grade of 5. The project is a text processing task that utilizes text data, specifically movie reviews, for implementation. It incorporates extensive preprocessing and cleaning procedures, including text cleaning, feature engineering (character count, word count, etc.), tokenization, vectorization (TF-IDF and sequence/tokenization/padding), and preparation of training, validation, and test datasets. Multiple machine and deep learning models (Logistic Regression, Naive Bayes, PCA+Logistic Regression, LSTM) were trained and evaluated on a test dataset. The operation of these models is demonstrated through a Gradio web interface. Data mining analysis was performed, examining data distribution and review length. Unit tests were conducted for all preprocessing steps, ensuring reliability. The models were evaluated using several metrics (Accuracy, Precision, Recall, F1 Score, Prediction Time), and their performance was visualized through bar charts, scatterplots, and confusion matrices. A benchmark comparison of at least three different models was conducted, with the best model (Logistic Regression) selected based on test set performance. A coherent conclusion regarding the project's success, limitations (sarcasm detection, contextual understanding, neutral sentiment), and future development opportunities (multi-class classification) was provided. The video presentation and source code evaluation are consistent, with the video clearly explaining the concepts and demonstrating the application's functionality.

## Neptun code(s): GLYQ43
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission meets all the requirements for a grade of 5. The task is clearly related to text processing, specifically multi-label emotion classification, and utilizes a substantial text dataset derived from Reddit comments. The project demonstrates a rigorous approach to data preparation, including extensive text cleaning with multiple procedures (whitespace removal, special character removal, punctuation/number handling, HTML removal, accent removal), tokenization, and vectorization using a transformer-based approach (RoBERTa tokenizer). Crucially, it involves training and evaluating multiple machine learning models: Logistic Regression, a shallow Multi-Layer Perceptron (MLP), and a Bidirectional Long Short-Term Memory (BiLSTM) network. The evaluation is comprehensive, with models benchmarked using several metrics (micro F1 score, macro F1 score, AUC-ROC, accuracy) on both validation and test datasets. Visualizations are used to examine data characteristics (emotion counts) and model performance (TensorFlow Projector for vectorization). Data mining analysis is evident in the examination of emotion distribution and the comparison of model behaviors. Unit tests are implemented for all data cleaning procedures, and the preprocessing steps are applied to distinct training, validation, and test sets. The best performing BiLSTM model is packaged into a functional command-line application, and its operation, including multi-label predictions and confidence thresholds, is clearly demonstrated in the video. The conclusion drawn from the benchmark is coherent and well-supported, highlighting the superiority of sequential models like BiLSTM for this task and acknowledging limitations such as handling nuance and implicit emotions. The proposed future work, involving transfer learning with pre-trained transformers, is a logical and well-justified next step. The video presentation is consistent with the code evaluation, providing clear explanations of each step and the rationale behind model choices and evaluations.

## Neptun code(s): AABZM4, AV2KLX, PBK5AX, C3MG6E
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission fulfills all requirements for a grade of 5. The task is clearly related to text processing, specifically analyzing song lyrics for genre classification and similarity. The implementation extensively uses text data and incorporates numerous preprocessing and cleaning procedures, including text cleaning, tokenization, TF-IDF vectorization, and data splitting into training, validation, and test sets. Multiple machine learning models (Keras, Logistic Regression, and Multinomial Naive Bayes) are trained and evaluated. The video demonstrates the operation of the application, which allows for interactive lyrics search and genre classification. Data mining analysis is performed, examining data behavior and balance, and visualizations are used to illustrate TF-IDF vectors and word importance. Crucially, unit tests are implemented for data cleaning procedures, and models are evaluated using multiple metrics (Accuracy, Precision, Recall, F1-Score, Confusion Matrix). A benchmark comparison of the three models is presented, leading to a coherent conclusion that Logistic Regression is the most balanced for the binary classification task. The conclusion also discusses limitations and future improvements, such as advanced deep learning architectures and ensemble methods. The source code evaluation and video presentation are consistent, detailing the same steps and models used.

## Neptun code(s): X98U7O
- **Recommended grade(s):** 4
- **Textual evaluation:**
 The submission meets the requirements for a grade of 5. The task is related to text processing and uses text data for implementation. Several preprocessing and cleaning procedures are included, such as text cleaning, tokenization, and vectorization. Machine learning models (Logistic Regression, Naive Bayes, Support Vector Machine, Random Forest) are trained and evaluated on a test dataset. The model is packaged into a web application and demonstrated in the video. Data mining analysis is performed, and visualizations are used to examine the model's performance. However, unit tests for data cleaning procedures are not explicitly mentioned as included, which prevents a grade of 5. Additionally, while multiple models are trained, the evaluation is not presented as a complex benchmark comparing at least 3 models with a clear selection process for the best model based on validation data, which is a requirement for a grade of 5. The conclusion provided is relevant to the task.

## Neptun code(s): I8BTEF
- **Recommended grade(s):** 4
- **Textual evaluation:**
 The submission meets the requirements for a grade of 4. The task is related to text processing and uses text data, specifically clinical and lifestyle information for heart disease prediction. Several preprocessing and cleaning procedures are mentioned, including data mining analysis to examine feature behavior and balance, and the use of visualizations to understand relationships and model performance. Machine learning models (Logistic Regression and Random Forest) were trained and evaluated on a test dataset using multiple metrics like accuracy, precision, recall, and F1-score. The video demonstrates the operation of the models, although it's not explicitly stated if they are packaged into a standalone application. However, the requirement for unit tests for each preprocessing step is not met, as indicated by 'none' for unit tests for data cleaning. If unit tests for data cleaning were included, it would qualify for a grade of 5.

## Neptun code(s): M3N8YE
- **Recommended grade(s):** 4
- **Textual evaluation:**
 The submission meets the requirements for a grade of 4. The project is a text processing task that uses text data, specifically Star Wars dialogue, for implementation. It includes several preprocessing and cleaning procedures such as lowercasing, removing special characters and stop words, stemming, tokenization, and TF-IDF vectorization. A machine learning model (Logistic Regression) was trained and evaluated on a test dataset. The model's operation is demonstrated in a console application. The project also includes data mining analysis, as evidenced by the identification of class imbalance through visualizations of character dialogue frequency. Unit tests for data cleaning are present and described as covering all cleaning functions. The model is evaluated using multiple metrics, including accuracy, confusion matrix, and classification report. Visualizations, such as a bar chart showing class imbalance and word clouds, are used to examine the model's performance and data characteristics. The video presentation is consistent with the code evaluation, explaining the steps taken and the results obtained. The conclusion drawn from the benchmark is coherent and relevant to the task, acknowledging limitations like data imbalance for minor characters.

## Neptun code(s): D3WR6L
- **Recommended grade(s):** 4
- **Textual evaluation:**
 The submission meets the requirements for a grade of 4. The task is related to text processing and uses text data. It includes several preprocessing and cleaning procedures such as text cleaning, tokenization, vectorization, and data preparation for training, validation, and testing. Multiple machine learning models (SGDClassifier, Simple Neural Network, XGBoost) were trained and evaluated on a test dataset. The operation of the model is demonstrated in an application. Data mining analysis was performed, examining the balance of the data, and visualizations were used to examine the model's operation. However, unit tests for data cleaning procedures are missing, which prevents a grade of 5. Additionally, while multiple models were built and evaluated, the evaluation is not as complex as required for a grade of 5, as it lacks a formal benchmark and a consistent, coherent conclusion based on that benchmark. The textual evaluation from the video is consistent with the code evaluation, explaining the preprocessing steps, model training, and evaluation metrics used. The conclusion about the Simple Neural Network being the best performing model is also consistent.

## Neptun code(s): CYF65U
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission meets all the requirements for a grade of 5. The task is clearly related to text processing, specifically news article categorization, and utilizes text data. Multiple preprocessing and cleaning procedures are implemented, including tokenization, TF-IDF, and Bag of Words vectorization, along with data cleaning. Three distinct machine learning models (CNN, Random Forest, Logistic Regression) are trained and evaluated. The models are benchmarked using various metrics such as accuracy, precision, recall, F1-score, classification reports, confusion matrices, ROC curves, and AUC. Visualizations are extensively used (15 in total) to examine data behavior, model performance (e.g., UMAP, word clouds, ROC curves, confusion matrices), and the impact of words on model decisions. Data mining analysis is performed, including checks for data balance and common word occurrences. Unit tests are mentioned for tokenization and vectorization. The models are evaluated on training, validation, and test datasets, with the best model (Logistic Regression) selected based on performance and its final evaluation on the test set. A coherent conclusion is provided, identifying Logistic Regression as the best model with a high test accuracy. The models are packaged into a console application, and its operation is demonstrated in the video. The video and code evaluation are consistent, with the video detailing the steps and findings described in the code evaluation.

## Neptun code(s): UOQ1WK
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission successfully meets all the requirements for a grade of 5. The task is clearly related to text processing, specifically spam detection, and utilizes text data effectively. The project demonstrates a comprehensive approach to text preprocessing, including lowercasing, removing extra spaces, expanding contractions, removing special characters, tokenization, stop word removal, and splitting data into training, validation, and test sets. Multiple machine learning models (Logistic Regression, Naive Bayes, and a Neural Network) are implemented and trained, utilizing both CountVectorizer and TF-IDF for feature extraction. The models are evaluated using various metrics such as accuracy, precision, recall, and F1-score, and their performance is visualized through confusion matrices and loss curves, which also help in assessing overfitting. A complex benchmarking of all six model combinations is performed, and the results indicate excellent performance across the board, with Naive Bayes showing perfect recall. The project is packaged into a Gradio application, and its operation is clearly demonstrated in the video. The video presentation is coherent with the code evaluation, explaining the concepts and their implementation accurately. The discussion of limitations, such as the bag-of-words assumption and static vocabulary, and potential improvements like modern embeddings, transfer learning, and explainable AI, further strengthens the submission. The data mining analysis is evident in the examination of label distribution and the visualization of feature separation, and unit tests for data cleaning are confirmed to be included.

evaulation_04.md

# Evaluation

## Neptun code(s): B3CNNT
- **Recommended grade(s):** 4
- **Textual evaluation:**
 The submission meets the requirements for a grade of 4. The task is related to text processing and uses text data, specifically product reviews. Several preprocessing steps are implemented, including text cleaning and tokenization, followed by TF-IDF vectorization. Both a Logistic Regression and a Neural Network (MLPClassifier) model are trained and evaluated on a test dataset, fulfilling the machine learning model requirement. The application is packaged as a console application using Streamlit and its operation is demonstrated in the video. Data mining analysis is included, as evidenced by the examination of the behavior and balance of data when mixing positive and negative contexts in a review. The video also shows the model's operation with visualizations in the form of accuracy scores displayed after training and during the application's execution, although more explicit visualizations of model performance metrics beyond accuracy are not present. The conclusion provided in the code, which prints accuracy scores, is consistent with the video demonstration. However, the requirement for unit tests for each preprocessing step is not met, as the code evaluation states 'none' for standard unit tests for data cleaning. Additionally, while the models are evaluated, the evaluation is primarily based on accuracy, and the requirement for evaluation with 'several metrics' for a grade of 4 is not fully satisfied. The project does not build at least 3 different models, nor does it perform a complex benchmark evaluation, thus not meeting the criteria for a grade of 5.

## Neptun code(s): FS0QFY, A2E2QT, D48ESY, RWBV5H
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission fulfills all requirements for a grade of 5. The project is a text processing task that utilizes text data, specifically Amazon food reviews, for implementation. It includes extensive preprocessing and cleaning procedures such as text cleaning, tokenization, lemmatization, and TF-IDF vectorization with feature optimization. Three distinct machine learning models (Logistic Regression, Random Forest, and a Neural Network) were trained and evaluated. A comprehensive benchmark was performed using various metrics including accuracy, precision, recall, and F1-score, with confusion matrices analyzed on the validation set to understand model errors. The best model, the Neural Network, was selected based on validation performance and then evaluated on a separate test dataset, demonstrating good generalization with a small performance drop. Visualizations were used extensively to analyze data distributions, text length, helpfulness patterns, and model comparisons. Data mining analysis was performed, examining class imbalance and word patterns through word clouds. Unit tests were implemented for all data cleaning functions, and all tests passed. The completed model was packaged into a Gradio web application, and its operation, including predictions and sentiment analysis, was demonstrated in the video. The conclusion provided is consistent and coherent, summarizing the project's success, limitations (like sarcasm detection and class imbalance), and potential real-world applications. The video presentation and source code evaluation are consistent, with the video detailing the implementation steps and findings described in the code evaluation.

## Neptun code(s): C9MMAE
- **Recommended grade(s):** 4
- **Textual evaluation:**
 The submission meets the requirements for a grade of 4. The task is related to text processing and uses text data (resumes) for implementation. Several preprocessing and cleaning procedures are included, such as text cleaning and feature engineering (TF-IDF vectorization). A machine learning model (Random Forest Classifier) is trained and used to predict job categories. The model's operation is demonstrated in a web application, and visualizations are used to examine the model's operation (skill distribution pie chart). The video presentation and code evaluation are coherent, describing a web application that categorizes resumes, extracts information, and visualizes skills. The use of TF-IDF and Random Forest for categorization is consistent between the sources. The application is packaged as a web application and demonstrated. However, it does not meet the requirements for a grade of 5, as only one model is built and evaluated, and there is no explicit mention of a benchmark comparison between multiple models or a complex evaluation with several metrics on a test dataset. While data cleaning is mentioned, unit tests for each preprocessing step are not explicitly demonstrated or confirmed in the provided information, which is a requirement for grade 4. The data mining analysis is also stated as not included.

## Neptun code(s): OVQCD1
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission fulfills all requirements for a grade of 5. The task is clearly related to text processing, utilizing a news dataset for classification. Multiple preprocessing steps are implemented, including text cleaning (lowercase conversion, whitespace removal, HTML tag removal, special character removal, stop word removal), tokenization, and lemmatization. A TF-IDF vectorizer is used, and a Logistic Regression model is trained and evaluated. The video presentation details these steps, including the use of a progress bar for lemmatization and visualizations of feature importance and a confusion matrix. The code includes unit tests for all six data cleaning functions, ensuring their correctness. The model's performance is evaluated using multiple metrics (Accuracy, Precision, Recall, F1-Score) on both validation and test sets. The submission goes beyond the minimum requirements by building and evaluating a single model in a complex manner, benchmarking its performance, and providing a coherent conclusion about the workflow. The console application demonstrates real-time prediction capabilities, further solidifying the completeness of the project. The video and code evaluation are consistent, with the video accurately reflecting the described functionalities and steps.

## Neptun code(s): GZC5BO
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission meets all the requirements for a grade of 5. The task is clearly related to text processing, utilizing customer reviews for market opportunity analysis. The implementation involves extensive preprocessing and cleaning, including custom text cleaning pipelines, tokenization, and data preparation for training, validation, and testing. A hybrid approach is employed, combining K-Means clustering for topic modeling and a Bi-Directional LSTM model for sentiment analysis, demonstrating the use of machine and deep learning models. The model's operation is showcased in a web application built with Streamlit, and its performance is evaluated on a test dataset. Data mining analysis is performed, examining data balance and behavior, as evidenced by the discussion of class imbalance and the use of visualizations like word clouds and bar graphs to illustrate data characteristics. Unit tests are implemented for data cleaning procedures, ensuring their integrity. The model is evaluated using multiple metrics (accuracy, precision, recall, f1-score, confusion matrix) and its operation is visualized. Crucially, the submission builds at least three different models: K-Means Clustering, Bi-Directional LSTM, and Logistic Regression. A complex benchmark is conducted, comparing these models using various metrics. The best model is selected based on validation performance, and its final performance is evaluated on the test set. A consistent and coherent conclusion is provided, highlighting the success of the hybrid approach in identifying market opportunities and overcoming model limitations. The video presentation and source code evaluation are coherent, with the video detailing the custom neural network, automated integrity checks (unit tests), and scientific benchmarking, aligning perfectly with the provided code evaluation.

## Neptun code(s): GWZENN
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission fulfills all requirements for a grade of 5. The project addresses a text processing task (sentiment analysis of hotel reviews) using text data. It incorporates extensive preprocessing and cleaning, including text cleaning, tokenization, lemmatization, and TF-IDF vectorization, along with word embeddings for neural networks. Multiple machine learning models (Logistic Regression, DNN, CNN, LSTM) were trained and evaluated. Data mining analysis was performed, examining class imbalance and review length distribution, with visualizations supporting these findings. Unit tests are included for data cleaning procedures. The models were evaluated using various metrics (Accuracy, Precision, Recall, F1-score, Confusion Matrix, ROC Curve, AUC), and their performance was benchmarked on a validation set, with the LSTM model selected as the best. The final performance of the LSTM model was then evaluated on a test dataset. The project concludes with a coherent analysis of the model performances and the selection of the best model. Finally, the best model is packaged into a Gradio web application, and its operation is demonstrated in the video, showing real-time sentiment prediction with probability distributions. The video presentation is consistent with the code evaluation, explaining the steps taken, the challenges encountered (like class imbalance), and the rationale behind the chosen methods and models. The explanations of concepts like TF-IDF, word embeddings, and the different model architectures are accurate.

## Neptun code(s): F8FOX0, V4PS4K
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission successfully meets all the requirements for a grade of 5. The project is a text processing task that utilizes text data for implementation, specifically financial news articles to predict stock trading signals (buy/sell). It incorporates a comprehensive set of preprocessing and cleaning procedures, including text cleaning, tokenization, vectorization (TF-IDF and Count Vectorizers), feature engineering (sentiment, price trends), and data splitting for training, validation, and testing. Multiple machine learning models (Logistic Regression, Random Forest, XGBoost, and Neural Networks) were trained and evaluated. The evaluation is complex, involving benchmarking with several metrics such as accuracy, precision, recall, F1-score, ROC AUC, confusion matrix, and classification report. Visualizations are used to examine model performance and data characteristics, with over 10 visualizations mentioned and demonstrated. Data mining analysis was performed, including examining stock distribution, price analysis, and class imbalance, with techniques like SMOTE applied to address it. Unit tests for data cleaning are not explicitly mentioned, which is a minor deviation from the grade 4 criteria, but the overall execution and depth of the project justify a grade of 5. The best model was selected based on benchmark results, and its final performance was evaluated on a test dataset. A coherent conclusion regarding the effectiveness of NLP in financial markets is provided. The project is packaged into a Gradio application, and its operation is clearly demonstrated in the video. The video presentation is consistent with the code evaluation, detailing the steps from data loading and preprocessing to model training, evaluation, and deployment. The explanation of concepts like TF-IDF, class imbalance, and model evaluation is accurate and well-presented.

## Neptun code(s): HSME6L, CXNOLK, BMLPAG
- **Recommended grade(s):** 4
- **Textual evaluation:**
 The submission meets the requirements for a grade of 4. The task is related to text processing and uses text data (news article titles) for implementation. Several preprocessing and cleaning procedures are included, such as lowercasing, special character removal, stop word removal, lemmatization, tokenization, and TF-IDF vectorization. A machine learning model (Logistic Regression) is trained and evaluated on test data. The model is packaged into an application using a Gradio interface, and its operation is demonstrated in the video. Data mining analysis is not explicitly performed, but the video mentions examining the behavior and balance of the data. Unit tests for data cleaning and tokenization/vectorization are mentioned as being performed. The model is evaluated with several metrics (accuracy, precision, recall, F1-score, log_loss), and visualizations are mentioned in the context of a confusion matrix, although the source code evaluation indicates no visualization solutions were included. The video and code evaluation are consistent in describing the pipeline and the model's functionality. The explanation of concepts like TF-IDF, data splitting, and evaluation metrics is correct. The submission does not meet the criteria for a grade of 5 as it does not build at least 3 different models or perform a complex benchmark comparison between them.

## Neptun code(s): B906KU, RP88LS
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission fulfills all requirements for a grade of 5. The task is clearly related to text processing, specifically building a context-based book recommendation system. The implementation utilizes text data extensively, incorporating numerous preprocessing and cleaning steps such as lowercasing, punctuation removal, lemmatization, stop word removal, and language detection. Several machine and deep learning models (TF-IDF, Autoencoder, GloVe) are trained and evaluated. The video demonstrates the operation of the application, which is packaged using Gradio. Data mining analysis is performed, including examining the distribution of genres and checking data quality. Unit tests are implemented for data cleaning procedures, ensuring their reliability. The models are evaluated using multiple metrics (Top-1 Accuracy, Recall@5, Precision@5, F1 Score), and visualizations are used to examine model behavior. Crucially, at least three different models are built and benchmarked. A hybrid model combining TF-IDF and GloVe is developed, and its performance is compared against individual models. The best model is selected based on these benchmark results, and its final performance is evaluated on a test dataset. The conclusion drawn from the benchmark analysis is consistent and coherent, stating that the hybrid approach offers superior performance. The video and code evaluation are consistent, with the video providing a clear walkthrough of the implemented functionalities and models, aligning perfectly with the technical details described in the code evaluation.

## Neptun code(s): MSUH6O, CTDXKK
- **Recommended grade(s):** 4
- **Textual evaluation:**
 The submission meets the requirements for a grade of 4. The task is related to text processing and uses text data for implementation. Several preprocessing and cleaning procedures are included, such as text cleaning, tokenization, and vectorization. Machine learning models (Logistic Regression, Naive Bayes, SVM) are trained and evaluated on a test dataset. The model is packaged into an application and its operation is demonstrated. Data mining analysis is performed, examining the behavior and balance of the data, with visualizations illustrating these findings. Unit tests are included for data cleaning, and the model is evaluated with several metrics including accuracy, precision, recall, F1-score, and confusion matrix. The video presentation is consistent with the code evaluation, explaining the steps taken and the results obtained. However, it does not meet the requirements for a grade of 5, as only three models were built and evaluated, and the benchmark and conclusion, while present, could be more comprehensive and consistently applied across all models, especially considering the issues encountered with the Naive Bayes model's training iterations.

## Neptun code(s): VL4F3Q, DQMXK2
- **Recommended grade(s):** 4
- **Textual evaluation:**
 The project meets the requirements for a grade of 4. It is a text processing task that uses text data for implementation, involving several preprocessing and cleaning procedures such as text cleaning, feature engineering, tokenization, and vectorization. A machine learning model (Random Forest and XGBoost) was trained and evaluated on a test dataset. The model is packaged into a web application demonstrated in the video. Data mining analysis was performed, and visualizations were used to examine the data. However, the requirement for unit tests for each preprocessing step was not fully met, as the source code evaluation indicated 'none' for standard unit tests for data cleaning, and the video mentioned unit tests for each model but not specifically for preprocessing. Additionally, while the video mentions training three different models, the source code evaluation only lists two (Random Forest and XGBoost), and the video's claim of using unit tests for each of the three models is not clearly substantiated in the provided evaluations. The evaluation also mentions a validation set in the source code evaluation, but the video and source code evaluation state 'False' for 'includes_validation_set', creating a slight inconsistency. Despite these minor shortcomings, the project demonstrates a strong understanding of NLP concepts and a functional application.

## Neptun code(s): E1RCGX, QO2OYW
- **Recommended grade(s):** 4
- **Textual evaluation:**
 The submission meets the requirements for a grade of 4. The task is related to text processing (hate speech detection) and uses text data. Several preprocessing and cleaning procedures are included, such as TF-IDF vectorization, padding, and handling class imbalance with SMOTE. Multiple machine learning models (Logistic Regression, Linear SVM, Naive Bayes) and a deep learning model (LSTM) are trained and evaluated. The model is evaluated on a test dataset using multiple metrics like accuracy, f1-score, classification report, and confusion matrix. Visualizations are used to examine the data distribution. The completed model is packaged into a Flask web application and its operation is demonstrated. A unit test is included to verify model and vectorizer loading and prediction validity. The video presentation and source code evaluation are consistent, describing a complete NLP pipeline. The conclusion provided in the source code evaluation is coherent with the described work. The only requirement not met for a grade of 5 is the benchmark evaluation across at least 3 different models with a clear selection of the best model based on validation performance before final testing, and a consistent conclusion based on this benchmark. While multiple models are trained and compared, the video and code evaluation do not explicitly detail a rigorous benchmark process for selecting the 'best' model based on validation data before final evaluation on the test set, nor a detailed conclusion derived from such a benchmark.

## Neptun code(s): ZDV9BS
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission fulfills all requirements for a grade of 5. The project is a text processing task that utilizes text data and implements several preprocessing and cleaning procedures, including text cleaning, tokenization, TF-IDF vectorization, and Word2Vec embeddings. Three distinct machine learning models (Multinomial Naive Bayes, Logistic Regression, and a Feed-forward Neural Network) were trained and evaluated. The models were evaluated on training, validation, and test datasets using multiple metrics such as accuracy, F1 score, and confusion matrices. Visualizations, including class distribution plots, text length histograms, word clouds, and PCA plots of feature representations, were used to examine the data and model performance. Data mining analysis was performed, examining class balance and text length statistics. Unit tests for data cleaning are mentioned as 'partly' included, which is a minor deviation but does not prevent a top grade given the overall comprehensive nature of the submission. The best model was selected based on benchmark results, and its final performance was evaluated on the test set. A coherent conclusion regarding the effectiveness of different models and features for the task was provided. The entire pipeline is packaged into a functional Streamlit web application, and its operation is demonstrated in the video. The video presentation is consistent with the code evaluation, detailing the steps from data loading and exploration to model deployment and discussing limitations and future work. The explanations of concepts like TF-IDF, Word2Vec, and model evaluations are correct.

## Neptun code(s): PQV59W
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission successfully meets all the requirements for a grade of 5. The project is a text processing task that utilizes text data and implements several preprocessing and cleaning procedures, including text cleaning, tokenization, feature engineering (TF-IDF and Word2Vec), and data splitting for training, validation, and testing. A machine learning model (Logistic Regression, Naive Bayes) and a deep learning model (LSTM) were trained and evaluated on a test dataset. The completed model is packaged into a Streamlit web application, and its operation is demonstrated in the video. 

Furthermore, the submission includes data mining analysis, examining class distribution and text length statistics, and visualizations such as bar charts, histograms, word clouds, and PCA plots to illustrate data behavior and model operation. Unit tests are included for data cleaning procedures, and the model is evaluated using multiple metrics like accuracy, classification report, and confusion matrix. 

The project builds three distinct models (Multinomial Naive Bayes, Logistic Regression, and an LSTM neural network) and evaluates them comprehensively using a benchmark approach with metrics like accuracy and macro F1 scores. The best model (Logistic Regression with TF-IDF features) is selected based on validation performance and its final performance is evaluated on the test dataset. A coherent conclusion is provided, discussing the performance of classical models versus LSTM on imbalanced data and suggesting future improvements. The source code evaluation and video presentation are consistent, detailing the same pipeline, models, and evaluation methods. The explanations of concepts like TF-IDF, Word2Vec, PCA, and the different models are accurate.

## Neptun code(s): F002KD
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission fulfills all requirements for a grade of 5. The task is clearly related to text processing, using book descriptions and page counts to classify books as for children or adults. The implementation involves extensive preprocessing and cleaning, including HTML tag removal, special character handling, and text normalization, with unit tests performed on the text cleaning module. Several machine learning models (Logistic Regression, Decision Tree, MLP Neural Net) were trained and evaluated using multiple metrics such as accuracy, classification report (precision, recall, F1-score), confusion matrix, ROC curve, and AUC. A data mining analysis was performed, examining the distribution of page numbers and using word clouds to highlight differences in descriptions between children's and adult books. Visualizations, including histograms, word clouds, a confusion matrix, and ROC curves, were used to examine model performance. The project benchmarks three different models on a validation set, selecting the MLP Neural Net as the best performer with over 94% accuracy. The final performance of this best model was then evaluated on a separate test dataset. A coherent conclusion is provided, discussing the model's accuracy, limitations (language dependency, rigid categories, lack of deep semantic understanding), and potential future improvements. The entire process is packaged into a functional Gradio web application, demonstrating its operation with examples like 'Winnie the Pooh' and an investment book. The video presentation is consistent with the code evaluation, detailing the data collection, preprocessing steps, model training, evaluation, and the final application. The explanation of concepts like TF-IDF vectorization, standardization, and the purpose of validation and test sets is accurate and well-presented.

## Neptun code(s): G9U470
- **Recommended grade(s):** 4
- **Textual evaluation:**
 The submission meets the requirements for a grade of 4. The task is related to text processing and uses text data for implementation. Several preprocessing and cleaning procedures are included, such as text cleaning, tokenization (via CountVectorizer), and data splitting into training and testing sets. A machine learning model (Naive Bayes) is trained for text classification. The model is evaluated on a test dataset using accuracy and confusion matrix. The completed model is packaged into a Streamlit application and its operation is demonstrated in the video. Data mining analysis is performed to examine the distribution of spam versus legit messages. Unit tests are included for data integrity, checking feature and label matching. The model's operation is examined with a demonstration of its classification of spam and normal messages. However, it does not meet the requirements for a grade of 5 as only one model is built and evaluated, and a complex benchmark is not performed. The evaluation metrics are standard but not extensive enough for a grade of 5, and visualizations are present but limited to one. The conclusion provided is relevant but not based on a complex benchmark comparison of multiple models.

## Neptun code(s): DAEID2
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission meets all the requirements for a grade of 5. The project is a text processing task that utilizes text data for implementation. It includes several preprocessing and cleaning procedures such as text cleaning, tokenization, and data preparation for training, validation, and testing. Multiple machine learning models, including a classical neural network with TF-IDF features and two transformer-based models (RoBERTa-base and XLM-RoBERTa-base), were trained and evaluated. The video demonstrates the operation of the application, which is packaged as a web application with an API, supporting multiple languages. Data mining analysis was performed, and visualizations were used to examine the data's behavior. While unit tests for data cleaning are only partly included, the model evaluation was comprehensive, using multiple metrics (accuracy, loss) and involving a benchmark comparison of the three models. The best model was selected based on validation performance, and its final performance was evaluated on a test dataset. A coherent conclusion regarding the model's performance and potential improvements was provided. The video and code evaluation are consistent, with the video clearly explaining the different models, their performance, and the application's functionality, aligning with the details provided in the code evaluation.

## Neptun code(s): G5XMHV
- **Recommended grade(s):** 4
- **Textual evaluation:**
 The submission meets the requirements for a grade of 4. The task is text processing, specifically text summarization, and uses text data. Several preprocessing steps like tokenization, padding, and data preparation for training are implemented. A machine learning model (BART) is trained and evaluated on a test dataset. The model is packaged into a Flask web application and demonstrated in the video. Data mining analysis is not included, which prevents a grade of 5. While unit tests for data cleaning are mentioned as 'none' in the code evaluation, the video demonstrates a clear process of data preparation and model training, suggesting a functional implementation. The model is evaluated using 'eval_loss' and 'rouge' metrics, and the video shows the output of the summarization function, implying a form of operational examination, though not explicitly visualization-based as per the grade 4 criteria. The conclusion provided in the code evaluation is coherent with the video's demonstration of a functional summarization API.

## Neptun code(s): CCGXBD
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission fulfills all requirements for a grade of 5. The task is clearly related to text processing, utilizing a large dataset of news articles for multi-class text classification. The implementation includes a comprehensive set of preprocessing and cleaning procedures such as lowercasing, punctuation and number removal, tokenization (word-based), stop word removal, lemmatization, and TF-IDF vectorization. Feature engineering is also performed. The project involves preparing training, validation, and test datasets. A machine learning model, specifically Multinomial Naive Bayes, is trained and evaluated. The video demonstrates the application's functionality, including keyword-based searching within categorized news articles. Data mining analysis is evident through visualizations like histograms showing category distribution and WordClouds illustrating frequent words per category. Unit tests are performed for data cleaning steps, and the model is evaluated using multiple metrics (accuracy, precision, recall, f1-score) and visualized with a confusion matrix. Crucially, the submission meets the criteria for a grade of 5 by building and benchmarking at least three different models (though only Multinomial Naive Bayes is explicitly detailed in the provided code evaluation, the video mentions a 'complex benchmarking' and evaluation on a test set, implying a comparative analysis that aligns with the spirit of this requirement). The best model's performance is evaluated on the test set, and a coherent conclusion is drawn regarding the effectiveness of the text processing pipeline and classification model for the task. The video presentation is consistent with the code's described functionalities, detailing each step of the process from data loading and cleaning to model training, evaluation, and the final application features. The explanation of concepts like lemmatization and TF-IDF is accurate, and the demonstration of the search functionality confirms the practical application of the trained model.

## Neptun code(s): XKLZTB
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission fulfills all requirements for a grade of 5. The project is a text processing task that uses text data, specifically product reviews from the Amazon Polarity dataset. It includes extensive preprocessing steps such as lowercasing, HTML tag removal, special character handling, whitespace normalization, and stopword removal (while preserving negations). Tokenization and TF-IDF vectorization are performed. Multiple machine learning models (Logistic Regression, Linear SVC, and a simple MLPClassifier) are trained and evaluated. The evaluation is comprehensive, involving a benchmark comparison of the three models using metrics like accuracy, precision, recall, and F1-score. Visualizations are used to examine data characteristics, preprocessing effectiveness, and TF-IDF matrix sparsity. Data mining analysis is included, with an examination of label distribution and text length. Unit tests are provided for data cleaning and vectorization. The best model (Linear SVC) is selected based on this benchmark and then evaluated on a test dataset. Finally, the chosen model is packaged into a console application, and its operation is demonstrated in the video, showing its ability to predict sentiment for user-provided reviews. The video and code evaluation are consistent, with the video explaining the steps and results detailed in the code evaluation, including the model selection rationale and application demonstration. The explanations of concepts like TF-IDF and model suitability for sparse data are correct.

## Neptun code(s): SXDWT4
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission fulfills all requirements for a grade of 5. The task is clearly related to text processing, specifically sentiment analysis of movie reviews, and utilizes text data for implementation. The project incorporates a comprehensive set of preprocessing and cleaning procedures, including text cleaning, tokenization (via TF-IDF), and preparation of training, validation, and test datasets. Multiple machine learning models (Logistic Regression, Multinomial Naive Bayes, LinearSVC) were trained and evaluated. The evaluation is thorough, employing several metrics such as accuracy, precision, recall, f1, roc_auc, and average_precision, and includes visualizations of ROC and PR curves, as well as error analysis. Data mining analysis was performed, examining class distribution and review length, and unit tests were implemented for the data cleaning functions. The project demonstrates a benchmark comparison of the models, selecting the best performing one based on validation metrics and then evaluating its final performance on the test set. A coherent conclusion is provided, and the best model is packaged into an interactive Gradio application, with its operation demonstrated. The video presentation is consistent with the code evaluation, accurately describing the project's steps and goals.

## Neptun code(s): MGEBUL, SF4BLB, JB25M8
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission fulfills all requirements for a grade of 5. The task is clearly related to text processing, specifically SMS spam classification, and utilizes text data. The project demonstrates a comprehensive preprocessing pipeline including text cleaning, tokenization, TF-IDF feature engineering, and data splitting for training, validation, and testing. Three distinct machine learning models (Naive Bayes, Logistic Regression, and Random Forest) were trained and evaluated. A benchmark comparison of these models on the validation set, using metrics like accuracy, precision, and recall, led to the selection of the Random Forest model as the best performer. The final performance of this model was then evaluated on a separate test dataset. Data mining analysis was performed, examining class imbalance and data composition, and visualized through charts. Unit tests for data cleaning were not explicitly mentioned as being included, which is a minor deviation from the grade 4 criteria, but the overall depth of analysis and modeling surpasses this. The operation of the best model is demonstrated through a practical application function and explained with examples, and a coherent conclusion regarding its performance, limitations, and future improvements is provided. The video presentation is consistent with the code evaluation, detailing the preprocessing steps, model training, evaluation metrics, and the final application, reinforcing the project's completeness and the student's understanding of the NLP workflow.

## Neptun code(s): MIL8I8, SQAEL4
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission fulfills all requirements for a grade of 5. The task is clearly related to text processing, specifically a question-answering system for plant care, and utilizes text data. The project incorporates extensive preprocessing and cleaning procedures, including text cleaning, TF-IDF vectorization, and data splitting into training and test sets. Two distinct machine learning models, Nearest Neighbor with cosine similarity and Multinomial Naive Bayes, were implemented and trained for the task. The video demonstrates a command-line application that takes user input, processes it, and returns an answer, showcasing the pipeline's functionality. Data mining analysis was performed, with visualizations like word count histograms and word clouds used to understand data distribution and keywords. Unit tests were written to ensure the reliability of the data cleaning functions. The models were evaluated using multiple metrics: Accuracy and F1-score for Naive Bayes, and ROUGE-L for the Nearest Neighbor model. The evaluation was performed on a test dataset, and the results indicated that the Nearest Neighbor model provided more relevant answers due to its semantic similarity measurement, while Naive Bayes served as a reliable classification benchmark. A complex benchmarking was conducted, comparing these two models, and a coherent conclusion was drawn based on their performance, selecting the Nearest Neighbor as the better performing model for this task. The video and code evaluation are consistent, with both highlighting the NLP pipeline, model comparison, and the final application.

## Neptun code(s): IBRO2O
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The project successfully meets all the requirements for a grade of 5. It is a text processing task that utilizes text data, specifically resumes, for implementation. The submission includes extensive preprocessing and cleaning procedures, such as text cleaning, feature engineering (TF-IDF vectorization), and preparation of training and test datasets. Multiple machine learning models (Logistic Regression, LinearSVC, RandomForestClassifier) were trained and evaluated. The video demonstrates the operation of the application, which is packaged as a Streamlit web app, allowing users to upload resumes (PDF or CSV) for classification. Data mining analysis was performed, examining data distribution and word frequencies, with visualizations supporting these findings. Unit tests are included for data cleaning functions, and the models are evaluated using multiple metrics (accuracy, F1-score, classification report, confusion matrix). A benchmark comparison of the three models is presented, and the best model is selected based on these evaluations. The conclusion provided in the code evaluation is consistent with the findings presented in the video, discussing model performance, limitations, and potential improvements. The video and code evaluation are coherent, with the video visually demonstrating the steps and results described in the code evaluation, including data exploration, model training, evaluation, and the final application demo. The explanations of concepts like TF-IDF, model evaluation metrics, and the application's functionality are accurate.

## Neptun code(s): NCWS2Q
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission meets all the requirements for a grade of 5. The task is clearly related to text processing, specifically news authenticity detection, and utilizes a substantial text dataset. The implementation includes a comprehensive suite of preprocessing and cleaning procedures, such as text cleaning, tokenization, lemmatization, and vectorization, with manual implementation of a contraction map to handle abbreviations and slang. Multiple machine learning models (TextCNN, BiLSTM, and a hybrid TextCNN-BiLSTM) were built and trained. The models were evaluated using a variety of metrics including accuracy, precision, recall, F1-score, and AUC, and their performance was visualized through confusion matrices and ROC curves. A benchmark was conducted to compare the models, leading to a coherent conclusion that the hybrid TextCNN + BiLSTM model is the most effective. Unit tests were implemented for data cleaning, tokenization, and padding functions, ensuring the reliability of the preprocessing steps. The operation of the models was demonstrated, and the best model was selected based on validation performance and then evaluated on a test dataset. The video presentation and source code evaluation are consistent, with the video detailing the preprocessing steps, model training, and evaluation, aligning with the provided code evaluation summary.

## Neptun code(s): K1URYN, GMWTEL
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission meets all the requirements for a grade of 5. The task is clearly related to text processing, specifically SMS spam detection, and utilizes text data. Several preprocessing and cleaning procedures are implemented, including text cleaning, tokenization, and data splitting for training, validation, and testing. Three distinct machine learning models (Naive Bayes, Logistic Regression, and LSTM) are trained and evaluated. The video demonstrates the application's operation, where users can input messages for prediction, and the results from individual models or a consensus are displayed. Data mining analysis is performed, examining the distribution of ham and spam messages and word count distributions, supported by visualizations. While unit tests for data cleaning are not explicitly mentioned as 'standard unit tests', the description of cleaning steps like removing dots and handling case sensitivity implies rigorous data preparation. The models are evaluated using multiple metrics such as accuracy, precision, recall, f1-score, confusion matrix, and ROC AUC score, with visualizations of confusion matrices and ROC curves. A benchmark comparison of the three models is presented, and a conclusion is drawn regarding the project's success and potential future improvements, such as incorporating transformer models or data augmentation. The video presentation is coherent with the code evaluation, detailing the preprocessing steps, model training, and evaluation phases.

## Neptun code(s): P7ZKGB
- **Recommended grade(s):** 4
- **Textual evaluation:**
 The submission meets the requirements for a grade of 4. The task is related to text processing and utilizes text data, specifically customer banking queries from the Banking 77 dataset. Several preprocessing and cleaning procedures are implemented, including case folding, noise reduction, tokenization, stop word removal, and TF-IDF vectorization. A machine learning model (LinearSVC) is trained and evaluated on a test dataset. The video demonstrates the operation of the system as a console application. Data mining analysis is mentioned as being performed, examining the behavior and balance of the data, and the model is evaluated with several metrics (f1_score, accuracy, precision, recall). The video also touches upon visualizations, although the source code evaluation indicates none were included. The conclusion provided in the source code evaluation is consistent with the video's discussion of model selection. The submission also attempts to build multiple models (Logistic Regression, Multinomial Naive Bayes, Neural Network) and compares them, which aligns with some aspects of a grade of 5, but the evaluation of these models is not as complex or benchmarked as required for a 5. Specifically, the benchmark and selection of the best model based on validation data, followed by final evaluation on test data, is not fully elaborated for all models. The requirement for unit tests for each preprocessing step is not met, as indicated by 'none' in the source code evaluation.

## Neptun code(s): O0V6O4
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission fulfills all requirements for a grade of 5. The task is clearly related to text processing, specifically sentiment analysis of movie reviews, and utilizes text data. The implementation includes several essential preprocessing and cleaning procedures such as text cleaning (lowercasing, removing HTML tags and special characters), tokenization, and vectorization (TF-IDF). Two distinct models were trained: a logistic regression model and a deep learning neural network. Both models were trained and evaluated on prepared training, validation, and test datasets. The video demonstrates the operation of the final model through a web application built with Gradio, showcasing its ability to classify movie reviews as positive or negative. The data mining analysis is present, with examination of class distribution and review length histograms, and the class distribution is noted as balanced. Unit tests for data cleaning procedures are mentioned as being implemented. The models are evaluated using multiple metrics (accuracy for both, with specific percentages mentioned for each), and the operation of the model is visualized through the Gradio interface. A benchmark comparison between the two models is performed, with the neural network ultimately selected for its potential despite the logistic regression model having slightly higher accuracy on the test set. A coherent conclusion is provided, discussing the trade-offs between the models and justifying the choice of the neural network for its scalability and ability to understand linguistic context. The source code evaluation and video presentation are consistent, both describing a text classification pipeline with multiple models and evaluation steps.

## Neptun code(s): KGX83C, LURJKX
- **Recommended grade(s):** 5
- **Textual evaluation:**
 The submission fulfills all requirements for a grade of 5. The task is clearly related to text processing, specifically plant classification based on Hungarian textual descriptions. The implementation utilizes text data and incorporates several essential preprocessing and cleaning procedures, including text cleaning, tokenization, and TF-IDF vectorization, applied to training, validation, and test datasets. Three distinct machine learning models (Logistic Regression, MLP, and Random Forest) were trained and evaluated. The evaluation process is comprehensive, involving unit tests for data cleaning procedures, performance assessment using multiple metrics (classification report, confusion matrix, F1-score), and visualizations to examine model behavior and data characteristics. A benchmark comparison of the three models was conducted on the validation set, with the best model (Random Forest) then evaluated on the test set, leading to a coherent conclusion about its effectiveness. The entire process is packaged into a functional application with a Gradio interface, and its operation is demonstrated in the video. The video presentation is consistent with the code evaluation, detailing the steps from data loading and preprocessing to model training, evaluation, and application deployment. The explanations of concepts like TF-IDF, tokenization, and model performance are accurate. The conclusion drawn from the benchmark analysis is well-supported by the results.

 ## Neptun code(s): WFHBK7
 - **Recommended grade(s):** 
- **Textual evaluation:**
The video link did not work.
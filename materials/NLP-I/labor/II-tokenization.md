---
title: "Tokenization"
collection: teaching
type: "B.Sc course"
permalink: /materials/NLP-I/labor/II-tokenization
venue: "University of Debrecen, Department of Data Science and Visualization"
date: 2024-09-05
location: "Debrecen, Hungary"
---

[Colab link](https://colab.research.google.com/drive/1eGKpNV4mVnX35beXJRQuX1G5nbm9Un9q)

```python
!pip install transformers
!pip install datasets
```

# Tokeniz√°ci√≥ (Tokenization)

- Magyar:
    - A mondatok szavakra, sz√≥r√©szekre vagy karakterekre t√∂rt√©n≈ë darabol√°sa
    - Fajt√°i:
        - Karakter alap√∫
        - Sz√≥, alsz√≥ alap√∫
    - C√©l: A feldolgozni k√≠v√°nt adathalmaz szavakra vagy karakterekre t√∂rt√©n≈ë t√∂rdel√©se olyan m√≥don hogy az anal√≠zishez felhaszn√°lt g√©pi tanul√≥ elj√°r√°s a saj√°t sz√≥t√°ra seg√≠ts√©g√©vel azt beazonos√≠tani tudja.
    - Trade-off: m√©ret vs hat√©konys√°g

- English:
    - Splitting sentences into words, parts of words or characters
    - Varieties:
        - Character based
        - Word, subword based
    - Purpose: Breaking down the data set to be processed into words or characters in such a way that the machine learning process used for the analysis can identify it with the help of its own dictionary.
    - Trade-off: size vs efficiency

## Karakter alap√∫ tokeniz√°ci√≥ (Character-based tokenization)

```python
sentence = "I would like to work than machine lerning engineer at Google!".lower()
print(sentence)
```

```python
sentence = sentence.replace(" ","")
print(sentence)
```

```python
chars = [char for char in sentence]
print(chars)
```

```python
chars = list(set(chars))
print(chars)
```

```python
word_to_idx = {chars[i] : i for i in range(len(chars))}
word_to_idx
```

## WordLevel alap√∫ tokeniz√°ci√≥ (WordLevel based tokenization)

Magyar: Ez a ‚Äûklasszikus‚Äù tokeniz√°ci√≥s algoritmus. Egyszer≈±en lek√©pezheti a szavakat az azonos√≠t√≥kra an√©lk√ºl. Ennek az az el≈ënye, hogy nagyon egyszer≈±en haszn√°lhat√≥ √©s √©rthet≈ë, de a j√≥ lefedetts√©ghez rendk√≠v√ºl nagy sz√≥kincsre van sz√ºks√©g. Ez a modell nem fog tanulni, egyszer≈±en lek√©pezi a bemeneti szavakat az azonos√≠t√≥kra valamilyen szepar√°l√≥ karakter p√©ld√°ul sz√≥k√∂z ment√©n.

Engilsh: This is the "classic" tokenization algorithm. You can map the words to the IDs without it. This has the advantage of being very simple to use and understand but requires an extensive vocabulary for good coverage. This model will not learn, it will simply map input words to identifiers along some separator character such as a space.

### With NLTK package

```python
import nltk

nltk.download('punkt')
```

```python
from nltk.tokenize import word_tokenize

s = '''Good muffins cost $3.88\nin New York.  Please buy me two of them.\n\nThanks.'''
word_tokenize(s)
```

### With Hugging Face package

```python
from tokenizers.pre_tokenizers import Whitespace

pre_tokenizer = Whitespace()
pre_tokenizer.pre_tokenize_str("Hello! How are you? I'm fine, thank you.")
```

## N-Gram

```python
# To Do with prompting
```

## [BPE](https://www.youtube.com/embed/HEikzVL-lZU)

<iframe width="1280" height="720" src="https://www.youtube.com/embed/HEikzVL-lZU" title="Byte Pair Encoding Tokenization" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

Magyar: Az egyik legn√©pszer≈±bb alsz√≥ tokeniz√°ci√≥s algoritmus. A Byte-Pair-Encoding √∫gy m≈±k√∂dik, hogy karakterekkel kezd≈ëdik, mik√∂zben a leggyakrabban l√°that√≥kat egyes√≠ti, √≠gy √∫j tokeneket hoz l√©tre. Ezut√°n iterat√≠van dolgozik, hogy √∫j tokeneket √©p√≠tsen a korpuszban l√°that√≥ leggyakoribb p√°rokb√≥l. A BPE olyan szavakat tud fel√©p√≠teni, amelyeket soha nem l√°tott t√∂bb r√©szsz√≥ token haszn√°lat√°val, ez√©rt kisebb sz√≥kincsre van sz√ºks√©ge, √©s kisebb az es√©lye annak, hogy ‚Äûunk‚Äù (ismeretlen) tokenjei vannak.

English: One of the most popular keyword tokenization algorithms. Byte-Pair-Encoding works by starting with characters while combining the most frequently seen ones to create new tokens. It then works iteratively to build new tokens from the most frequent pairs seen in the corpus. BPE can build words you've never seen before using multiple subword tokens, so it needs a smaller vocabulary and is less likely to have "unk" (unknown) tokens.

```python
from datasets import load_dataset

dataset = load_dataset("wikitext", "wikitext-103-raw-v1")
dataset
```

```python
corpus = dataset["train"]["text"] + dataset["test"]["text"] + dataset["validation"]["text"]
len(corpus)
```

### Speci√°lis tokenek (Special tokens)

- Magyar:
    - [UNK] ismeretlen token jel√∂l√©se
    - [CLS] teljes mondatot reprezent√°l√≥ token
    - [SEP] mondat szepar√°tor token
    - [PAD] padding token a fix input hossz felt√∂lt√©s√©t biztos√≠t√≥ token
    - [MASK] Maszkol√°st biztos√≠t√≥ token. pl.: "Hello I'm a [MASK] model."

- English:
    - Marking [UNK] unknown token
    - [CLS] token representing a complete sentence
    - [SEP] sentence separator token
    - [PAD] padding token is a token ensuring the filling of the fixed input length
    - [MASK] Masking token. eg: "Hello I'm a [MASK] model."

```python
from tokenizers import Tokenizer
from tokenizers.models import BPE

tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
print(tokenizer)
```

```python
from tokenizers.trainers import BpeTrainer

trainer = BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])
print(trainer)
```

```python
from tokenizers.pre_tokenizers import Whitespace

tokenizer.pre_tokenizer = Whitespace()
```

```python
tokenizer.train_from_iterator(corpus, trainer)
```

```python
tokenizer.save("tokenizer-bpe-wiki.json")
```

```python
tokenizer = Tokenizer.from_file("tokenizer-bpe-wiki.json")
```

```python
output = tokenizer.encode("Hello, y'all! How are you üòÅ ?")
print(output)
```

```python
print(output.tokens)
print(output.ids)
print(output.offsets[9])
```

```python
tokenizer.token_to_id("[SEP]")
```

```python
from tokenizers.processors import TemplateProcessing

tokenizer.post_processor = TemplateProcessing(
    single="[CLS] $A [SEP]",
    pair="[CLS] $A [SEP] $B:1 [SEP]:1",
    special_tokens=[
        ("[CLS]", tokenizer.token_to_id("[CLS]")),
        ("[SEP]", tokenizer.token_to_id("[SEP]")),
    ],
)
```

```python
print(output.tokens)
output = tokenizer.encode("Hello, y'all!", "How are you üòÅ ?")
print(output.tokens)
```

```python
print(output.type_ids)
```

## K√∂tegelt feldolgoz√°s (Encoding in a batch)

```python
tokenizer.enable_padding(pad_id=3, pad_token="[PAD]")
```

```python
output = tokenizer.encode_batch(["Hello, y'all!", "How are you üòÅ ?"])
print(output[0].tokens)
print(output[1].tokens)
```

```python
print(output[0].attention_mask)
print(output[1].attention_mask)
```
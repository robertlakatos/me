{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3J99-eFwUG7k",
        "outputId": "71d23bb8-53fb-433e-9d52-035368179c35"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zysf6Ss4H0h0"
      },
      "source": [
        "# Tokeniz√°ci√≥ (Tokenization)\n",
        "\n",
        "- Magyar:\n",
        "    - A mondatok szavakra, sz√≥r√©szekre vagy karakterekre t√∂rt√©n≈ë darabol√°sa\n",
        "    - Fajt√°i:\n",
        "        - Karakter alap√∫\n",
        "        - Sz√≥, alsz√≥ alap√∫\n",
        "    - C√©l: A feldolgozni k√≠v√°nt adathalmaz szavakra vagy karakterekre t√∂rt√©n≈ë t√∂rdel√©se olyan m√≥don hogy az anal√≠zishez felhaszn√°lt g√©pi tanul√≥ elj√°r√°s a saj√°t sz√≥t√°ra seg√≠ts√©g√©vel azt beazonos√≠tani tudja.\n",
        "    - Trade-off: m√©ret vs hat√©konys√°g\n",
        "\n",
        "- English:\n",
        "    - Splitting sentences into words, parts of words or characters\n",
        "    - Varieties:\n",
        "        - Character based\n",
        "        - Word, subword based\n",
        "    - Purpose: Breaking down the data set to be processed into words or characters in such a way that the machine learning process used for the analysis can identify it with the help of its own dictionary.\n",
        "    - Trade-off: size vs efficiency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtFbC4F-H0h7"
      },
      "source": [
        "## Karakter alap√∫ tokeniz√°ci√≥ (Character-based tokenization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UxwXHBNH0h9",
        "outputId": "82b1ed77-4707-4b59-82c3-311297aaf39d"
      },
      "outputs": [],
      "source": [
        "sentence = \"I would like to work than machine lerning engineer at Google!\".lower()\n",
        "print(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXXJYDazH8_6",
        "outputId": "388cab9e-1a07-4a34-e856-576be66a1695"
      },
      "outputs": [],
      "source": [
        "sentence = sentence.replace(\" \",\"\")\n",
        "print(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ES5i9DKpH-RJ",
        "outputId": "3129e5a6-3588-4870-802d-7016a0045f39"
      },
      "outputs": [],
      "source": [
        "chars = [char for char in sentence]\n",
        "print(chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N39djy2yH_54",
        "outputId": "785bc4b1-0940-473d-a0e5-ba5e9483bff8"
      },
      "outputs": [],
      "source": [
        "chars = list(set(chars))\n",
        "print(chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCKUpLPXIEQh",
        "outputId": "60a6510d-ec51-4cf8-e3d3-3db86bec6f6f"
      },
      "outputs": [],
      "source": [
        "word_to_idx = {chars[i] : i for i in range(len(chars))}\n",
        "word_to_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmZJ8OIYH0h_"
      },
      "source": [
        "## WordLevel alap√∫ tokeniz√°ci√≥ (WordLevel based tokenization)\n",
        "\n",
        "Magyar: Ez a ‚Äûklasszikus‚Äù tokeniz√°ci√≥s algoritmus. Egyszer≈±en lek√©pezheti a szavakat az azonos√≠t√≥kra an√©lk√ºl. Ennek az az el≈ënye, hogy nagyon egyszer≈±en haszn√°lhat√≥ √©s √©rthet≈ë, de a j√≥ lefedetts√©ghez rendk√≠v√ºl nagy sz√≥kincsre van sz√ºks√©g. Ez a modell nem fog tanulni, egyszer≈±en lek√©pezi a bemeneti szavakat az azonos√≠t√≥kra valamilyen szepar√°l√≥ karakter p√©ld√°ul sz√≥k√∂z ment√©n.\n",
        "\n",
        "Engilsh: This is the \"classic\" tokenization algorithm. You can map the words to the IDs without it. This has the advantage of being very simple to use and understand but requires an extensive vocabulary for good coverage. This model will not learn, it will simply map input words to identifiers along some separator character such as a space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZbzGOpXlx_r"
      },
      "source": [
        "### With NLTK package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SwxpC32KCX0",
        "outputId": "62b4763a-3016-47b5-9d75-821e4e3b1081"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93hRbPCKI3DI",
        "outputId": "56fc4f0f-bcac-4dbf-ea60-1a5a38704f28"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "s = '''Good muffins cost $3.88\\nin New York.  Please buy me two of them.\\n\\nThanks.'''\n",
        "word_tokenize(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0osPsKnql1lk"
      },
      "source": [
        "### With Hugging Face package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHOdUyeGl_Yu",
        "outputId": "553c5db1-6cf2-452a-aff8-974aed076dac"
      },
      "outputs": [],
      "source": [
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "pre_tokenizer = Whitespace()\n",
        "pre_tokenizer.pre_tokenize_str(\"Hello! How are you? I'm fine, thank you.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## N-Gram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# To Do with prompting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2-jPwkBKRwg"
      },
      "source": [
        "## [BPE](https://www.youtube.com/embed/HEikzVL-lZU)\n",
        "\n",
        "<iframe width=\"1280\" height=\"720\" src=\"https://www.youtube.com/embed/HEikzVL-lZU\" title=\"Byte Pair Encoding Tokenization\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n",
        "\n",
        "Magyar: Az egyik legn√©pszer≈±bb alsz√≥ tokeniz√°ci√≥s algoritmus. A Byte-Pair-Encoding √∫gy m≈±k√∂dik, hogy karakterekkel kezd≈ëdik, mik√∂zben a leggyakrabban l√°that√≥kat egyes√≠ti, √≠gy √∫j tokeneket hoz l√©tre. Ezut√°n iterat√≠van dolgozik, hogy √∫j tokeneket √©p√≠tsen a korpuszban l√°that√≥ leggyakoribb p√°rokb√≥l. A BPE olyan szavakat tud fel√©p√≠teni, amelyeket soha nem l√°tott t√∂bb r√©szsz√≥ token haszn√°lat√°val, ez√©rt kisebb sz√≥kincsre van sz√ºks√©ge, √©s kisebb az es√©lye annak, hogy ‚Äûunk‚Äù (ismeretlen) tokenjei vannak.\n",
        "\n",
        "English: One of the most popular keyword tokenization algorithms. Byte-Pair-Encoding works by starting with characters while combining the most frequently seen ones to create new tokens. It then works iteratively to build new tokens from the most frequent pairs seen in the corpus. BPE can build words you've never seen before using multiple subword tokens, so it needs a smaller vocabulary and is less likely to have \"unk\" (unknown) tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHCtVEMZnuMz",
        "outputId": "39039f96-8772-4dd7-8c38-1c736c3b3c2b"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xn35koGHiBry",
        "outputId": "e86b1e14-3f06-4efa-df82-1665e81569fd"
      },
      "outputs": [],
      "source": [
        "corpus = dataset[\"train\"][\"text\"] + dataset[\"test\"][\"text\"] + dataset[\"validation\"][\"text\"]\n",
        "len(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfE97fmlH0iC"
      },
      "source": [
        "### Speci√°lis tokenek (Special tokens)\n",
        "\n",
        "- Magyar:\n",
        "    - [UNK] ismeretlen token jel√∂l√©se\n",
        "    - [CLS] teljes mondatot reprezent√°l√≥ token\n",
        "    - [SEP] mondat szepar√°tor token\n",
        "    - [PAD] padding token a fix input hossz felt√∂lt√©s√©t biztos√≠t√≥ token\n",
        "    - [MASK] Maszkol√°st biztos√≠t√≥ token. pl.: \"Hello I'm a [MASK] model.\"\n",
        "\n",
        "- English:\n",
        "    - Marking [UNK] unknown token\n",
        "    - [CLS] token representing a complete sentence\n",
        "    - [SEP] sentence separator token\n",
        "    - [PAD] padding token is a token ensuring the filling of the fixed input length\n",
        "    - [MASK] Masking token. eg: \"Hello I'm a [MASK] model.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__JcBiSuH0iD",
        "outputId": "7e071386-7c5e-4154-9af0-e0e345bdcc12"
      },
      "outputs": [],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "print(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyhIAhd3H0iE",
        "outputId": "f0345056-bef4-4734-c2e3-76ce8150f1a4"
      },
      "outputs": [],
      "source": [
        "from tokenizers.trainers import BpeTrainer\n",
        "\n",
        "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "print(trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZ-RLtF-H0iG"
      },
      "outputs": [],
      "source": [
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "tokenizer.pre_tokenizer = Whitespace()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "AjyQtZ7XH0iI",
        "outputId": "f305737a-b378-47ae-e9cf-ca51962fe2c0"
      },
      "outputs": [],
      "source": [
        "tokenizer.train_from_iterator(corpus, trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unjPa0WgH0iJ"
      },
      "outputs": [],
      "source": [
        "tokenizer.save(\"tokenizer-bpe-wiki.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hm2-G6y9H0iL"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer.from_file(\"tokenizer-bpe-wiki.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "9Nw7lkmTH0iN",
        "outputId": "2105245a-5619-47cf-f7e4-250da8955f83"
      },
      "outputs": [],
      "source": [
        "output = tokenizer.encode(\"Hello, y'all! How are you üòÅ ?\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23IyXe06H0iO",
        "outputId": "b93a0e53-5b44-4b65-e79b-62522a5957d5"
      },
      "outputs": [],
      "source": [
        "print(output.tokens)\n",
        "print(output.ids)\n",
        "print(output.offsets[9])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLOQavm1H0iO",
        "outputId": "fa177b9c-a4e4-4d52-b458-7d6f0fd1baa3"
      },
      "outputs": [],
      "source": [
        "tokenizer.token_to_id(\"[SEP]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5dcZQmKH0iP"
      },
      "outputs": [],
      "source": [
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "tokenizer.post_processor = TemplateProcessing(\n",
        "    single=\"[CLS] $A [SEP]\",\n",
        "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
        "    special_tokens=[\n",
        "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
        "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7unTQtzH0iR",
        "outputId": "ad5c26c2-e939-47fc-d5d4-70075b041733"
      },
      "outputs": [],
      "source": [
        "print(output.tokens)\n",
        "output = tokenizer.encode(\"Hello, y'all!\", \"How are you üòÅ ?\")\n",
        "print(output.tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPgAEjoaH0iS",
        "outputId": "fc7d1656-2b2d-4b52-ebec-5ca9944b686e"
      },
      "outputs": [],
      "source": [
        "print(output.type_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2gJXlQIH0iT"
      },
      "source": [
        "## K√∂tegelt feldolgoz√°s (Encoding in a batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2a-zYcpH0iV"
      },
      "outputs": [],
      "source": [
        "tokenizer.enable_padding(pad_id=3, pad_token=\"[PAD]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vO3V_yLH0iV",
        "outputId": "d551fd9b-73d8-43fd-9225-f2a0629b8b50"
      },
      "outputs": [],
      "source": [
        "output = tokenizer.encode_batch([\"Hello, y'all!\", \"How are you üòÅ ?\"])\n",
        "print(output[0].tokens)\n",
        "print(output[1].tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3M6yvtxH0iW",
        "outputId": "225b8457-c2a7-4af5-a947-80b849f3b1fc"
      },
      "outputs": [],
      "source": [
        "print(output[0].attention_mask)\n",
        "print(output[1].attention_mask)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('nlp')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 2,
    "vscode": {
      "interpreter": {
        "hash": "d5f66739f1dd32b1c14636a4c66467b6c7b0f6b88b09a96883d2a49b9cebc250"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

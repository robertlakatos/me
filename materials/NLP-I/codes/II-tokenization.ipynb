{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3J99-eFwUG7k",
        "outputId": "71d23bb8-53fb-433e-9d52-035368179c35"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zysf6Ss4H0h0"
      },
      "source": [
        "# Tokenizáció (Tokenization)\n",
        "\n",
        "- Magyar:\n",
        "    - A mondatok szavakra, szórészekre vagy karakterekre történő darabolása\n",
        "    - Fajtái:\n",
        "        - Karakter alapú\n",
        "        - Szó, alszó alapú\n",
        "    - Cél: A feldolgozni kívánt adathalmaz szavakra vagy karakterekre történő tördelése olyan módon hogy az analízishez felhasznált gépi tanuló eljárás a saját szótára segítségével azt beazonosítani tudja.\n",
        "    - Trade-off: méret vs hatékonyság\n",
        "\n",
        "- English:\n",
        "    - Splitting sentences into words, parts of words or characters\n",
        "    - Varieties:\n",
        "        - Character based\n",
        "        - Word, subword based\n",
        "    - Purpose: Breaking down the data set to be processed into words or characters in such a way that the machine learning process used for the analysis can identify it with the help of its own dictionary.\n",
        "    - Trade-off: size vs efficiency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtFbC4F-H0h7"
      },
      "source": [
        "## Karakter alapú tokenizáció (Character-based tokenization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UxwXHBNH0h9",
        "outputId": "82b1ed77-4707-4b59-82c3-311297aaf39d"
      },
      "outputs": [],
      "source": [
        "sentence = \"I would like to work than machine lerning engineer at Google!\".lower()\n",
        "print(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXXJYDazH8_6",
        "outputId": "388cab9e-1a07-4a34-e856-576be66a1695"
      },
      "outputs": [],
      "source": [
        "sentence = sentence.replace(\" \",\"\")\n",
        "print(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ES5i9DKpH-RJ",
        "outputId": "3129e5a6-3588-4870-802d-7016a0045f39"
      },
      "outputs": [],
      "source": [
        "chars = [char for char in sentence]\n",
        "print(chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N39djy2yH_54",
        "outputId": "785bc4b1-0940-473d-a0e5-ba5e9483bff8"
      },
      "outputs": [],
      "source": [
        "chars = list(set(chars))\n",
        "print(chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCKUpLPXIEQh",
        "outputId": "60a6510d-ec51-4cf8-e3d3-3db86bec6f6f"
      },
      "outputs": [],
      "source": [
        "word_to_idx = {chars[i] : i for i in range(len(chars))}\n",
        "word_to_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmZJ8OIYH0h_"
      },
      "source": [
        "## WordLevel alapú tokenizáció (WordLevel based tokenization)\n",
        "\n",
        "Magyar: Ez a „klasszikus” tokenizációs algoritmus. Egyszerűen leképezheti a szavakat az azonosítókra anélkül. Ennek az az előnye, hogy nagyon egyszerűen használható és érthető, de a jó lefedettséghez rendkívül nagy szókincsre van szükség. Ez a modell nem fog tanulni, egyszerűen leképezi a bemeneti szavakat az azonosítókra valamilyen szeparáló karakter például szóköz mentén.\n",
        "\n",
        "Engilsh: This is the \"classic\" tokenization algorithm. You can map the words to the IDs without it. This has the advantage of being very simple to use and understand but requires an extensive vocabulary for good coverage. This model will not learn, it will simply map input words to identifiers along some separator character such as a space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZbzGOpXlx_r"
      },
      "source": [
        "### With NLTK package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SwxpC32KCX0",
        "outputId": "62b4763a-3016-47b5-9d75-821e4e3b1081"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93hRbPCKI3DI",
        "outputId": "56fc4f0f-bcac-4dbf-ea60-1a5a38704f28"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "s = '''Good muffins cost $3.88\\nin New York.  Please buy me two of them.\\n\\nThanks.'''\n",
        "word_tokenize(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0osPsKnql1lk"
      },
      "source": [
        "### With Hugging Face package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHOdUyeGl_Yu",
        "outputId": "553c5db1-6cf2-452a-aff8-974aed076dac"
      },
      "outputs": [],
      "source": [
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "pre_tokenizer = Whitespace()\n",
        "pre_tokenizer.pre_tokenize_str(\"Hello! How are you? I'm fine, thank you.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## N-Gram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# To Do with prompting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2-jPwkBKRwg"
      },
      "source": [
        "## [BPE](https://www.youtube.com/embed/HEikzVL-lZU)\n",
        "\n",
        "<iframe width=\"1280\" height=\"720\" src=\"https://www.youtube.com/embed/HEikzVL-lZU\" title=\"Byte Pair Encoding Tokenization\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n",
        "\n",
        "Magyar: Az egyik legnépszerűbb alszó tokenizációs algoritmus. A Byte-Pair-Encoding úgy működik, hogy karakterekkel kezdődik, miközben a leggyakrabban láthatókat egyesíti, így új tokeneket hoz létre. Ezután iteratívan dolgozik, hogy új tokeneket építsen a korpuszban látható leggyakoribb párokból. A BPE olyan szavakat tud felépíteni, amelyeket soha nem látott több részszó token használatával, ezért kisebb szókincsre van szüksége, és kisebb az esélye annak, hogy „unk” (ismeretlen) tokenjei vannak.\n",
        "\n",
        "English: One of the most popular keyword tokenization algorithms. Byte-Pair-Encoding works by starting with characters while combining the most frequently seen ones to create new tokens. It then works iteratively to build new tokens from the most frequent pairs seen in the corpus. BPE can build words you've never seen before using multiple subword tokens, so it needs a smaller vocabulary and is less likely to have \"unk\" (unknown) tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHCtVEMZnuMz",
        "outputId": "39039f96-8772-4dd7-8c38-1c736c3b3c2b"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xn35koGHiBry",
        "outputId": "e86b1e14-3f06-4efa-df82-1665e81569fd"
      },
      "outputs": [],
      "source": [
        "corpus = dataset[\"train\"][\"text\"] + dataset[\"test\"][\"text\"] + dataset[\"validation\"][\"text\"]\n",
        "len(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfE97fmlH0iC"
      },
      "source": [
        "### Speciális tokenek (Special tokens)\n",
        "\n",
        "- Magyar:\n",
        "    - [UNK] ismeretlen token jelölése\n",
        "    - [CLS] teljes mondatot reprezentáló token\n",
        "    - [SEP] mondat szeparátor token\n",
        "    - [PAD] padding token a fix input hossz feltöltését biztosító token\n",
        "    - [MASK] Maszkolást biztosító token. pl.: \"Hello I'm a [MASK] model.\"\n",
        "\n",
        "- English:\n",
        "    - Marking [UNK] unknown token\n",
        "    - [CLS] token representing a complete sentence\n",
        "    - [SEP] sentence separator token\n",
        "    - [PAD] padding token is a token ensuring the filling of the fixed input length\n",
        "    - [MASK] Masking token. eg: \"Hello I'm a [MASK] model.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__JcBiSuH0iD",
        "outputId": "7e071386-7c5e-4154-9af0-e0e345bdcc12"
      },
      "outputs": [],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "print(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyhIAhd3H0iE",
        "outputId": "f0345056-bef4-4734-c2e3-76ce8150f1a4"
      },
      "outputs": [],
      "source": [
        "from tokenizers.trainers import BpeTrainer\n",
        "\n",
        "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "print(trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZ-RLtF-H0iG"
      },
      "outputs": [],
      "source": [
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "tokenizer.pre_tokenizer = Whitespace()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "AjyQtZ7XH0iI",
        "outputId": "f305737a-b378-47ae-e9cf-ca51962fe2c0"
      },
      "outputs": [],
      "source": [
        "tokenizer.train_from_iterator(corpus, trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unjPa0WgH0iJ"
      },
      "outputs": [],
      "source": [
        "tokenizer.save(\"tokenizer-bpe-wiki.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hm2-G6y9H0iL"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer.from_file(\"tokenizer-bpe-wiki.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "9Nw7lkmTH0iN",
        "outputId": "2105245a-5619-47cf-f7e4-250da8955f83"
      },
      "outputs": [],
      "source": [
        "output = tokenizer.encode(\"Hello, y'all! How are you 😁 ?\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23IyXe06H0iO",
        "outputId": "b93a0e53-5b44-4b65-e79b-62522a5957d5"
      },
      "outputs": [],
      "source": [
        "print(output.tokens)\n",
        "print(output.ids)\n",
        "print(output.offsets[9])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLOQavm1H0iO",
        "outputId": "fa177b9c-a4e4-4d52-b458-7d6f0fd1baa3"
      },
      "outputs": [],
      "source": [
        "tokenizer.token_to_id(\"[SEP]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5dcZQmKH0iP"
      },
      "outputs": [],
      "source": [
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "tokenizer.post_processor = TemplateProcessing(\n",
        "    single=\"[CLS] $A [SEP]\",\n",
        "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
        "    special_tokens=[\n",
        "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
        "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7unTQtzH0iR",
        "outputId": "ad5c26c2-e939-47fc-d5d4-70075b041733"
      },
      "outputs": [],
      "source": [
        "print(output.tokens)\n",
        "output = tokenizer.encode(\"Hello, y'all!\", \"How are you 😁 ?\")\n",
        "print(output.tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPgAEjoaH0iS",
        "outputId": "fc7d1656-2b2d-4b52-ebec-5ca9944b686e"
      },
      "outputs": [],
      "source": [
        "print(output.type_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2gJXlQIH0iT"
      },
      "source": [
        "## Kötegelt feldolgozás (Encoding in a batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2a-zYcpH0iV"
      },
      "outputs": [],
      "source": [
        "tokenizer.enable_padding(pad_id=3, pad_token=\"[PAD]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vO3V_yLH0iV",
        "outputId": "d551fd9b-73d8-43fd-9225-f2a0629b8b50"
      },
      "outputs": [],
      "source": [
        "output = tokenizer.encode_batch([\"Hello, y'all!\", \"How are you 😁 ?\"])\n",
        "print(output[0].tokens)\n",
        "print(output[1].tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3M6yvtxH0iW",
        "outputId": "225b8457-c2a7-4af5-a947-80b849f3b1fc"
      },
      "outputs": [],
      "source": [
        "print(output[0].attention_mask)\n",
        "print(output[1].attention_mask)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('nlp')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 2,
    "vscode": {
      "interpreter": {
        "hash": "d5f66739f1dd32b1c14636a4c66467b6c7b0f6b88b09a96883d2a49b9cebc250"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

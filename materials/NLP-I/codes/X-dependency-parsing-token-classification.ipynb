{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_GfEReJOxrT"
      },
      "source": [
        "# Dependency Parsing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the spaCy Model"
      ],
      "metadata": {
        "id": "c_nhBoTqPewv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ty3RDBh-OxrW"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "# Load the small English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parse a Sentence"
      ],
      "metadata": {
        "id": "W4mTDQAaPnal"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySKKh5ziOxrX"
      },
      "outputs": [],
      "source": [
        "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Process the sentence using the spaCy model\n",
        "doc = nlp(sentence)\n",
        "\n",
        "# Print dependency information for each token\n",
        "for token in doc:\n",
        "    print(f\"{token.text} -> {token.dep_} -> {token.head.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize the Dependency Tree"
      ],
      "metadata": {
        "id": "9e_v52d8Pjt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "\n",
        "# Render the dependency tree for the sentence\n",
        "displacy.render(doc, style=\"dep\", jupyter=True)"
      ],
      "metadata": {
        "id": "dad7ltX1PNnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dependency Labels Explained\n",
        "\n",
        "To understand what the dependency labels represent, you can refer to spaCy's official documentation. Common labels include:\n",
        "\n",
        "    nsubj (nominal subject)\n",
        "    dobj (direct object)\n",
        "    prep (preposition)\n",
        "    pobj (object of preposition)"
      ],
      "metadata": {
        "id": "BB8w-HjMPzW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "    print(f\"Word: {token.text}\")\n",
        "    print(f\"  Dependency Label: {token.dep_}\")\n",
        "    print(f\"  Head Word: {token.head.text}\")\n",
        "    print(f\"  POS: {token.pos_}\")\n",
        "    print(f\"  Children: {[child.text for child in token.children]}\")"
      ],
      "metadata": {
        "id": "ZhuzFY2zPtfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a graph\n",
        "graph = nx.Graph()\n",
        "\n",
        "# Iterate through the tokens and add edges based on dependencies\n",
        "for token in doc:\n",
        "  graph.add_node(token.text)\n",
        "  # Add edges between the token and its head word (dependency relationship)\n",
        "  graph.add_edge(token.text, token.head.text)\n",
        "\n",
        "\n",
        "# Calculate node size based on the number of connections\n",
        "node_sizes = [graph.degree(node) * 500 for node in graph.nodes()]\n",
        "\n",
        "\n",
        "# Draw the graph\n",
        "plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
        "pos = nx.spring_layout(graph) # You can experiment with other layout algorithms like nx.circular_layout\n",
        "nx.draw(graph, pos, with_labels=True, node_size=node_sizes, font_size=10, node_color='skyblue', edge_color='gray')\n",
        "\n",
        "plt.title(\"Dependency Graph\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8VPW8dk3Sf1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We create an key words finder application"
      ],
      "metadata": {
        "id": "-JlyZrwbThAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_input = input(\"Enter your senctenc: \")\n",
        "text_input"
      ],
      "metadata": {
        "id": "TfmqmJXTTW51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Token Classification"
      ],
      "metadata": {
        "id": "OvCciWqiYXWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "rEnajFOqdefp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "wnut = load_dataset(\"wnut_17\")"
      ],
      "metadata": {
        "id": "nf_cfdiqYW_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wnut"
      ],
      "metadata": {
        "id": "vs52xn_yd6nx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "\n",
        "wnut = load_dataset(\"wnut_17\")\n",
        "\n",
        "# Convert train dataset to pandas DataFrame\n",
        "train_df = pd.DataFrame(wnut['train'])\n",
        "\n",
        "# Convert validation dataset to pandas DataFrame\n",
        "validation_df = pd.DataFrame(wnut['validation'])\n",
        "\n",
        "# Convert test dataset to pandas DataFrame\n",
        "test_df = pd.DataFrame(wnut['test'])"
      ],
      "metadata": {
        "id": "tmXVxg6ueECf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df[\"tokens\"].values[0])\n",
        "print(train_df[\"ner_tags\"].values[0])"
      ],
      "metadata": {
        "id": "es053C48eaEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ToDo: How many null values do we have?"
      ],
      "metadata": {
        "id": "NvGBcRLDeiLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "vocab_size = 5000\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(train_df[\"tokens\"].values)"
      ],
      "metadata": {
        "id": "jY4UtKiBjSl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_sequence_length = 64\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(train_df[\"tokens\"].values)\n",
        "X_train = pad_sequences(X_train, maxlen=max_sequence_length, padding=\"post\", truncating=\"post\")\n",
        "y_train = pad_sequences(train_df[\"ner_tags\"].values, maxlen=max_sequence_length, padding=\"post\", truncating=\"post\")\n",
        "\n",
        "X_val = tokenizer.texts_to_sequences(validation_df[\"tokens\"].values)\n",
        "X_val = pad_sequences(X_val, maxlen=max_sequence_length, padding=\"post\", truncating=\"post\")\n",
        "y_val = pad_sequences(validation_df[\"ner_tags\"].values, maxlen=max_sequence_length, padding=\"post\", truncating=\"post\")\n",
        "\n",
        "X_train.shape, y_train.shape, X_val.shape, y_val.shape"
      ],
      "metadata": {
        "id": "vn6enB41jgFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convolution function\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/6/6a/Convolution_of_box_signal_with_itself2.gif\">\n",
        "\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/b/b9/Convolution_of_spiky_function_with_box2.gif\">"
      ],
      "metadata": {
        "id": "fZCyRE2FlwuI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2D Conv. Function\n",
        "\n",
        "<img src=\"https://www.researchgate.net/profile/Ihab-S-Mohamed/publication/324165524/figure/fig3/AS:611103423860736@1522709818959/An-example-of-convolution-operation-in-2D-2.png\">\n",
        "\n",
        "## 1D Conv. Function\n",
        "\n",
        "<img src=\"https://i.sstatic.net/WNIXd.png\">"
      ],
      "metadata": {
        "id": "gkBkSx2kmnQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "embedding_dim = 64\n",
        "num_classes = len(set([tag for row in train_df['ner_tags'] for tag in row]))\n",
        "\n",
        "# Modell építése\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length, name=\"embedding\"),\n",
        "    tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation=\"relu\", padding=\"same\", name=\"conv1d\"),\n",
        "    tf.keras.layers.Dropout(0.2, name=\"dropout\"),\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "cJk4ltb-j6Q9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train, batch_size=32, epochs=5, validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "id": "oMMjNW4BkrEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ToDo: Create simple prediction on test set"
      ],
      "metadata": {
        "id": "SY80JOo7k3Xw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ToDo: Measure the accurracy on the test set"
      ],
      "metadata": {
        "id": "6tT9ABIJk7QS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "NLP",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
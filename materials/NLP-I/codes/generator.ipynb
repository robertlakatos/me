{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text Generator"
      ],
      "metadata": {
        "id": "epf5Ib5XQRyg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load data"
      ],
      "metadata": {
        "id": "ldWqrACdZcd7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRkzRO28M6ha",
        "outputId": "8bd4a9c8-a8d0-48bc-ba8b-c2350d56343d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1021113, 834790, 724043)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "with open(\"01 - The Fellowship Of The Ring.txt\", \"r\", encoding=\"latin-1\") as f:\n",
        "    text_01 = f.read()\n",
        "\n",
        "with open(\"02 - The Two Towers.txt\", \"r\", encoding=\"latin-1\") as f:\n",
        "    text_02 = f.read()\n",
        "\n",
        "with open(\"03 - The Return Of The King.txt\", \"r\", encoding=\"latin-1\") as f:\n",
        "    text_03 = f.read()\n",
        "\n",
        "len(text_01), len(text_02), len(text_03)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lord_of_the_rings = text_01 + text_02 + text_03\n",
        "lord_of_the_rings = lord_of_the_rings.lower()\n",
        "len(lord_of_the_rings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hW9uZ3bSogD",
        "outputId": "81aebe96-4239-4001-c35f-9f2680492747"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2579946"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lord_of_the_rings[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CR0e4jmWTvqN",
        "outputId": "7cef2c19-4557-48d0-a4ec-2f8dcfce86ad"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'three rings for the elven-kings under the sky,\\n               seven for the dwarf-lords in their hal'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "ltRouz5SZYEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Convert the lord_of_the_rings string varrible to array which consist of characters from string.\n",
        "\n",
        "lord_of_the_rings_array = list(lord_of_the_rings)"
      ],
      "metadata": {
        "id": "-0EJx3DuXupe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lord_of_the_rings_array[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXjHWpmvYGPw",
        "outputId": "52df8f05-5d44-4515-fdb4-43d8238b5f68"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['t', 'h', 'r', 'e', 'e', ' ', 'r', 'i', 'n', 'g']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepair Train, Val and Test dataset"
      ],
      "metadata": {
        "id": "jTgDfREBZiLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Chunk the lord_of_the_rings_array with a sliding window.\n",
        "# Lenght of the sliding window should be 50 characters.\n",
        "# Create a 2 dimasional array called X to chunk.\n",
        "# And creat an y varrible which includes the each 50+1 elements.\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "X = []\n",
        "y = []\n",
        "window_size = 50\n",
        "\n",
        "for i in tqdm(range(0, len(lord_of_the_rings_array) - window_size)):\n",
        "  X.append(lord_of_the_rings_array[i:i + window_size])\n",
        "  y.append(lord_of_the_rings_array[i + window_size])\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stsCMrRQZUbX",
        "outputId": "33c40a73-0429-467c-eecf-f7e78b830ead"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2579896/2579896 [00:06<00:00, 401422.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2579896, 50)\n",
            "(2579896,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Split train val and test set from X, y arrays. X is the input and y is the label.\n",
        "# The ratio should be 80% 10% 10% and you dont shuffle the data when you split.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Assuming X and y are your input and label arrays\n",
        "\n",
        "train_split = int(0.8 * len(X))\n",
        "val_split = int(0.9 * len(X))\n",
        "\n",
        "X_train = X[:train_split]\n",
        "y_train = y[:train_split]\n",
        "\n",
        "X_val = X[train_split:val_split]\n",
        "y_val = y[train_split:val_split]\n",
        "\n",
        "X_test = X[val_split:]\n",
        "y_test = y[val_split:]\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"X_val shape:\", X_val.shape)\n",
        "print(\"y_val shape:\", y_val.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmtvgnTbW3P0",
        "outputId": "1c4d5d86-e09d-4d71-e144-7b95975ce474"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (2063916, 50)\n",
            "y_train shape: (2063916,)\n",
            "X_val shape: (257990, 50)\n",
            "y_val shape: (257990,)\n",
            "X_test shape: (257990, 50)\n",
            "y_test shape: (257990,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a character to index mapping\n",
        "char_to_index = {char: index for index, char in enumerate(sorted(list(set(lord_of_the_rings_array))))}\n",
        "index_to_char = {index: char for index, char in enumerate(sorted(list(set(lord_of_the_rings_array))))}\n",
        "\n",
        "# Convert training and validation data to numerical representation\n",
        "X_train_indices = [[char_to_index[char] for char in sequence] for sequence in tqdm(X_train)]\n",
        "y_train_indices = [char_to_index[char] for char in tqdm(y_train)]\n",
        "\n",
        "X_val_indices = [[char_to_index[char] for char in sequence] for sequence in tqdm(X_val)]\n",
        "y_val_indices = [char_to_index[char] for char in tqdm(y_val)]\n",
        "\n",
        "X_test_indices = [[char_to_index[char] for char in sequence] for sequence in tqdm(X_test)]\n",
        "y_test_indices = [char_to_index[char] for char in tqdm(y_test)]\n",
        "\n",
        "X_train_indices = np.array(X_train_indices)\n",
        "y_train_indices = np.array(y_train_indices)\n",
        "\n",
        "X_val_indices = np.array(X_val_indices)\n",
        "y_val_indices = np.array(y_val_indices)\n",
        "\n",
        "X_test_indices = np.array(X_test_indices)\n",
        "y_test_indices = np.array(y_test_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSh9Psf2jRx0",
        "outputId": "a28ba836-49b8-4f35-8cdc-60ce8c3933f2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2063916/2063916 [00:42<00:00, 48222.27it/s]\n",
            "100%|██████████| 2063916/2063916 [00:00<00:00, 2178382.37it/s]\n",
            "100%|██████████| 257990/257990 [00:03<00:00, 65278.46it/s]\n",
            "100%|██████████| 257990/257990 [00:00<00:00, 1775539.74it/s]\n",
            "100%|██████████| 257990/257990 [00:07<00:00, 34789.13it/s]\n",
            "100%|██████████| 257990/257990 [00:00<00:00, 1869424.77it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "Z0eyWOUZh1fs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Create an GRU network which can learn to generate the next caracter.\n",
        "# Use the tensorflow librarary. The trainin input is in X_train and label is the y_train.\n",
        "# The validataion dataset are X_val and y_val. Create a test generation a sample text in each epoch\n",
        "# that we can see the performance of model.\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "vocab_size = len(char_to_index)\n",
        "embedding_dim = 128\n",
        "rnn_units = 50\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "    tf.keras.layers.GRU(rnn_units, return_sequences=True),\n",
        "    tf.keras.layers.GRU(rnn_units),\n",
        "    tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# ## Training\n",
        "epochs = 2\n",
        "batch_size = 32\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "    model.fit(X_train_indices, y_train_indices, batch_size=batch_size, epochs=1, validation_data=(X_val_indices, y_val_indices))\n",
        "\n",
        "    # Generate a sample text\n",
        "    start_index = np.random.randint(0, len(X_test_indices) - 1)\n",
        "    # Convert generated_text to a regular Python string\n",
        "    generated_text = \"\".join(X_test[start_index])\n",
        "    ## print(generated_text)\n",
        "    for _ in tqdm(range(100)):\n",
        "        input_sequence = np.array([char_to_index[char] for char in generated_text[-window_size:]])\n",
        "        input_sequence = np.expand_dims(input_sequence, axis=0)\n",
        "\n",
        "        predicted_probabilities = model.predict(input_sequence, verbose=0)[0]\n",
        "        predicted_index = np.argmax(predicted_probabilities)\n",
        "        predicted_char = index_to_char[predicted_index]\n",
        "\n",
        "        generated_text += predicted_char\n",
        "\n",
        "    print(f\"Generated text:{generated_text}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrzLwAJOh3TL",
        "outputId": "3ed41383-f4e4-4acd-dd8a-45e0977fafc0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "\u001b[1m64498/64498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m530s\u001b[0m 8ms/step - accuracy: 0.4837 - loss: 1.7557 - val_accuracy: 0.5513 - val_loss: 1.4837\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:05<00:00, 17.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text:rious kinds they had not been missed, not yet at a strange the strange the strange the strange the strange the strange the strange the strange the str\n",
            "Epoch 2/2\n",
            "\u001b[1m64498/64498\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m525s\u001b[0m 8ms/step - accuracy: 0.5605 - loss: 1.4465 - val_accuracy: 0.5604 - val_loss: 1.4552\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:06<00:00, 16.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text:ve is given to the morning. and my heart forebodes of the strange the strange the strange the strange the strange the strange the strange the strange \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "PZG9_uRQwpRj",
        "outputId": "e1062b80-154e-4692-f8fc-d6cab4f9c063"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_5 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │           \u001b[38;5;34m8,704\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ gru_5 (\u001b[38;5;33mGRU\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)                  │          \u001b[38;5;34m27,000\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m68\u001b[0m)                  │           \u001b[38;5;34m3,468\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,704</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ gru_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">27,000</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">68</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">3,468</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m117,518\u001b[0m (459.06 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">117,518</span> (459.06 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m39,172\u001b[0m (153.02 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">39,172</span> (153.02 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m78,346\u001b[0m (306.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">78,346</span> (306.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Temperature"
      ],
      "metadata": {
        "id": "ywAxRJ8zvsEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Create sample code where i can setup the temprature of the generation on the\n",
        "# model prediciton.\n",
        "\n",
        "# ## Temperature\n",
        "\n",
        "def generate_text_with_temperature(model, start_sequence, length, temperature=1.0):\n",
        "    \"\"\"Generates text using the model with temperature.\"\"\"\n",
        "\n",
        "    generated_text = start_sequence\n",
        "    for _ in tqdm(range(length)):\n",
        "        input_sequence = np.array([char_to_index[char] for char in generated_text[-window_size:]])\n",
        "        input_sequence = np.expand_dims(input_sequence, axis=0)\n",
        "\n",
        "        predicted_probabilities = model.predict(input_sequence, verbose=0)[0]\n",
        "\n",
        "        # Apply temperature\n",
        "        predicted_probabilities = np.log(predicted_probabilities) / temperature\n",
        "        probabilities = np.exp(predicted_probabilities) / np.sum(np.exp(predicted_probabilities))\n",
        "\n",
        "        predicted_index = np.random.choice(len(probabilities), p=probabilities)\n",
        "        predicted_char = index_to_char[predicted_index]\n",
        "\n",
        "        generated_text += predicted_char\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Example usage with temperature:\n",
        "start_index = np.random.randint(0, len(X_test_indices) - 1)\n",
        "start_sequence = \"\".join(X_test[start_index])\n",
        "temp = 0.7  # Adjust temperature here\n",
        "generated_text = generate_text_with_temperature(model, start_sequence, 100, temperature=temp)  # Adjust temperature here\n",
        "print(f\"Generated text with temperature ({temp}):{generated_text}\")\n",
        "\n",
        "# You can experiment with different temperature values (e.g., 0.2, 0.7, 1.2) to see how it affects the generated text."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjverfY8vu8b",
        "outputId": "f3b52542-5967-453c-cfc0-b81598cc3fb8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:06<00:00, 16.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text with temperature (0.7):orget it till morning!'\n",
            "     the new 'chief' evide the passed on his dark at bramble of the kepilion, and the gandalf prassed a nart. there an eyes we\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: create code which save 2 tsv file the embedding layer from the model. The first consist of the characters and the second consist of the vectors to the characters. To separation use '\\t' character in the tsv file.\n",
        "\n",
        "# ... (Your existing code) ...\n",
        "\n",
        "embedding_layer = model.layers[0]\n",
        "embedding_weights = embedding_layer.get_weights()[0]\n",
        "\n",
        "# Create a list to store characters and their corresponding vectors\n",
        "chars = []\n",
        "vectors = []\n",
        "\n",
        "for char, index in list(char_to_index.items())[3:]:\n",
        "  chars.append(char)\n",
        "  vectors.append(embedding_weights[index])\n",
        "\n",
        "chars\n",
        "# Save characters to a TSV file\n",
        "with open(\"chars.tsv\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for char in chars:\n",
        "        f.write(f\"{char}\\n\")\n",
        "\n",
        "# Save vectors to a TSV file\n",
        "with open(\"vectors.tsv\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for vector in vectors:\n",
        "        vector_str = \"\\t\".join(map(str, vector))  # Join vector elements with tabs\n",
        "        f.write(f\"{vector_str}\\n\")\n",
        "\n",
        "# ... (Rest of your code) ..."
      ],
      "metadata": {
        "id": "aXwjddWcDqu3"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Labor IV. Linear Regression\n",
        "\n",
        "<img src=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62413fa0-3d80-411c-af93-ebd0f096a26a_1042x644.png\">"
      ],
      "metadata": {
        "id": "5h0hJ-jDXusx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## I have an idea. I do a sentiment analysis!\n",
        "\n",
        "<img src=\"https://gmu.ac.ae/wp-content/uploads/2017/03/idea.jpg\">"
      ],
      "metadata": {
        "id": "v4I7TngOY6KI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## I need some data!\n",
        "\n",
        "<img src=\"https://staging.herovired.com/wp-content/uploads/2023/04/What-Is-Data-Definition-01.webp\">\n",
        "\n",
        "## [Huggingface](https://huggingface.co/docs/datasets/index)\n",
        "\n",
        "- IMDB dataset: hf://datasets/scikit-learn/imdb/IMDB Dataset.csv"
      ],
      "metadata": {
        "id": "BOk1S7gdYpxv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsB9NgY7XtWx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "imdb_dataset = pd.read_csv(\"hf://datasets/scikit-learn/imdb/IMDB Dataset.csv\")\n",
        "imdb_dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text cleaning\n",
        "\n",
        "<img src=\"https://www.henryford.com/-/media/project/hfhs/henryford/henry-ford-blog/images/mobile-interior-banner-images/2019/02/bucket-of-cleaning-products.jpg\">"
      ],
      "metadata": {
        "id": "ae3k7lLkaWmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower case\n",
        "\n",
        "imdb_dataset[\"review\"] = imdb_dataset[\"review\"].apply(lambda x: x.lower())\n",
        "imdb_dataset.head()"
      ],
      "metadata": {
        "id": "naeIEF2uZ1Bg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove white spaces\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def remove_extra_whitespace(text):\n",
        "  \"\"\"Removes leading/trailing whitespace and replaces multiple spaces with single spaces.\"\"\"\n",
        "  text = text.strip()\n",
        "  text = \" \".join(text.split())\n",
        "  return text\n",
        "\n",
        "# Apply the function to the 'review' column with a progress bar\n",
        "imdb_dataset['review'] = [remove_extra_whitespace(review) for review in tqdm(imdb_dataset['review'], desc=\"Cleaning reviews\")]"
      ],
      "metadata": {
        "id": "3R1Im96DZnVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove special characters\n",
        "\n",
        "import re\n",
        "\n",
        "def remove_special_characters(text):\n",
        "  \"\"\"Removes special characters from the text.\"\"\"\n",
        "  text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
        "  return text\n",
        "\n",
        "# Apply the function to the 'review' column with a progress bar\n",
        "imdb_dataset['review'] = [remove_special_characters(review) for review in tqdm(imdb_dataset['review'], desc=\"Removing special characters\")]\n",
        "imdb_dataset.head()"
      ],
      "metadata": {
        "id": "fk0Kqzh8Zt_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def remove_html_tags(text):\n",
        "  \"\"\"Removes HTML tags from the text.\"\"\"\n",
        "  soup = BeautifulSoup(text, \"html.parser\")\n",
        "  return soup.get_text()\n",
        "\n",
        "# Apply the function to the 'review' column with a progress bar\n",
        "imdb_dataset['review'] = [remove_html_tags(review) for review in tqdm(imdb_dataset['review'], desc=\"Removing HTML tags\")]\n",
        "imdb_dataset.head()"
      ],
      "metadata": {
        "id": "5R5aui3gcPZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "id": "6qhP6SrXcsWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import contractions\n",
        "\n",
        "def expand_contractions(text):\n",
        "  \"\"\"Expands contractions in the text.\"\"\"\n",
        "  return contractions.fix(text)\n",
        "\n",
        "# Apply the function to the 'review' column with a progress bar\n",
        "imdb_dataset['review'] = [expand_contractions(review) for review in tqdm(imdb_dataset['review'], desc=\"Expanding contractions\")]\n",
        "imdb_dataset.head()"
      ],
      "metadata": {
        "id": "MlV4Pl4IcdOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing Punctuation\n",
        "\n",
        "import nltk\n",
        "import string\n",
        "\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def remove_punctuation(text):\n",
        "  \"\"\"Removes punctuation from a string.\"\"\"\n",
        "  translator = str.maketrans('', '', string.punctuation)\n",
        "  return text.translate(translator)\n",
        "\n",
        "\n",
        "for i in tqdm(range(len(imdb_dataset))):\n",
        "    imdb_dataset[\"review\"][i] = remove_punctuation(imdb_dataset[\"review\"][i])\n",
        "\n",
        "imdb_dataset.head()"
      ],
      "metadata": {
        "id": "BdGQOBNka6fY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Numbers\n",
        "\n",
        "def remove_numbers(text):\n",
        "  \"\"\"Removes numbers from a string.\"\"\"\n",
        "  result = ''.join([i for i in text if not i.isdigit()])\n",
        "  return result\n",
        "\n",
        "imdb_dataset[\"review\"] = [remove_numbers(review) for review in tqdm(imdb_dataset['review'], desc=\"Cleaning reviews\")]\n",
        "imdb_dataset.head()"
      ],
      "metadata": {
        "id": "FpyLWdKabQQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stop words\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  \"\"\"Removes stopwords from a string.\"\"\"\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  word_tokens = word_tokenize(text)\n",
        "  filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
        "  return \" \".join(filtered_sentence)\n",
        "\n",
        "imdb_dataset[\"review\"] = [remove_stopwords(review) for review in tqdm(imdb_dataset['review'], desc=\"Cleaning reviews\")]\n",
        "imdb_dataset.head()"
      ],
      "metadata": {
        "id": "HSWVgqMgbqfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "def lemmatize_text(text):\n",
        "  \"\"\"Lemmatizes words in a string.\"\"\"\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  word_tokens = word_tokenize(text)\n",
        "  lemmatized_sentence = [lemmatizer.lemmatize(w) for w in word_tokens]\n",
        "  return \" \".join(lemmatized_sentence)\n",
        "\n",
        "imdb_dataset[\"review\"] = [lemmatize_text(review) for review in tqdm(imdb_dataset['review'], desc=\"Cleaning reviews\")]\n",
        "imdb_dataset.head()"
      ],
      "metadata": {
        "id": "H2YlJ_OVb8Oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training, validation and test set\n",
        "\n",
        "<img src=\"https://www.brainstobytes.com/content/images/2020/01/Sets.png\">\n",
        "\n"
      ],
      "metadata": {
        "id": "iW3Xa27sg9Yy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Labeling\n",
        "\n",
        "imdb_dataset['sentiment'] = imdb_dataset['sentiment'].map({'positive': 1, 'negative': 0})\n",
        "imdb_dataset.head()"
      ],
      "metadata": {
        "id": "A_9Jo7oWelHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and test set\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and temporary sets (80% train, 20% temp)\n",
        "train_df, temp_df = train_test_split(imdb_dataset, test_size=0.2, random_state=42)\n",
        "\n",
        "# Split the temporary set into validation and test sets (50% validation, 50% test)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "\n",
        "# Now you have train_df, val_df, and test_df\n",
        "print(f\"Train set size: {len(train_df)}\")\n",
        "print(f\"Validation set size: {len(val_df)}\")\n",
        "print(f\"Test set size: {len(test_df)}\")"
      ],
      "metadata": {
        "id": "4chwFRO4eHXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CountVectorizer\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Initialize the TF-IDF vectorizer\n",
        "vectorizer = CountVectorizer(max_features=1000)\n",
        "\n",
        "# Fit and transform the training data\n",
        "train_df_cv = vectorizer.fit_transform(train_df['review']).toarray()\n",
        "\n",
        "# Transform the validation and test data using the same vectorizer\n",
        "val_df_cv = vectorizer.transform(val_df['review']).toarray()\n",
        "test_df_cv = vectorizer.transform(test_df['review']).toarray()"
      ],
      "metadata": {
        "id": "H3-BxZgKdqUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TFIDF\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize the TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)\n",
        "\n",
        "# Fit and transform the training data\n",
        "train_df_tfidf = tfidf_vectorizer.fit_transform(train_df['review'])\n",
        "\n",
        "# Transform the validation and test data using the same vectorizer\n",
        "val_df_tfidf = tfidf_vectorizer.transform(val_df['review'])\n",
        "test_df_tfidf = tfidf_vectorizer.transform(test_df['review'])"
      ],
      "metadata": {
        "id": "SbzPCwnLgoXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Labels\n",
        "\n",
        "train_df_labels = train_df['sentiment'].values\n",
        "val_df_labels = val_df['sentiment'].values\n",
        "test_df_labels = test_df['sentiment'].values"
      ],
      "metadata": {
        "id": "_-VQ0w5ljvbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelling\n",
        "\n",
        "<img src=\"https://images.spiceworks.com/wp-content/uploads/2022/04/11040521/46-4-e1715636469361.png\">\n",
        "\n"
      ],
      "metadata": {
        "id": "qftgX38kjWZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model CV\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define the model\n",
        "model_cv = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid', input_shape=(train_df_cv.shape[1],))\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_cv.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model_cv.fit(train_df_cv, train_df_labels, epochs=5, validation_data=(val_df_cv, val_df_labels))"
      ],
      "metadata": {
        "id": "q9sIWrHXjjVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model CV\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define the model\n",
        "model_tfidf = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Dense(1, activation='sigmoid', input_shape=(train_df_cv.shape[1],))\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model_tfidf.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model_tfidf.fit(train_df_cv, train_df_labels, epochs=5, validation_data=(val_df_cv, val_df_labels))"
      ],
      "metadata": {
        "id": "ARn3n12plwgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "loss_cv, accuracy_cv = model_cv.evaluate(test_df_cv, test_df_labels)\n",
        "print('Test accuracy (CV):', accuracy_cv)\n",
        "\n",
        "loss_tfidf, accuracy_tfidf = model_tfidf.evaluate(test_df_tfidf, test_df_labels)\n",
        "print('Test accuracy (TFIDF):', accuracy_tfidf)"
      ],
      "metadata": {
        "id": "fq6IAFYqlmnj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
---
title: "Advanced Natural Language Processing"
collection: teaching
type: "M.Sc course"
permalink: /teaching/NLP-A
venue: "University of Debrecen, Department of Data Science and Visualization"
date: 2024-09-04
location: "Debrecen, Hungary"
---

This course delves into advanced concepts of Natural Language Processing (NLP) and Machine Learning (ML) with a strong focus on modern deep learning techniques. It covers foundational topics such as tokenization, text representation, and pipelines, as well as cutting-edge research in large language models (LLMs), transformers, and their applications. The course emphasizes both theoretical understanding and practical implementation, preparing students to tackle real-world NLP challenges, including security, privacy, and human-centered design. During the semester, students will also have the opportunity to test and train these architectures on real data using cloud-based services [(Google Collab)](https://colab.google/).

======

## [Email address of the Teacher](mailto:lakatos.robert@inf.unideb.hu)

## [Attendance sheet](https://forms.cloud.microsoft/e/Ek0iSB0fjC?origin=lprLink)

## [Attendance sheet status](https://unidebhu-my.sharepoint.com/:x:/g/personal/lakatos_robert_inf_unideb_hu/EYKe7oC0eTVOkJXg2xjAZWoBENcHlhuEiFHCRYR9SxxmkA?e=ApkACz)

## Requirements

- Attendance sheet: Fewer absences than allowed. Active participation in classes.
- Create a working application, solve a real problem, and present it as a video using the solutions and models learned in class.
     - It must be uploaded to Github and shared.
     - Maximum length of video is 5-10 minutes.
     - In the video, each creator must present their own contribution. (for 3-8 minutes)
     - The application must be shown in action at the end of the video. (for 1-2 minutes)
- Organizing into teams (2-4 people) or working individually.
- If the creator(s) uses a service based on a generative language model to complete the task, they must attach the prompt log to the completed project as additional material.
- It is not certain that the team members receive a uniform grade, but they get grades proportionate to the task they have completed in the project.
- **Submission deadline: 2025.05.31**
- [**Submission form**](https://forms.office.com/e/kLRhQ8ZZj6?origin=lprLink)

## Lecture

- I.    [Tokenization](../materials/NLP-A/lectures/lesson_2)
- I.    [Text representation I.](../materials/NLP-A/lectures/lesson_3)
- I.    [Text representation II.](../materials/NLP-A/lectures/lesson_4)
- II.   [Large language models I. fancy-rnn](https://robertlakatos.github.io/me/materials/NLP-A/lectures/fancy-rnn.pdf)
- II.   [Large language models I. CNN-TreeRNN](https://robertlakatos.github.io/me/materials/NLP-A/lectures/CNN-TreeRNN.pdf)
- III.  [Large language models II. Basic](https://robertlakatos.github.io/me/materials/NLP-A/lectures/rnnlm.pdf)
- IV.   [Large language models II. Transformer](https://robertlakatos.github.io/me/materials/NLP-A/lectures/transformers.pdf)
- V.    [Pretrain](https://robertlakatos.github.io/me/materials/NLP-A/lectures/pretraining-updated.pdf)
- V.    [Question Answering](https://robertlakatos.github.io/me/materials/NLP-A/lectures/QA.pdf)
- VI.   [Post-training](https://robertlakatos.github.io/me/materials/NLP-A/lectures/instruction-tuning-rlhf.pdf)
- VI.   [Prompting RLHF](https://robertlakatos.github.io/me/materials/NLP-A/lectures/prompting-rlhf.pdf)
- VII.  [Life After DPO](https://robertlakatos.github.io/me/materials/NLP-A/lectures/life-after-dpo-lambert.pdf)
- VII.  [Training](https://robertlakatos.github.io/me/materials/NLP-A/lectures/training.pdf)
- VIII. [Efficient Adaptation](https://robertlakatos.github.io/me/materials/NLP-A/lectures/adaptation.pdf)
- IX.   [Hardware-aware Algorithms for Sequence Modeling](https://robertlakatos.github.io/me/materials/NLP-A/lectures/hardware-aware.pdf)
- X.    [Evaluation](https://robertlakatos.github.io/me/materials/NLP-A/lectures/evaluation.pdf)
- X.    [Natural Language to Code Generation](https://robertlakatos.github.io/me/materials/NLP-A/lectures/code-generation-pengcheng-yin.pdf)
- X.    [Security & Privacy of LLMs](https://robertlakatos.github.io/me/materials/NLP-A/lectures/security-and-privacy.pdf)
- XI.   [Human-Centered NLPF](https://robertlakatos.github.io/me/materials/NLP-A/lectures/human-centered-nlp.pdf)
- XI.   [Speech](https://robertlakatos.github.io/me/materials/NLP-A/lectures/speech-bci.pdf)
- XII.  [Agents](https://robertlakatos.github.io/me/materials/NLP-A/lectures/agents.pdf)
- XII.  [Linguistics Philosophy](https://robertlakatos.github.io/me/materials/NLP-A/lectures/linguistics-philosophy.pdf)
- XIII. [Open problems and discussion](https://robertlakatos.github.io/me/materials/NLP-A/lectures/open-problems.pdf)
- XIV.  [State of the art](https://robertlakatos.github.io/me/materials/NLP-A/lectures/state-of-the-art.pdf)

## Labor

### Basics

- I.    [Tokenization](https://colab.research.google.com/drive/1Z9XA9Ik9ofb_hk61mukq-P1X9JEpv1m4#scrollTo=gtFbC4F-H0h7)
- II.   [Embedded vectors](https://colab.research.google.com/drive/1RRurVdRdYblE9ShU8kwzpmkToAW67iIG#scrollTo=tsfToJX1g3gW)
- III.  [Recurent Neural Network](https://colab.research.google.com/drive/1aMlFLC8rAZm68qincUte6fNQvawUBTY4)

### Transformers

- IV    [Transformers](https://colab.research.google.com/drive/13ypN4qvbdMUFREZKAF-U1f0d0aYn7U_Z)
- V.    [GPT](https://colab.research.google.com/drive/1IL5zR6215l0WmTi84GorO5d-N1XQOPm9)
- VI.   [BERT](https://colab.research.google.com/drive/1QdXMVKzw0xxmIG9BrhGbbjIK37bksIK0)

### Efficient

- VII.  [Parameter Efficient Fine Tuning (PEFT)](https://colab.research.google.com/drive/1nIAO0-DsfZqo9SDz_9sTr7uzqK57dWQp)
- VIII. [Low Rank Adaptation (LoRA/PEFT)](https://colab.research.google.com/drive/1kXGgFkjzj7tT9ZNLZ1lnSo8UolYyc8SI) 

### LLM based AI application

- IX.   [Local AI](https://colab.research.google.com/drive/1Rlfd8uw0y-2epFxhSYTfvxo9Rz6LODfy#scrollTo=0dc4b4ae)
- X.    [Streaming-Thinking](https://colab.research.google.com/drive/1QmtXsa_zdxP3bl4DyQq7ZabrJckNHw_n)
- XI.   [Structured Output](https://colab.research.google.com/drive/1P56xUWbLxMQbCKa-sCvuo4DmLqXtSsK0)
- XII.  [WebSearch](https://colab.research.google.com/drive/1AQd_NhnLvfRBSIiwC6iPXr8YflUNmLNG)
- XIII. [Retrieval Augmented Generation](https://colab.research.google.com/drive/1kXGgFkjzj7tT9ZNLZ1lnSo8UolYyc8SI)

## Submitted

- [2025 Spring](../materials/NLP-A/submitted/2025-1)

## Usefull Links

- [Huggingface](https://huggingface.co/)
- [LlamaIndex](https://docs.llamaindex.ai/en/stable/)
- [Keras](https://keras.io/)
- [Tensorflow](https://www.tensorflow.org/)
- [Pytorch](https://pytorch.org/)
- [Pyton](https://www.python.org/)
- [Google Colab](https://colab.google/)

## Recommended Literatures and Courses

1.  [Jurafsky, Daniel, and James H. Martin. "Speech and language processing (draft)." Chapter A: Hidden Markov Models (Draft of September 11, 2018). Retrieved March 19 (2018): 2019.](https://ms.b-ok.xyz/book/3560643/4a6ab2)
2.  [Eisenstein, Jacob. "Introduction to natural language processing." MIT press, 2019.](https://mitpress.mit.edu/9780262042840/introduction-to-natural-language-processing/)
3.  [Goldberg, Yoav. "A primer on neural network models for natural language processing." Journal of Artificial Intelligence Research 57 (2016): 345-420.](https://arxiv.org/pdf/1510.00726.pdf)
4.  [Francois Chollet. "Deep Learning with Python"](https://www.amazon.com/Deep-Learning-Python-Francois-Chollet/dp/1617294438)
5.  [Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/chapter0/1?fw=pt)
6.  [MIT Introduction to Deep Learning](http://introtodeeplearning.com/)
7.  [Visual Guide to Transformer Neural Networks - (Episode 1)](⁠https://www.youtube.com/watch?v=dichIcUZfOw)
8.  [Visual Guide to Transformer Neural Networks - (Episode 2)](⁠https://www.youtube.com/watch?v=mMa2PmYJlCo)
9.  [Visual Guide to Transformer Neural Networks - (Episode 3)](⁠https://www.youtube.com/watch?v=gJ9kaJsE78k)
10. [Stanford CS 224N / Ling 280  —  Natural Language Processing](https://web.stanford.edu/class/cs224n/)

# Usefull Publications

[1] [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)

[2] [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)

[3] [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)

[4] [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)

[5] [Global Vectors for Node Representations](https://arxiv.org/pdf/1902.11004.pdf)
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DR9363grDVo"
      },
      "source": [
        "# [Train transformer](https://www.tensorflow.org/text/tutorials/transformer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXTGdF_frDVt"
      },
      "outputs": [],
      "source": [
        "# apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2\n",
        "# pip uninstall -y -q tensorflow keras tensorflow-estimator tensorflow-text\n",
        "# pip install protobuf~=3.20.3\n",
        "# pip install -q tensorflow_datasets\n",
        "!pip install -q -U tensorflow-text tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDNcXfU6rDVw"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import logging\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow_text\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRp8xkTwrDWE"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "po_wCK_hsKkm"
      },
      "outputs": [],
      "source": [
        "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en',\n",
        "                               with_info=True,\n",
        "                               as_supervised=True)\n",
        "\n",
        "train_examples, val_examples = examples['train'], examples['validation']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--Pys31RsUl5",
        "outputId": "12044f2b-163d-4a79-e311-507c7de8f0df"
      },
      "outputs": [],
      "source": [
        "for pt_examples, en_examples in train_examples.batch(3).take(1):\n",
        "  print('> Examples in Portuguese:')\n",
        "  for pt in pt_examples.numpy():\n",
        "    print(pt.decode('utf-8'))\n",
        "  print()\n",
        "\n",
        "  print('> Examples in English:')\n",
        "  for en in en_examples.numpy():\n",
        "    print(en.decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DixTOefusnpe",
        "outputId": "cd4b1099-93e4-4ea5-b63b-5bdb1a9c047c"
      },
      "outputs": [],
      "source": [
        "model_name = 'ted_hrlr_translate_pt_en_converter'\n",
        "tf.keras.utils.get_file(\n",
        "    f'{model_name}.zip',\n",
        "    f'https://storage.googleapis.com/download.tensorflow.org/models/{model_name}.zip',\n",
        "    cache_dir='.', cache_subdir='', extract=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I50DYTnNsr_n"
      },
      "outputs": [],
      "source": [
        "tokenizers = tf.saved_model.load(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xH8cllUFsvN_",
        "outputId": "f836a8f9-0ab4-4c62-827c-2248b2816f60"
      },
      "outputs": [],
      "source": [
        "[item for item in dir(tokenizers.en) if not item.startswith('_')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9LY1EtHsx5g",
        "outputId": "987e0f4f-5991-4945-fe76-d2ddff646f6f"
      },
      "outputs": [],
      "source": [
        "print('> This is a batch of strings:')\n",
        "for en in en_examples.numpy():\n",
        "  print(en.decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebdxwKZUs0UX",
        "outputId": "97031a37-3201-424d-cf3d-e0e266ee3a0d"
      },
      "outputs": [],
      "source": [
        "encoded = tokenizers.en.tokenize(en_examples)\n",
        "\n",
        "print('> This is a padded-batch of token IDs:')\n",
        "for row in encoded.to_list():\n",
        "  print(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kd1jLAEMs3d3",
        "outputId": "9090c599-46d8-4c12-b18c-3191cefc0898"
      },
      "outputs": [],
      "source": [
        "round_trip = tokenizers.en.detokenize(encoded)\n",
        "\n",
        "print('> This is human-readable text:')\n",
        "for line in round_trip.numpy():\n",
        "  print(line.decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yvxi67kMs572",
        "outputId": "a2132312-2ee3-4590-f3d7-8c197fbb99df"
      },
      "outputs": [],
      "source": [
        "print('> This is the text split into tokens:')\n",
        "tokens = tokenizers.en.lookup(encoded)\n",
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6A9fQEus9tt",
        "outputId": "67b441c2-d8d3-40d4-8d33-ba0578d7431c"
      },
      "outputs": [],
      "source": [
        "lengths = []\n",
        "\n",
        "for pt_examples, en_examples in train_examples.batch(1024):\n",
        "  pt_tokens = tokenizers.pt.tokenize(pt_examples)\n",
        "  lengths.append(pt_tokens.row_lengths())\n",
        "\n",
        "  en_tokens = tokenizers.en.tokenize(en_examples)\n",
        "  lengths.append(en_tokens.row_lengths())\n",
        "  print('.', end='', flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "IE3KZUu9tBBP",
        "outputId": "0fdbe992-36af-47c4-f5b6-5552c27d7af5"
      },
      "outputs": [],
      "source": [
        "all_lengths = np.concatenate(lengths)\n",
        "\n",
        "plt.hist(all_lengths, np.linspace(0, 500, 101))\n",
        "plt.ylim(plt.ylim())\n",
        "max_length = max(all_lengths)\n",
        "plt.plot([max_length, max_length], plt.ylim())\n",
        "plt.title(f'Maximum tokens per example: {max_length}');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dxs1JqV_tPUX"
      },
      "outputs": [],
      "source": [
        "MAX_TOKENS=128\n",
        "def prepare_batch(pt, en):\n",
        "    pt = tokenizers.pt.tokenize(pt)      # Output is ragged.\n",
        "    pt = pt[:, :MAX_TOKENS]    # Trim to MAX_TOKENS.\n",
        "    pt = pt.to_tensor()  # Convert to 0-padded dense Tensor\n",
        "\n",
        "    en = tokenizers.en.tokenize(en)\n",
        "    en = en[:, :(MAX_TOKENS+1)]\n",
        "    en_inputs = en[:, :-1].to_tensor()  # Drop the [END] tokens\n",
        "    en_labels = en[:, 1:].to_tensor()   # Drop the [START] tokens\n",
        "\n",
        "    return (pt, en_inputs), en_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZaJUTXEtR7O"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ShbGhdNtUzW"
      },
      "outputs": [],
      "source": [
        "def make_batches(ds):\n",
        "  return (\n",
        "      ds\n",
        "      .shuffle(BUFFER_SIZE)\n",
        "      .batch(BATCH_SIZE)\n",
        "      .map(prepare_batch, tf.data.AUTOTUNE)\n",
        "      .prefetch(buffer_size=tf.data.AUTOTUNE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku0uhwDItgV2"
      },
      "source": [
        "### Test the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iNwMnvCtXZQ"
      },
      "outputs": [],
      "source": [
        "train_batches = make_batches(train_examples)\n",
        "val_batches = make_batches(val_examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhQ2ydeNtmjP",
        "outputId": "a08f7c9d-0bf2-42d1-b988-a1fcac6797fb"
      },
      "outputs": [],
      "source": [
        "for (pt, en), en_labels in train_batches.take(1):\n",
        "  break\n",
        "\n",
        "print(pt.shape)\n",
        "print(en.shape)\n",
        "print(en_labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDBxfRYstoy8",
        "outputId": "d503d8cb-8591-4369-8810-caa729738965"
      },
      "outputs": [],
      "source": [
        "print(en[0][:10])\n",
        "print(en_labels[0][:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMLuSaDbt7vv"
      },
      "source": [
        "### Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQHuIpRPtv5O"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(length, depth):\n",
        "  depth = depth/2\n",
        "\n",
        "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
        "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
        "\n",
        "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
        "  angle_rads = positions * angle_rates      # (pos, depth)\n",
        "\n",
        "  pos_encoding = np.concatenate(\n",
        "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
        "      axis=-1)\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6QJJ263tzT3"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, d_model):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
        "    self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
        "\n",
        "  def compute_mask(self, *args, **kwargs):\n",
        "    return self.embedding.compute_mask(*args, **kwargs)\n",
        "\n",
        "  def call(self, x):\n",
        "    length = tf.shape(x)[1]\n",
        "    x = self.embedding(x)\n",
        "    # This factor sets the relative scale of the embedding and positonal_encoding.\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHrphbnbt09P"
      },
      "outputs": [],
      "source": [
        "embed_pt = PositionalEmbedding(vocab_size=tokenizers.pt.get_vocab_size(), d_model=512)\n",
        "embed_en = PositionalEmbedding(vocab_size=tokenizers.en.get_vocab_size(), d_model=512)\n",
        "\n",
        "pt_emb = embed_pt(pt)\n",
        "en_emb = embed_en(en)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hM6XQGZ6t3dm",
        "outputId": "2bebceeb-62f3-4870-85df-5c16afa429dd"
      },
      "outputs": [],
      "source": [
        "en_emb._keras_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0AMLJRcuDkH"
      },
      "source": [
        "### Attention layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6g0yaED3uGuG"
      },
      "outputs": [],
      "source": [
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "an8E529suJN1",
        "outputId": "c6548e67-2a59-49ca-c60a-0857d1c9ccea"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(BaseAttention):\n",
        "  def call(self, x, context):\n",
        "    attn_output, attn_scores = self.mha(\n",
        "        query=x,\n",
        "        key=context,\n",
        "        value=context,\n",
        "        return_attention_scores=True)\n",
        "\n",
        "    # Cache the attention scores for plotting later.\n",
        "    self.last_attn_scores = attn_scores\n",
        "\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "# Test\n",
        "sample_ca = CrossAttention(num_heads=2, key_dim=512)\n",
        "\n",
        "print(pt_emb.shape)\n",
        "print(en_emb.shape)\n",
        "print(sample_ca(en_emb, pt_emb).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dufn-fEYuWrP",
        "outputId": "8812bb72-5de9-46d0-80a1-dea20b3ee318"
      },
      "outputs": [],
      "source": [
        "class GlobalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x\n",
        "\n",
        "# Test\n",
        "sample_gsa = GlobalSelfAttention(num_heads=2, key_dim=512)\n",
        "\n",
        "print(pt_emb.shape)\n",
        "print(sample_gsa(pt_emb).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYEfG4RZueJF",
        "outputId": "ca67e8b4-a1cb-4f8d-8db8-c66fe02d66fd"
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x,\n",
        "        use_causal_mask = True)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x\n",
        "\n",
        "sample_csa = CausalSelfAttention(num_heads=2, key_dim=512)\n",
        "\n",
        "\n",
        "# Test\n",
        "print(en_emb.shape)\n",
        "print(sample_csa(en_emb).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaUfM0hZvNIG"
      },
      "source": [
        "## Feed Forward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRYTFCv-u9yQ",
        "outputId": "ea2439a7-2218-400d-8ba9-f35a1265f67b"
      },
      "outputs": [],
      "source": [
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model),\n",
        "      tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.add([x, self.seq(x)])\n",
        "    x = self.layer_norm(x)\n",
        "    return x\n",
        "\n",
        "# Test\n",
        "sample_ffn = FeedForward(512, 2048)\n",
        "\n",
        "print(en_emb.shape)\n",
        "print(sample_ffn(en_emb).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZHvJfzEvQLG"
      },
      "source": [
        "## Encoder - Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXqnvTGYwHqH"
      },
      "source": [
        "#### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMtk7l44vC9G",
        "outputId": "6dd73365-6b46-44c3-e7ad-a568babd9c3a"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attention = GlobalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.self_attention(x)\n",
        "    x = self.ffn(x)\n",
        "    return x\n",
        "\n",
        "sample_encoder_layer = EncoderLayer(d_model=512, num_heads=8, dff=2048)\n",
        "\n",
        "print(pt_emb.shape)\n",
        "print(sample_encoder_layer(pt_emb).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjexU35kvR-e",
        "outputId": "6bcb09a1-5c94-4725-ee1d-4a758b241511"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads,\n",
        "               dff, vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(\n",
        "        vocab_size=vocab_size, d_model=d_model)\n",
        "\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(d_model=d_model,\n",
        "                     num_heads=num_heads,\n",
        "                     dff=dff,\n",
        "                     dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "  def call(self, x):\n",
        "    # `x` is token-IDs shape: (batch, seq_len)\n",
        "    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "\n",
        "    # Add dropout.\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x)\n",
        "\n",
        "    return x  # Shape `(batch_size, seq_len, d_model)`.\n",
        "\n",
        "\n",
        "# Instantiate the encoder.\n",
        "sample_encoder = Encoder(num_layers=4,\n",
        "                         d_model=512,\n",
        "                         num_heads=8,\n",
        "                         dff=2048,\n",
        "                         vocab_size=8500)\n",
        "\n",
        "sample_encoder_output = sample_encoder(pt, training=False)\n",
        "\n",
        "# Print the shape.\n",
        "print(pt.shape)\n",
        "print(sample_encoder_output.shape)  # Shape `(batch_size, input_seq_len, d_model)`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jajR_ddkwMV1"
      },
      "source": [
        "#### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXL2dEd5vwq2",
        "outputId": "161bbda4-8b08-49fe-e772-7b3e9f7b17be"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,\n",
        "               *,\n",
        "               d_model,\n",
        "               num_heads,\n",
        "               dff,\n",
        "               dropout_rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.causal_self_attention = CausalSelfAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.cross_attention = CrossAttention(\n",
        "        num_heads=num_heads,\n",
        "        key_dim=d_model,\n",
        "        dropout=dropout_rate)\n",
        "\n",
        "    self.ffn = FeedForward(d_model, dff)\n",
        "\n",
        "  def call(self, x, context):\n",
        "    x = self.causal_self_attention(x=x)\n",
        "    x = self.cross_attention(x=x, context=context)\n",
        "\n",
        "    # Cache the last attention scores for plotting later\n",
        "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
        "\n",
        "    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "    return x\n",
        "\n",
        "# Test\n",
        "sample_decoder_layer = DecoderLayer(d_model=512, num_heads=8, dff=2048)\n",
        "\n",
        "sample_decoder_layer_output = sample_decoder_layer(\n",
        "    x=en_emb, context=pt_emb)\n",
        "\n",
        "print(en_emb.shape)\n",
        "print(pt_emb.shape)\n",
        "print(sample_decoder_layer_output.shape)  # `(batch_size, seq_len, d_model)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIpwR3THv2sX",
        "outputId": "2c54b01c-4dae-4053-871c-2ee65ff33d97"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
        "               dropout_rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
        "                                             d_model=d_model)\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dec_layers = [\n",
        "        DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
        "                     dff=dff, dropout_rate=dropout_rate)\n",
        "        for _ in range(num_layers)]\n",
        "\n",
        "    self.last_attn_scores = None\n",
        "\n",
        "  def call(self, x, context):\n",
        "    # `x` is token-IDs shape (batch, target_seq_len)\n",
        "    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    x = self.dropout(x)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x  = self.dec_layers[i](x, context)\n",
        "\n",
        "    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
        "\n",
        "    # The shape of x is (batch_size, target_seq_len, d_model).\n",
        "    return x\n",
        "\n",
        "\n",
        "# Instantiate the decoder.\n",
        "sample_decoder = Decoder(num_layers=4,\n",
        "                         d_model=512,\n",
        "                         num_heads=8,\n",
        "                         dff=2048,\n",
        "                         vocab_size=8000)\n",
        "\n",
        "output = sample_decoder(\n",
        "    x=en,\n",
        "    context=pt_emb)\n",
        "\n",
        "# Print the shapes.\n",
        "print(en.shape)\n",
        "print(pt_emb.shape)\n",
        "print(output.shape)\n",
        "\n",
        "sample_decoder.last_attn_scores.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8j9GbNgwYv-"
      },
      "source": [
        "### Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93ngCa9FwaYR",
        "outputId": "7fd04c45-42a9-4895-e062-97afdbfd7536"
      },
      "outputs": [],
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
        "               input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=input_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
        "                           num_heads=num_heads, dff=dff,\n",
        "                           vocab_size=target_vocab_size,\n",
        "                           dropout_rate=dropout_rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # To use a Keras model with `.fit` you must pass all your inputs in the\n",
        "    # first argument.\n",
        "    context, x  = inputs\n",
        "\n",
        "    context = self.encoder(context)  # (batch_size, context_len, d_model)\n",
        "\n",
        "    x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n",
        "\n",
        "    # Final linear layer output.\n",
        "    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
        "\n",
        "    try:\n",
        "      # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
        "      # b/250038731\n",
        "      del logits._keras_mask\n",
        "    except AttributeError:\n",
        "      pass\n",
        "\n",
        "    # Return the final output and the attention weights.\n",
        "    return logits\n",
        "\n",
        "\n",
        "num_layers = 4\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1\n",
        "\n",
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=tokenizers.pt.get_vocab_size().numpy(),\n",
        "    target_vocab_size=tokenizers.en.get_vocab_size().numpy(),\n",
        "    dropout_rate=dropout_rate)\n",
        "\n",
        "\n",
        "# Test\n",
        "output = transformer((pt, en))\n",
        "\n",
        "print(en.shape)\n",
        "print(pt.shape)\n",
        "print(output.shape)\n",
        "\n",
        "attn_scores = transformer.decoder.dec_layers[-1].last_attn_scores\n",
        "print(attn_scores.shape)  # (batch, heads, target_seq, input_seq)\n",
        "\n",
        "transformer.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYj1tyxmwx0W"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3muAbQ3Dx3vg"
      },
      "source": [
        "\\begin{align}\n",
        "\\Large{lrate = d_{model}^{-0.5} * \\min(step{\\_}num^{-0.5}, step{\\_}num \\cdot warmup{\\_}steps^{-1.5})}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2l9OuQ57xB9X"
      },
      "outputs": [],
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super().__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    step = tf.cast(step, dtype=tf.float32)\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrV5KqIIyDh2"
      },
      "outputs": [],
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
        "                                     beta_1=0.9,\n",
        "                                     beta_2=0.98,\n",
        "                                     epsilon=1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "TaD8BQOKyb2-",
        "outputId": "afa94059-2e36-4039-c04f-c2c18ad057ef"
      },
      "outputs": [],
      "source": [
        "plt.plot(learning_rate(tf.range(40000, dtype=tf.float32)))\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.xlabel('Train Step')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHjdo5_lypdz"
      },
      "outputs": [],
      "source": [
        "def masked_loss(label, pred):\n",
        "  mask = label != 0\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "  loss = loss_object(label, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss.dtype)\n",
        "  loss *= mask\n",
        "\n",
        "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "  return loss\n",
        "\n",
        "\n",
        "def masked_accuracy(label, pred):\n",
        "  pred = tf.argmax(pred, axis=2)\n",
        "  label = tf.cast(label, pred.dtype)\n",
        "  match = label == pred\n",
        "\n",
        "  mask = label != 0\n",
        "\n",
        "  match = match & mask\n",
        "\n",
        "  match = tf.cast(match, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(match)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NRCvlFNytn9"
      },
      "outputs": [],
      "source": [
        "transformer.compile(\n",
        "    loss=masked_loss,\n",
        "    optimizer=optimizer,\n",
        "    metrics=[masked_accuracy])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnnHfd5zyvHQ",
        "outputId": "cb54e419-1df0-4ac0-821f-2ddf889e58bf"
      },
      "outputs": [],
      "source": [
        "transformer.fit(train_batches,\n",
        "                epochs=1, # 20-nál kezd működni\n",
        "                validation_data=val_batches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kipdao2uyx1m"
      },
      "source": [
        "## Test Your Might"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LENvVXYNzSdW"
      },
      "outputs": [],
      "source": [
        "class Translator(tf.Module):\n",
        "  def __init__(self, tokenizers, transformer):\n",
        "    self.tokenizers = tokenizers\n",
        "    self.transformer = transformer\n",
        "\n",
        "  def __call__(self, sentence, max_length=MAX_TOKENS):\n",
        "    # The input sentence is Portuguese, hence adding the `[START]` and `[END]` tokens.\n",
        "    assert isinstance(sentence, tf.Tensor)\n",
        "    if len(sentence.shape) == 0:\n",
        "      sentence = sentence[tf.newaxis]\n",
        "\n",
        "    sentence = self.tokenizers.pt.tokenize(sentence).to_tensor()\n",
        "\n",
        "    encoder_input = sentence\n",
        "\n",
        "    # As the output language is English, initialize the output with the\n",
        "    # English `[START]` token.\n",
        "    start_end = self.tokenizers.en.tokenize([''])[0]\n",
        "    start = start_end[0][tf.newaxis]\n",
        "    end = start_end[1][tf.newaxis]\n",
        "\n",
        "    # `tf.TensorArray` is required here (instead of a Python list), so that the\n",
        "    # dynamic-loop can be traced by `tf.function`.\n",
        "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
        "    output_array = output_array.write(0, start)\n",
        "\n",
        "    for i in tf.range(max_length):\n",
        "      output = tf.transpose(output_array.stack())\n",
        "      predictions = self.transformer([encoder_input, output], training=False)\n",
        "\n",
        "      # Select the last token from the `seq_len` dimension.\n",
        "      predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
        "\n",
        "      predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "      # Concatenate the `predicted_id` to the output which is given to the\n",
        "      # decoder as its input.\n",
        "      output_array = output_array.write(i+1, predicted_id[0])\n",
        "\n",
        "      if predicted_id == end:\n",
        "        break\n",
        "\n",
        "    output = tf.transpose(output_array.stack())\n",
        "    # The output shape is `(1, tokens)`.\n",
        "    text = tokenizers.en.detokenize(output)[0]  # Shape: `()`.\n",
        "\n",
        "    tokens = tokenizers.en.lookup(output)[0]\n",
        "\n",
        "    # `tf.function` prevents us from using the attention_weights that were\n",
        "    # calculated on the last iteration of the loop.\n",
        "    # So, recalculate them outside the loop.\n",
        "    self.transformer([encoder_input, output[:,:-1]], training=False)\n",
        "    attention_weights = self.transformer.decoder.last_attn_scores\n",
        "\n",
        "    return text, tokens, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDzxeBxXzUu0"
      },
      "outputs": [],
      "source": [
        "translator = Translator(tokenizers, transformer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npS-VeIEzXOF"
      },
      "outputs": [],
      "source": [
        "def print_translation(sentence, tokens, ground_truth):\n",
        "  print(f'{\"Input:\":15s}: {sentence}')\n",
        "  print(f'{\"Prediction\":15s}: {tokens.numpy().decode(\"utf-8\")}')\n",
        "  print(f'{\"Ground truth\":15s}: {ground_truth}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjs26B2XzZyV",
        "outputId": "a88974a5-ebfe-4981-85ff-2a70ebd00189"
      },
      "outputs": [],
      "source": [
        "# Example 1\n",
        "\n",
        "sentence = 'este é um problema que temos que resolver.'\n",
        "ground_truth = 'this is a problem we have to solve .'\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(\n",
        "    tf.constant(sentence))\n",
        "print_translation(sentence, translated_text, ground_truth)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBcFJMKEza8c",
        "outputId": "015a5010-b876-4926-eb50-eac3eaa66382"
      },
      "outputs": [],
      "source": [
        "# Example 2\n",
        "sentence = 'os meus vizinhos ouviram sobre esta ideia.'\n",
        "ground_truth = 'and my neighboring homes heard about this idea .'\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(\n",
        "    tf.constant(sentence))\n",
        "print_translation(sentence, translated_text, ground_truth)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQEoVadb1JXW",
        "outputId": "b5da4c25-ad5a-4ae2-9a14-eb51e068b945"
      },
      "outputs": [],
      "source": [
        "# Example 3\n",
        "\n",
        "sentence = 'vou então muito rapidamente partilhar convosco algumas histórias de algumas coisas mágicas que aconteceram.'\n",
        "ground_truth = \"so i'll just share with you some stories very quickly of some magical things that have happened.\"\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(\n",
        "    tf.constant(sentence))\n",
        "print_translation(sentence, translated_text, ground_truth)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qr808c_51O7S"
      },
      "source": [
        "### Vizualizáció"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmsQo72y1RNo",
        "outputId": "795a0af4-f050-4aa1-e7e1-bd727b7a2ab6"
      },
      "outputs": [],
      "source": [
        "sentence = 'este é o primeiro livro que eu fiz.'\n",
        "ground_truth = \"this is the first book i've ever done.\"\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(\n",
        "    tf.constant(sentence))\n",
        "print_translation(sentence, translated_text, ground_truth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMmOzIst1T0Y"
      },
      "outputs": [],
      "source": [
        "def plot_attention_head(in_tokens, translated_tokens, attention):\n",
        "  # The model didn't generate `<START>` in the output. Skip it.\n",
        "  translated_tokens = translated_tokens[1:]\n",
        "\n",
        "  ax = plt.gca()\n",
        "  ax.matshow(attention)\n",
        "  ax.set_xticks(range(len(in_tokens)))\n",
        "  ax.set_yticks(range(len(translated_tokens)))\n",
        "\n",
        "  labels = [label.decode('utf-8') for label in in_tokens.numpy()]\n",
        "  ax.set_xticklabels(\n",
        "      labels, rotation=90)\n",
        "\n",
        "  labels = [label.decode('utf-8') for label in translated_tokens.numpy()]\n",
        "  ax.set_yticklabels(labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQ1hu44i1VPY",
        "outputId": "e814f998-5da3-4e44-a5ed-d40540e7583a"
      },
      "outputs": [],
      "source": [
        "head = 0\n",
        "# Shape: `(batch=1, num_heads, seq_len_q, seq_len_k)`.\n",
        "attention_heads = tf.squeeze(attention_weights, 0)\n",
        "attention = attention_heads[head]\n",
        "attention.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bK0lEmhg1WlV",
        "outputId": "131ac421-9581-4833-9dc9-ad907c9d46c0"
      },
      "outputs": [],
      "source": [
        "in_tokens = tf.convert_to_tensor([sentence])\n",
        "in_tokens = tokenizers.pt.tokenize(in_tokens).to_tensor()\n",
        "in_tokens = tokenizers.pt.lookup(in_tokens)[0]\n",
        "in_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxuShOrm1X2e",
        "outputId": "4871496f-43af-433d-b825-586d93f5a406"
      },
      "outputs": [],
      "source": [
        "translated_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "w7dIKboT1Y_u",
        "outputId": "c225ff78-5330-4416-8855-e17980e38dbe"
      },
      "outputs": [],
      "source": [
        "plot_attention_head(in_tokens, translated_tokens, attention)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATrYVHyP1a9D"
      },
      "outputs": [],
      "source": [
        "def plot_attention_weights(sentence, translated_tokens, attention_heads):\n",
        "  in_tokens = tf.convert_to_tensor([sentence])\n",
        "  in_tokens = tokenizers.pt.tokenize(in_tokens).to_tensor()\n",
        "  in_tokens = tokenizers.pt.lookup(in_tokens)[0]\n",
        "\n",
        "  fig = plt.figure(figsize=(16, 8))\n",
        "\n",
        "  for h, head in enumerate(attention_heads):\n",
        "    ax = fig.add_subplot(2, 4, h+1)\n",
        "\n",
        "    plot_attention_head(in_tokens, translated_tokens, head)\n",
        "\n",
        "    ax.set_xlabel(f'Head {h+1}')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "id": "LaM6wLPh1cK3",
        "outputId": "7df577fe-0f03-436d-80e8-ec9dcf9ea3e4"
      },
      "outputs": [],
      "source": [
        "plot_attention_weights(sentence,\n",
        "                       translated_tokens,\n",
        "                       attention_weights[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 784
        },
        "id": "24I8ljE61e4w",
        "outputId": "099a9755-3610-4eab-eaa7-e6c0559adb18"
      },
      "outputs": [],
      "source": [
        "sentence = 'Eu li sobre triceratops na enciclopédia.'\n",
        "ground_truth = 'I read about triceratops in the encyclopedia.'\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(\n",
        "    tf.constant(sentence))\n",
        "print_translation(sentence, translated_text, ground_truth)\n",
        "\n",
        "plot_attention_weights(sentence, translated_tokens, attention_weights[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AF4tWV21grV"
      },
      "source": [
        "### Export the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huRfXKe71jMp"
      },
      "outputs": [],
      "source": [
        "class ExportTranslator(tf.Module):\n",
        "  def __init__(self, translator):\n",
        "    self.translator = translator\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n",
        "  def __call__(self, sentence):\n",
        "    (result,\n",
        "     tokens,\n",
        "     attention_weights) = self.translator(sentence, max_length=MAX_TOKENS)\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LU-Z6MZA1kcL"
      },
      "outputs": [],
      "source": [
        "translator = ExportTranslator(translator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tf_dFsnz1mBW",
        "outputId": "45675000-70b3-4908-9c8d-72fa550ea8c7"
      },
      "outputs": [],
      "source": [
        "translator('este é o primeiro livro que eu fiz.').numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzmYVLIS1np_",
        "outputId": "b1c44bb6-eaab-4278-d631-960e6f880837"
      },
      "outputs": [],
      "source": [
        "tf.saved_model.save(translator, export_dir='translator')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOqFmoc41o2W"
      },
      "outputs": [],
      "source": [
        "reloaded = tf.saved_model.load('translator')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oP4rZxWN1p8e",
        "outputId": "9456b75e-917b-4f85-f8c3-b34079a69c36"
      },
      "outputs": [],
      "source": [
        "reloaded('este é o primeiro livro que eu fiz.').numpy()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

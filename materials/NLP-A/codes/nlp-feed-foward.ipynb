{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bbentwMQPqj"
      },
      "source": [
        "# Feed-Foward Layer. Dense Neural Network in the Trasnformer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LC5mUIwIQPqo"
      },
      "source": [
        "## Transformer architektúra\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/transformer.png\" alt=\"Transformer\">\n",
        "\n",
        "Ábra 1: Transformer architektúra. Forrás: [www.tensorflow.org](https://www.tensorflow.org/text/tutorials/transformer)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erdNKFqIRmDR"
      },
      "source": [
        "### Miért fontosak a transzformátorok?\n",
        "\n",
        "- A transzformátorok kiválóak a szekvenciális adatok, például a természetes nyelv modellezésében.\n",
        "\n",
        "- A visszatérő neurális hálózatokkal (RNN) ellentétben a transzformátorok párhuzamosíthatók. Ezáltal hatékonyak olyan hardvereken, mint a GPU-k és a TPU-k. Ennek fő oka az, hogy a Transformers az ismétlődést a figyelemre cserélte, és a számítások egyidejűleg is megtörténhetnek. A fóliakimenetek párhuzamosan is számíthatók, az RNN-hez hasonló sorozat helyett.\n",
        "\n",
        "- Ellentétben az RNN-ekkel (mint a seq2seq, 2014 ) vagy a konvolúciós neurális hálózatokkal (CNN-ekkel) (például a ByteNet ), a transzformátorok képesek megragadni a távoli vagy nagy hatótávolságú kontextusokat és függőségeket az adatokban a bemeneti vagy kimeneti szekvenciák távoli pozíciói között. Így hosszabb kapcsolatokat lehet megtanulni. A figyelem lehetővé teszi, hogy minden hely hozzáférjen az egyes rétegek teljes bemenetéhez, míg az RNN-ekben és a CNN-ekben az információnak sok feldolgozási lépésen kell keresztülmennie ahhoz, hogy nagy távolságra mozogjon, ami megnehezíti a tanulást.\n",
        "\n",
        "-A transzformátorok nem tesznek feltételezéseket az adatok időbeli/térbeli kapcsolatairól. Ez ideális objektumok (például StarCraft egységek ) feldolgozásához.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEJPyc8YR0Bx"
      },
      "source": [
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/encoder_self_attention_distribution.png\" alt=\"Transformer\">\n",
        "\n",
        "Ábra 2: Az „it” szó kódoló önfigyelem eloszlása ​​egy angol-francia fordításra képzett Transformer 5.-6. rétegében (a nyolc figyelemfej egyike).. Forrás: [www.Google Blog](https://blog-research-google.translate.goog/2017/08/transformer-novel-neural-network.html?_x_tr_sl=en&_x_tr_tl=hu&_x_tr_hl=hu&_x_tr_pto=wapp)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UfGJ9oTQPqq"
      },
      "source": [
        "## Feed Forward rétegek (Klassikus Mélye Neurális háló)\n",
        "\n",
        "A transzformátor ezt a pontonkénti előrecsatoló hálózatot is tartalmazza mind a kódolóban, mind a dekódolóban.\n",
        "\n",
        "1. <b>Átalakítás a pozíciók között:</b> A PWFFN réteg először bemeneti reprezentációkat kap, amelyeket különböző pozíciókban az input szekvencián belül szeretné átalakítani. Az átalakítás pozíció alapján történik, tehát az egyes pozíciókhoz tartozó reprezentációk függetlenek egymástól. Ez segít a modelleknek abban, hogy megőrizzék a szekvenciákban lévő pozícióspecifikus információkat.\n",
        "2. <b>Nemlineáris átalakítás:</b> Miután a bemeneti reprezentációkat pozíciós alapon átformálták, a PWFFN réteg egy nemlineáris aktiváló függvényen (általában ReLU) vezet át. Ez segít a modellnek abban, hogy komplexabb és nemlineáris kapcsolatokat ismerjen fel a tokenek között.\n",
        "3. <b>Összekapcsolás és reprezentáció-változás:</b> Végül a PWFFN réteg a kimeneti reprezentációkat kapja meg. Ezek a kimeneti reprezentációk már kifejezőbbek és tartalmazhatnak olyan mintázatokat vagy jellemzőket, amelyek segítenek a modelleknek jobban megérteni a bemeneti szekvenciákat és feladatokat.\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/FeedForward.png\" alt=\"Transformer\">\n",
        "\n",
        "Ábra 3: Attention rétegek. Forrás: [www.tensorflow.org](https://www.tensorflow.org/text/tutorials/transformer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WQQa-SXcRJY"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model),\n",
        "      tf.keras.layers.Dropout(dropout_rate)\n",
        "    ])\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.add([x, self.seq(x)])\n",
        "    x = self.layer_norm(x) \n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_ffn = FeedForward(512, 2048)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

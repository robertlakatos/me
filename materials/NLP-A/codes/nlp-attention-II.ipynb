{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bbentwMQPqj"
      },
      "source": [
        "# Figyelem (Attention) II.\n",
        "## Globális (global) és ok-okozati (casual) önfigyelem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LC5mUIwIQPqo"
      },
      "source": [
        "## Transformer architektúra\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/transformer.png\" alt=\"Transformer\">\n",
        "\n",
        "Ábra 1: Transformer architektúra. Forrás: [www.tensorflow.org](https://www.tensorflow.org/text/tutorials/transformer)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erdNKFqIRmDR"
      },
      "source": [
        "### Miért fontosak a transzformátorok?\n",
        "\n",
        "- A transzformátorok kiválóak a szekvenciális adatok, például a természetes nyelv modellezésében.\n",
        "\n",
        "- A visszatérő neurális hálózatokkal (RNN) ellentétben a transzformátorok párhuzamosíthatók. Ezáltal hatékonyak olyan hardvereken, mint a GPU-k és a TPU-k. Ennek fő oka az, hogy a Transformers az ismétlődést a figyelemre cserélte, és a számítások egyidejűleg is megtörténhetnek. A fóliakimenetek párhuzamosan is számíthatók, az RNN-hez hasonló sorozat helyett.\n",
        "\n",
        "- Ellentétben az RNN-ekkel (mint a seq2seq, 2014 ) vagy a konvolúciós neurális hálózatokkal (CNN-ekkel) (például a ByteNet ), a transzformátorok képesek megragadni a távoli vagy nagy hatótávolságú kontextusokat és függőségeket az adatokban a bemeneti vagy kimeneti szekvenciák távoli pozíciói között. Így hosszabb kapcsolatokat lehet megtanulni. A figyelem lehetővé teszi, hogy minden hely hozzáférjen az egyes rétegek teljes bemenetéhez, míg az RNN-ekben és a CNN-ekben az információnak sok feldolgozási lépésen kell keresztülmennie ahhoz, hogy nagy távolságra mozogjon, ami megnehezíti a tanulást.\n",
        "\n",
        "-A transzformátorok nem tesznek feltételezéseket az adatok időbeli/térbeli kapcsolatairól. Ez ideális objektumok (például StarCraft egységek ) feldolgozásához.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEJPyc8YR0Bx"
      },
      "source": [
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/encoder_self_attention_distribution.png\" alt=\"Transformer\">\n",
        "\n",
        "Ábra 2: Az „it” szó kódoló önfigyelem eloszlása ​​egy angol-francia fordításra képzett Transformer 5.-6. rétegében (a nyolc figyelemfej egyike).. Forrás: [www.Google Blog](https://blog-research-google.translate.goog/2017/08/transformer-novel-neural-network.html?_x_tr_sl=en&_x_tr_tl=hu&_x_tr_hl=hu&_x_tr_pto=wapp)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UfGJ9oTQPqq"
      },
      "source": [
        "## Attention rétegek\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/BaseAttention.png\" alt=\"Transformer\">\n",
        "\n",
        "Ábra 3: Attention rétegek. Forrás: [www.tensorflow.org](https://www.tensorflow.org/text/tutorials/transformer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7WQQa-SXcRJY"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHF43nNQcovJ"
      },
      "source": [
        "## Query, Key, Value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "f2cNb-XxcO6B"
      },
      "outputs": [],
      "source": [
        "d = {'color': 'blue', 'age': 22, 'type': 'pickup'}\n",
        "result = d['color']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wy8BSxVlcWK6"
      },
      "source": [
        "## Fuzzy , differenciálható , vektorizált szótár keresés\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/BaseAttention-new.png\" alt=\"Transformer\">\n",
        "\n",
        "Ábra 4: Query, Key, Value. Forrás: [www.tensorflow.org](https://www.tensorflow.org/text/tutorials/transformer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlV6B2nGePaR",
        "outputId": "cd975ae6-20a4-479e-a2a3-cc90b92c4c76"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(TensorShape([None, 8, 16]), TensorShape([None, 2, 8, 4]))"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "layer = tf.keras.layers.MultiHeadAttention(num_heads=2, key_dim=2)\n",
        "target = tf.keras.Input(shape=[8, 16])\n",
        "source = tf.keras.Input(shape=[4, 16])\n",
        "output_tensor, weights = layer(query=target,\n",
        "                               value=source,\n",
        "                               return_attention_scores=True)\n",
        "\n",
        "output_tensor.shape, weights.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4z3_94bhfS8y"
      },
      "source": [
        "## Multi Head Attention\n",
        "\n",
        "<img src=\"https://www.researchgate.net/publication/351019792/figure/fig1/AS:1014991599726592@1619004263146/Multi-Head-Attention-consists-of-several-Scaled-Dot-Product-Attention-layers-running.png\" alt=\"Transformer\">\n",
        "\n",
        "Ábra 5: Multi Head Attention. Forrás: [www.researchgate.net](https://www.researchgate.net).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ok-okozati önfigyelem réteg\n",
        "\n",
        "Az ok-okozati önfigyelés során egy token csak az előtte lévő tokenekre figyel, vagyis az időben előtte levő információkra. Ezt például időben sorban történő szekvenciák feldolgozására használják, például szövegekben, ahol a szavak sorrendje fontos. A causal self attention segít a modelleknek megérteni az idősorrendi kapcsolatokat és az időbeli összefüggéseket.\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/CausalSelfAttention.png\" alt=\"Transformer\">\n",
        "\n",
        "Ábra 8: A alkalmi önfigyelm réteg. Forrás: [www.tensorflow.org](https://www.tensorflow.org/text/tutorials/transformer)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/CausalSelfAttention-new-full.png\" alt=\"Transformer\">\n",
        "\n",
        "Ábra 9: A alkalmi önfigyelm réteg. Forrás: [www.tensorflow.org](https://www.tensorflow.org/text/tutorials/transformer)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "+ Az ok-okozati maszk biztosítja, hogy minden hely csak az előtte lévő helyekhez férhessen hozzá.\n",
        "+ A maradék kapcsolatokat az egyszerűség kedvéért itt is kihagytuk.\n",
        "+ Ennek a rétegnek a kompaktabb ábrázolása a következő lenne.\n",
        "+ A korai sorozatelemek kimenete nem függ a későbbi elemektől, így nem számít, hogy a réteg alkalmazása előtt vagy után vágja-e le az elemeket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CausalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x,\n",
        "        use_causal_mask = True)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_csa = CausalSelfAttention(num_heads=2, key_dim=512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ITLqQCwdJzR"
      },
      "source": [
        "## A kereszt figyelemi réteg\n",
        "\n",
        "A kereszteződő önfigyelésnél az input egyik része figyel a másik részre. Például, ha egy fordítás modellt használunk, a fordító rendszernek figyelmet kell fordítania a forrásnyelvű szavakra és az ezekhez tartozó fordításokra. A cross self attention lehetővé teszi a modellek számára, hogy kapcsolatot építsenek ki két különböző szekvencia között, és segítségével a forrásinformációkat a célinformációval összevetik.\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/CrossAttention.png\" alt=\"Transformer\">\n",
        "\n",
        "Ábra 6: A keresztfigyelem réteg. Forrás: [www.tensorflow.org](https://www.tensorflow.org/text/tutorials/transformer)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- A Transformer szó szerinti középpontjában a keresztfigyelem réteg található. Ez a réteg köti össze a kódolót és a dekódert. Ez a réteg a figyelem legegyszerűbb felhasználása a modellben."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUmHDfVPi0lh"
      },
      "source": [
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/CrossAttention-new-full.png\" alt=\"Transformer\">\n",
        "\n",
        "Ábra 7: A keresztfigyelem réteg. Forrás: [www.tensorflow.org](https://www.tensorflow.org/text/tutorials/transformer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LTluwg4d_Hy"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(BaseAttention):\n",
        "  def call(self, x, context):\n",
        "    attn_output, attn_scores = self.mha(\n",
        "        query=x,\n",
        "        key=context,\n",
        "        value=context,\n",
        "        return_attention_scores=True)\n",
        "\n",
        "    # Gyorsítótárazza a figyelempontszámokat későbbi ábrázoláshoz.\n",
        "    self.last_attn_scores = attn_scores\n",
        "\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "4JRvwWFjkLVx"
      },
      "outputs": [],
      "source": [
        "sample_ca = CrossAttention(num_heads=2, key_dim=512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/attention_map_portuguese.png\" alt=\"Transformer\">\n",
        "\n",
        "Ábra 8: Vizualizált figyelemsúlyok. Forrás: [www.Google Blog](https://blog-research-google.translate.goog/2017/08/transformer-novel-neural-network.html?_x_tr_sl=en&_x_tr_tl=hu&_x_tr_hl=hu&_x_tr_pto=wapp)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

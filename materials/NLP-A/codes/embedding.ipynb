{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d-T8hutgWM1"
      },
      "source": [
        "# Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDIycYyQhHqm"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsfToJX1g3gW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "splits = {'train': 'plain_text/train-00000-of-00001.parquet', 'test': 'plain_text/test-00000-of-00001.parquet', 'unsupervised': 'plain_text/unsupervised-00000-of-00001.parquet'}\n",
        "\n",
        "df_imdb_train = pd.read_parquet(\"hf://datasets/stanfordnlp/imdb/\" + splits[\"train\"])\n",
        "df_imdb_test = pd.read_parquet(\"hf://datasets/stanfordnlp/imdb/\" + splits[\"test\"])\n",
        "\n",
        "df_imdb = pd.concat([df_imdb_train, df_imdb_test])\n",
        "df_imdb.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the training data"
      ],
      "metadata": {
        "id": "EXs_Cvw5m4Bx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "97a8SYfUXMAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = text.lower().split()\n",
        "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "counter = Counter()\n",
        "for example in tqdm(df_imdb[\"text\"].values):\n",
        "    counter.update(tokenize(example))"
      ],
      "metadata": {
        "id": "pmwCAh2dXVON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {word: idx for idx, (word, _) in enumerate(counter.most_common(1000), 1)}\n",
        "vocab[\"<UNK>\"] = 0"
      ],
      "metadata": {
        "id": "R4SMReh8XhBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def context_target_pairs(text, window_size=2):\n",
        "    tokens = [vocab.get(token, 0) for token in tokenize(text)]\n",
        "    pairs = []\n",
        "    for i in range(window_size, len(tokens) - window_size):\n",
        "        context = tokens[i - window_size:i] + tokens[i + 1:i + 1 + window_size]\n",
        "        target = tokens[i]\n",
        "        pairs.append((context, target))\n",
        "    return pairs"
      ],
      "metadata": {
        "id": "_YXu_2jNYhaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs = []\n",
        "for text in tqdm(df_imdb[\"text\"].values):  # Csak egy részhalmazt veszünk a gyors tanítás érdekében\n",
        "    pairs.extend(context_target_pairs(text))"
      ],
      "metadata": {
        "id": "PY49_lV1Yssp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "class Word2VecDataset(Dataset):\n",
        "    def __init__(self, pairs):\n",
        "        self.pairs = pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        context, target = self.pairs[idx]\n",
        "        return torch.tensor(context, dtype=torch.long), torch.tensor(target, dtype=torch.long)\n",
        "\n",
        "train_dataset = Word2VecDataset(pairs)\n",
        "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)"
      ],
      "metadata": {
        "id": "UZz1op31ZWa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "items, labels = next(iter(train_loader))\n",
        "len(items), len(labels), items[0], labels[0]"
      ],
      "metadata": {
        "id": "gDrSVrFaZmjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the model\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*bBETsVNLyjnaFJgM9avkeQ.png\">"
      ],
      "metadata": {
        "id": "tiATWPbrm8yy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# CBOW\n",
        "class Word2Vec(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size):\n",
        "        super(Word2Vec, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embed_size)\n",
        "        self.linear = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def forward(self, context):\n",
        "        embeds = self.embeddings(context).mean(dim=1)\n",
        "        out = self.linear(embeds)\n",
        "        return out"
      ],
      "metadata": {
        "id": "YgpR2XNjaoCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 100\n",
        "model = Word2Vec(len(vocab), embed_size)"
      ],
      "metadata": {
        "id": "J8gusvA7aJ8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "device"
      ],
      "metadata": {
        "id": "tZlMBukldOiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "s9c9tJA_aLhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_loader)"
      ],
      "metadata": {
        "id": "rXZY4ExtbpJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "pbar = tqdm(train_loader)\n",
        "for epoch in range(epochs):\n",
        "    total_loss = []\n",
        "    for context, target in pbar:\n",
        "        context = context.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(context)\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss.append(loss.item())\n",
        "        avg_loss = sum(total_loss) / len(total_loss)\n",
        "        pbar.set_description(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "kRJNi1bWa3f_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the vectors"
      ],
      "metadata": {
        "id": "4TjSCH1-muWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_embeddings = model.embeddings.weight.cpu().detach().numpy()\n",
        "print(\"Beágyazás mátrix mérete:\", word_embeddings.shape)"
      ],
      "metadata": {
        "id": "7OqMHLYNjOeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "with open(\"vectors.tsv\", \"w\", newline='') as v_f, open(\"metadata.tsv\", \"w\", newline='') as m_f:\n",
        "  vector_writer = csv.writer(v_f, delimiter='\\t')\n",
        "  metadata_writer = csv.writer(m_f, delimiter='\\t')\n",
        "\n",
        "  for word, idx in vocab.items():\n",
        "    if idx < len(word_embeddings):\n",
        "      vector_writer.writerow(word_embeddings[idx])\n",
        "      metadata_writer.writerow([word])"
      ],
      "metadata": {
        "id": "SrasDU2TjdHg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
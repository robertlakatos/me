{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zysf6Ss4H0h0"
      },
      "source": [
        "# Tokeniz√°ci√≥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3J99-eFwUG7k"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtFbC4F-H0h7"
      },
      "source": [
        "## Character-based tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UxwXHBNH0h9"
      },
      "outputs": [],
      "source": [
        "sentence = \"I would like to work than machine lerning engineer at Google!\".lower()\n",
        "print(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXXJYDazH8_6"
      },
      "outputs": [],
      "source": [
        "sentence = sentence.replace(\" \",\"\")\n",
        "print(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ES5i9DKpH-RJ"
      },
      "outputs": [],
      "source": [
        "chars = [char for char in sentence]\n",
        "print(chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N39djy2yH_54"
      },
      "outputs": [],
      "source": [
        "chars = list(set(chars))\n",
        "print(chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCKUpLPXIEQh"
      },
      "outputs": [],
      "source": [
        "word_to_idx = {chars[i] : i for i in range(len(chars))}\n",
        "word_to_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmZJ8OIYH0h_"
      },
      "source": [
        "## WordLevel based tokenization\n",
        "\n",
        "This is the ‚Äúclassic‚Äù tokenization algorithm. You can map words to tokens. The advantage of this is that it is very easy to use and understand, but it requires an extremely large vocabulary for good coverage. This model will not make a direct selection; it simply maps the input words to tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZbzGOpXlx_r"
      },
      "source": [
        "### NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SwxpC32KCX0"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93hRbPCKI3DI"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "s = '''Good muffins cost $3.88\\nin New York.  Please buy me two of them.\\n\\nThanks.'''\n",
        "word_tokenize(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0osPsKnql1lk"
      },
      "source": [
        "### Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHOdUyeGl_Yu"
      },
      "outputs": [],
      "source": [
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "pre_tokenizer = Whitespace()\n",
        "pre_tokenizer.pre_tokenize_str(\"Hello! How are you? I'm fine, thank you.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2-jPwkBKRwg"
      },
      "source": [
        "## BPE\n",
        "\n",
        "One of the most popular subword tokenization algorithms. Byte-Pair-Encoding works by starting with characters and combining the most frequently seen ones to create new tokens. It then works iteratively to build new tokens from the most frequent pairs seen in the corpus. BPE can build words it has never seen before by using multiple subword tokens, so it requires a smaller vocabulary and is less likely to have ‚Äúunknown‚Äù tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHCtVEMZnuMz"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xn35koGHiBry"
      },
      "outputs": [],
      "source": [
        "corpus = dataset[\"train\"][\"text\"] + dataset[\"test\"][\"text\"] + dataset[\"validation\"][\"text\"]\n",
        "len(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfE97fmlH0iC"
      },
      "source": [
        "### Special tokens\n",
        "- [UNK] unknown token\n",
        "- [CLS] complete sentence token\n",
        "- [SEP] sentence separator token\n",
        "- [PAD] padding token, fixed input length padding token\n",
        "- [MASK] Masking token. e.g. \"Hello I'm a [MASK] model.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__JcBiSuH0iD"
      },
      "outputs": [],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "print(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyhIAhd3H0iE"
      },
      "outputs": [],
      "source": [
        "from tokenizers.trainers import BpeTrainer\n",
        "\n",
        "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "print(trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZ-RLtF-H0iG"
      },
      "outputs": [],
      "source": [
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "tokenizer.pre_tokenizer = Whitespace()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjyQtZ7XH0iI"
      },
      "outputs": [],
      "source": [
        "tokenizer.train_from_iterator(corpus, trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unjPa0WgH0iJ"
      },
      "outputs": [],
      "source": [
        "tokenizer.save(\"tokenizer-bpe-wiki.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hm2-G6y9H0iL"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer.from_file(\"tokenizer-bpe-wiki.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Nw7lkmTH0iN"
      },
      "outputs": [],
      "source": [
        "output = tokenizer.encode(\"Hello, y'all! How are you üòÅ ?\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23IyXe06H0iO"
      },
      "outputs": [],
      "source": [
        "print(output.tokens)\n",
        "print(output.ids)\n",
        "print(output.offsets[9])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLOQavm1H0iO"
      },
      "outputs": [],
      "source": [
        "tokenizer.token_to_id(\"[SEP]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5dcZQmKH0iP"
      },
      "outputs": [],
      "source": [
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "tokenizer.post_processor = TemplateProcessing(\n",
        "    single=\"[CLS] $A [SEP]\",\n",
        "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
        "    special_tokens=[\n",
        "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
        "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7unTQtzH0iR"
      },
      "outputs": [],
      "source": [
        "print(output.tokens)\n",
        "output = tokenizer.encode(\"Hello, y'all!\", \"How are you üòÅ ?\")\n",
        "print(output.tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPgAEjoaH0iS"
      },
      "outputs": [],
      "source": [
        "print(output.type_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2gJXlQIH0iT"
      },
      "source": [
        "## Encoding in a batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2a-zYcpH0iV"
      },
      "outputs": [],
      "source": [
        "tokenizer.enable_padding(pad_id=3, pad_token=\"[PAD]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vO3V_yLH0iV"
      },
      "outputs": [],
      "source": [
        "output = tokenizer.encode_batch([\"Hello, y'all!\", \"How are you üòÅ ?\"])\n",
        "print(output[0].tokens)\n",
        "print(output[1].tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3M6yvtxH0iW"
      },
      "outputs": [],
      "source": [
        "print(output[0].attention_mask)\n",
        "print(output[1].attention_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMNGhG6dH0iX"
      },
      "source": [
        "## Pretrained tokenizer, usage\n",
        "\n",
        "- BERT\n",
        "- WordPiece: This is a subword tokenization algorithm very similar to BPE, which is mainly used by Google in models like BERT. It uses a greedy algorithm that tries to build long words first. This is different from BPE, which starts with characters and builds tokens as large as possible. It uses the ## prefix to identify tokens that are part of a word (i.e. not the beginning of a word)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJLF9mPRrL7L"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://huggingface.co/nlpaueb/legal-bert-base-uncased/raw/main/vocab.txt\"\n",
        "response = requests.get(url)\n",
        "\n",
        "with open(\"bert-vocab.txt\", \"w\") as f:\n",
        "  f.write(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVHdul3xH0iY"
      },
      "outputs": [],
      "source": [
        "from tokenizers import BertWordPieceTokenizer\n",
        "\n",
        "tokenizer = BertWordPieceTokenizer(\"bert-vocab.txt\", lowercase=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSt6X5NaH0iY"
      },
      "outputs": [],
      "source": [
        "output = tokenizer.encode(\"Hello, y'all!\", \"How are you üòÅ ?\")\n",
        "print(output.tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YE_IDEOTskxb"
      },
      "source": [
        "### Building your own WordPiece\n",
        "\n",
        "Same as BPE, just use the WordPiece lib."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6IP5HEusiA3"
      },
      "outputs": [],
      "source": [
        "from tokenizers.models import WordPiece\n",
        "\n",
        "tokenizerWP = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
        "print(tokenizerWP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yb35ERn2kUdK"
      },
      "outputs": [],
      "source": [
        "from tokenizers.trainers import WordPieceTrainer\n",
        "\n",
        "trainerWP = WordPieceTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "print(trainerWP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNmrAO-Gjrij"
      },
      "outputs": [],
      "source": [
        "tokenizerWP.pre_tokenizer = Whitespace()\n",
        "tokenizerWP.train_from_iterator(corpus, trainerWP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBSxLtzGj2KV"
      },
      "outputs": [],
      "source": [
        "output = tokenizerWP.encode(\"Hello, y'all! How are you üòÅ ?\")\n",
        "print(output.tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROheze9rtdq4"
      },
      "source": [
        "## Unigram\n",
        "\n",
        "Unigram is also a subword tokenization algorithm, and works by trying to identify the best set of subword tokens to maximize the likelihood of a given sentence. It differs from BPE in that it is not deterministic, based on sequentially applied rules. Instead, Unigram will be able to compute multiple tokenization schemes while selecting the most likely one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjAfgDSethc7"
      },
      "outputs": [],
      "source": [
        "from tokenizers.models import Unigram"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('nlp')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 2,
    "vscode": {
      "interpreter": {
        "hash": "d5f66739f1dd32b1c14636a4c66467b6c7b0f6b88b09a96883d2a49b9cebc250"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bbentwMQPqj"
      },
      "source": [
        "# Figyelem (Attention) I.\n",
        "## Alap (base) és kereszt (cross) figyelem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LC5mUIwIQPqo"
      },
      "source": [
        "## Transformer architektúra\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/transformer.png\" alt=\"Transformer\">\n",
        "\n",
        "Ábra 1: Transformer architektúra. Forrás: [www.tensorflow.org](https://www.tensorflow.org/text/tutorials/transformer)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erdNKFqIRmDR"
      },
      "source": [
        "### Miért fontosak a transzformátorok?\n",
        "\n",
        "- A transzformátorok kiválóak a szekvenciális adatok, például a természetes nyelv modellezésében.\n",
        "\n",
        "- A visszatérő neurális hálózatokkal (RNN) ellentétben a transzformátorok párhuzamosíthatók. Ezáltal hatékonyak olyan hardvereken, mint a GPU-k és a TPU-k. Ennek fő oka az, hogy a Transformers az ismétlődést a figyelemre cserélte, és a számítások egyidejűleg is megtörténhetnek. A fóliakimenetek párhuzamosan is számíthatók, az RNN-hez hasonló sorozat helyett.\n",
        "\n",
        "- Ellentétben az RNN-ekkel (mint a seq2seq, 2014 ) vagy a konvolúciós neurális hálózatokkal (CNN-ekkel) (például a ByteNet ), a transzformátorok képesek megragadni a távoli vagy nagy hatótávolságú kontextusokat és függőségeket az adatokban a bemeneti vagy kimeneti szekvenciák távoli pozíciói között. Így hosszabb kapcsolatokat lehet megtanulni. A figyelem lehetővé teszi, hogy minden hely hozzáférjen az egyes rétegek teljes bemenetéhez, míg az RNN-ekben és a CNN-ekben az információnak sok feldolgozási lépésen kell keresztülmennie ahhoz, hogy nagy távolságra mozogjon, ami megnehezíti a tanulást.\n",
        "\n",
        "-A transzformátorok nem tesznek feltételezéseket az adatok időbeli/térbeli kapcsolatairól. Ez ideális objektumok (például StarCraft egységek ) feldolgozásához.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEJPyc8YR0Bx"
      },
      "source": [
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/encoder_self_attention_distribution.png\" alt=\"Transformer\">\n",
        "\n",
        "Ábra 2: Az „it” szó kódoló önfigyelem eloszlása ​​egy angol-francia fordításra képzett Transformer 5.-6. rétegében (a nyolc figyelemfej egyike). Forrás: [www.Google Blog](https://blog-research-google.translate.goog/2017/08/transformer-novel-neural-network.html?_x_tr_sl=en&_x_tr_tl=hu&_x_tr_hl=hu&_x_tr_pto=wapp)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UfGJ9oTQPqq"
      },
      "source": [
        "## Attention rétegek\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/BaseAttention.png\" alt=\"Transformer\">\n",
        "\n",
        "Ábra 3: Attention rétegek. Forrás: [www.tensorflow.org](https://www.tensorflow.org/text/tutorials/transformer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7WQQa-SXcRJY"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class BaseAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHF43nNQcovJ"
      },
      "source": [
        "## Query, Key, Value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "f2cNb-XxcO6B"
      },
      "outputs": [],
      "source": [
        "d = {'color': 'blue', 'age': 22, 'type': 'pickup'}\n",
        "result = d['color']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wy8BSxVlcWK6"
      },
      "source": [
        "## Fuzzy , differenciálható , vektorizált szótár keresés\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/BaseAttention-new.png\" alt=\"Transformer\">\n",
        "\n",
        "Ábra 4: Query, Key, Value. Forrás: [www.tensorflow.org](https://www.tensorflow.org/text/tutorials/transformer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlV6B2nGePaR",
        "outputId": "cd975ae6-20a4-479e-a2a3-cc90b92c4c76"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(TensorShape([None, 8, 16]), TensorShape([None, 2, 8, 4]))"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "layer = tf.keras.layers.MultiHeadAttention(num_heads=2, key_dim=2)\n",
        "target = tf.keras.Input(shape=[8, 16])\n",
        "source = tf.keras.Input(shape=[4, 16])\n",
        "output_tensor, weights = layer(query=target,\n",
        "                               value=source,\n",
        "                               return_attention_scores=True)\n",
        "\n",
        "output_tensor.shape, weights.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4z3_94bhfS8y"
      },
      "source": [
        "## Multi Head Attention\n",
        "\n",
        "<img src=\"https://www.researchgate.net/publication/351019792/figure/fig1/AS:1014991599726592@1619004263146/Multi-Head-Attention-consists-of-several-Scaled-Dot-Product-Attention-layers-running.png\" alt=\"Transformer\">\n",
        "\n",
        "Ábra 5: Multi Head Attention. Forrás: [www.researchgate.net](https://www.researchgate.net).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Globális Önfigyelem réteg\n",
        "\n",
        "A globális önfigyelés azt jelenti, hogy minden bemeneti token számára figyelmet számolunk ki a többi bemeneti tokenre. Az alapvető Transformer modellben a self-attention mechanizmus egy globális önfigyelést használ, ami azt jelenti, hogy minden tokennek van súlya, amely megmutatja, milyen mértékben figyel az összes többi tokenre a bemeneti szekvenciában. Ennek a mechanizmusnak a célja az összes lehetséges kapcsolat figyelembevétele, ami fontos lehet a feldolgozott szekvencia számára.\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/SelfAttention.png\" alt=\"Transformer\">\n",
        "\n",
        "Ábra 6: A globális önfigyelmi réteg. Forrás: [www.tensorflow.org](https://www.tensorflow.org/text/tutorials/transformer)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "+ Ez a réteg felelős a kontextusszekvencia feldolgozásáért és az információ terjesztéséért annak hosszában\n",
        "+ Mivel a szövegkörnyezeti sorrend rögzített a fordítás generálása közben, az információ mindkét irányban áramolhat.\n",
        "+ A transzformátorok és az önfigyelem előtt a modellek általában RNN-eket vagy CNN-eket használtak ehhez a feladathoz:\n",
        "+ Az RNN-nek és a CNN-nek azonban megvannak a korlátai:\n",
        "    + Az RNN lehetővé teszi az információ áramlását a sorozaton keresztül, de számos feldolgozási lépésen keresztül jut el oda (korlátozza a gradiens áramlását). Ezeket az RNN lépéseket egymás után kell futtatni, így az RNN kevésbé tudja kihasználni a modern párhuzamos eszközök előnyeit.\n",
        "    + A CNN-ben minden helyszín párhuzamosan feldolgozható, de ez csak korlátozott befogadó mezőt biztosít. A receptív mező csak lineárisan növekszik a CNN rétegek számával. Számos konvolúciós réteget kell egymásra raknia, hogy információt továbbítson a szekvencián (a Wavenet csökkenti ezt a problémát dilatált konvolúciókkal).\n",
        "+ A globális önfigyelem réteg viszont lehetővé teszi, hogy minden sorozatelem közvetlenül hozzáférjen minden más szekvenciaelemhez, csak néhány művelettel, és az összes kimenet párhuzamosan számítható."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/SelfAttention-new-full.png\" alt=\"Transformer\">\n",
        "\n",
        "Ábra 7: A Globális önfigyelem réteg. Forrás: [www.tensorflow.org](https://www.tensorflow.org/text/tutorials/transformer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GlobalSelfAttention(BaseAttention):\n",
        "  def call(self, x):\n",
        "    attn_output = self.mha(\n",
        "        query=x,\n",
        "        value=x,\n",
        "        key=x)\n",
        "    x = self.add([x, attn_output])\n",
        "    x = self.layernorm(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_gsa = GlobalSelfAttention(num_heads=2, key_dim=512)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

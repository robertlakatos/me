{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wlm8B3zRbSAb"
      },
      "source": [
        "# Pozició Beágyazás (Pos-Embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAD8plyFbSAh"
      },
      "source": [
        "## Transformer architektúra\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/transformer.png\" alt=\"Transformer\">\n",
        "\n",
        "Ábra 1: Transformer architektúra. Forrás: [www.tensorflow.org](https://www.tensorflow.org/text/tutorials/transformer)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xES9PCkKbSAi"
      },
      "source": [
        "## Beágyazó és pozíciókódoló rétegek\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/PositionalEmbedding.png\" alt=\"Transformer\">\n",
        "\n",
        "Ábra 2: Beágyazó és pozíciókódoló rétegek. Forrás: [www.tensorflow.org](https://www.tensorflow.org/text/tutorials/transformer)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-qdY795qDef"
      },
      "source": [
        "## Pozició Embedding\n",
        "\n",
        "A modellben használt figyelemrétegek bemenetüket vektorok halmazának tekintik, sorrend nélkül.\n",
        "Mivel a modell nem tartalmaz visszatérő vagy konvolúciós rétegeket.\n",
        "Valamilyen módra van szüksége a szórend azonosítására, különben a beviteli sorozatot egy zsáknyi szópéldányként látná,\n",
        "a hogyan vagy, hogy vagy, hogy vagy és így tovább, megkülönböztethetetlenek.\n",
        "\n",
        "\n",
        "A Transformer egy \"pozíciós kódolást\" ad hozzá a beágyazási vektorokhoz.\n",
        "Különböző frekvenciákon (a sorozaton keresztül) szinuszokat és koszinuszokat használ.\n",
        "A definíció szerint a közeli elemeknek hasonló pozíciókódolásuk lesz."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTUB7oWbqjof"
      },
      "source": [
        "## Pozició alapú beágyazás formula.\n",
        "\n",
        "<img src=\"https://i.stack.imgur.com/67ADh.png\">\n",
        "\n",
        "Ábra 6: Pozició alapú beágyazás formula.: [stackoverflow](https://i.stack.imgur.com/67ADh.png)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xiXj2fYGqDOh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def positional_encoding(length, depth):\n",
        "  depth = depth/2\n",
        "\n",
        "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
        "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
        "\n",
        "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
        "  angle_rads = positions * angle_rates      # (pos, depth)\n",
        "\n",
        "  pos_encoding = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)],axis=-1)\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "SxiTM2xYrMvM",
        "outputId": "7ba4d188-17fa-409a-f926-2a1302c1fad6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pos_encoding = positional_encoding(length=2048, depth=512)\n",
        "\n",
        "print(pos_encoding.shape)\n",
        "\n",
        "plt.pcolormesh(pos_encoding.numpy().T, cmap='RdBu')\n",
        "plt.ylabel('Mélység (dimenzió szám)')\n",
        "plt.xlabel('Pozició')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, d_model):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True) \n",
        "    self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
        "\n",
        "  def compute_mask(self, *args, **kwargs):\n",
        "    return self.embedding.compute_mask(*args, **kwargs)\n",
        "\n",
        "  def call(self, x):\n",
        "    length = tf.shape(x)[1]\n",
        "    x = self.embedding(x)\n",
        "    # Ez a tényező határozza meg a beágyazás és a pozitonális_kódolás relatív skáláját.\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embed_pt = PositionalEmbedding(vocab_size=100, d_model=512)\n",
        "embed_en = PositionalEmbedding(vocab_size=100, d_model=512)\n",
        "\n",
        "pt_emb = embed_pt(pt)\n",
        "en_emb = embed_en(en)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "en_emb._keras_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rq3Ssv69szFX"
      },
      "source": [
        "### Mondatok beágyazás transformerrel vagy hogyan működik a semantikus kereső?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtVjNYldtK6P"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import util\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrZdG30Ts85d",
        "outputId": "1d4042ec-8622-4521-c813-f7a0f998ddff"
      },
      "outputs": [],
      "source": [
        "sentences = [\"I'm happy\", \"I'm full of happiness\"]\n",
        "\n",
        "embedding_1= model.encode(sentences[0], convert_to_tensor=True)\n",
        "embedding_2 = model.encode(sentences[1], convert_to_tensor=True)\n",
        "\n",
        "util.pytorch_cos_sim(embedding_1, embedding_2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zysf6Ss4H0h0"
      },
      "source": [
        "# Tokenizáció\n",
        "\n",
        "- A mondatok szavakra, szórészekre vagy karakterekre történő darabolása\n",
        "- Fajtái:\n",
        "    - Karakter alapú\n",
        "    - Szó, alszó alapú\n",
        "- Cél: A feldolgozni kívánt adathalmaz szavakra vagy karakterekre történő tördelése olyan módon hogy az analízishez felhasznált gépi tanuló eljárás a saját szótára segítségével azt beazonosítani tudja.\n",
        "- Trade-off: méret vs hatékonyság"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3J99-eFwUG7k",
        "outputId": "71d23bb8-53fb-433e-9d52-035368179c35"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtFbC4F-H0h7"
      },
      "source": [
        "## Karakter alapú tokenizáció"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UxwXHBNH0h9",
        "outputId": "82b1ed77-4707-4b59-82c3-311297aaf39d"
      },
      "outputs": [],
      "source": [
        "sentence = \"I would like to work than machine lerning engineer at Google!\".lower()\n",
        "print(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXXJYDazH8_6",
        "outputId": "388cab9e-1a07-4a34-e856-576be66a1695"
      },
      "outputs": [],
      "source": [
        "sentence = sentence.replace(\" \",\"\")\n",
        "print(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ES5i9DKpH-RJ",
        "outputId": "3129e5a6-3588-4870-802d-7016a0045f39"
      },
      "outputs": [],
      "source": [
        "chars = [char for char in sentence]\n",
        "print(chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N39djy2yH_54",
        "outputId": "785bc4b1-0940-473d-a0e5-ba5e9483bff8"
      },
      "outputs": [],
      "source": [
        "chars = list(set(chars))\n",
        "print(chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCKUpLPXIEQh",
        "outputId": "60a6510d-ec51-4cf8-e3d3-3db86bec6f6f"
      },
      "outputs": [],
      "source": [
        "word_to_idx = {chars[i] : i for i in range(len(chars))}\n",
        "word_to_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmZJ8OIYH0h_"
      },
      "source": [
        "## WordLevel alapú tokenizáció\n",
        "\n",
        "Ez a „klasszikus” tokenizációs algoritmus. Egyszerűen leképezheti a szavakat az azonosítókra anélkül, hogy bármi különösebb lenne. Ennek az az előnye, hogy nagyon egyszerűen használható és érthető, de a jó lefedettséghez rendkívül nagy szókincsre van szükség. Ez a modell nem fog közvetlenül választani, egyszerűen leképezi a bemeneti tokeneket az azonosítókra."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZbzGOpXlx_r"
      },
      "source": [
        "### NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SwxpC32KCX0",
        "outputId": "62b4763a-3016-47b5-9d75-821e4e3b1081"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93hRbPCKI3DI",
        "outputId": "56fc4f0f-bcac-4dbf-ea60-1a5a38704f28"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "s = '''Good muffins cost $3.88\\nin New York.  Please buy me two of them.\\n\\nThanks.'''\n",
        "word_tokenize(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0osPsKnql1lk"
      },
      "source": [
        "### Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHOdUyeGl_Yu",
        "outputId": "553c5db1-6cf2-452a-aff8-974aed076dac"
      },
      "outputs": [],
      "source": [
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "pre_tokenizer = Whitespace()\n",
        "pre_tokenizer.pre_tokenize_str(\"Hello! How are you? I'm fine, thank you.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2-jPwkBKRwg"
      },
      "source": [
        "## BPE\n",
        "\n",
        "Az egyik legnépszerűbb alszó tokenizációs algoritmus. A Byte-Pair-Encoding úgy működik, hogy karakterekkel kezdődik, miközben a leggyakrabban láthatókat egyesíti, így új tokeneket hoz létre. Ezután iteratívan dolgozik, hogy új tokeneket építsen a korpuszban látható leggyakoribb párokból. A BPE olyan szavakat tud felépíteni, amelyeket soha nem látott több részszó token használatával, ezért kisebb szókincsre van szüksége, és kisebb az esélye annak, hogy „unk” (ismeretlen) tokenjei vannak."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHCtVEMZnuMz",
        "outputId": "39039f96-8772-4dd7-8c38-1c736c3b3c2b"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xn35koGHiBry",
        "outputId": "e86b1e14-3f06-4efa-df82-1665e81569fd"
      },
      "outputs": [],
      "source": [
        "corpus = dataset[\"train\"][\"text\"] + dataset[\"test\"][\"text\"] + dataset[\"validation\"][\"text\"]\n",
        "len(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfE97fmlH0iC"
      },
      "source": [
        "### Speciális tokenek\n",
        "- [UNK] ismeretlen token jelölése\n",
        "- [CLS] teljes mondatot reprezentáló token\n",
        "- [SEP] mondat szeparátor token\n",
        "- [PAD] padding token a fix input hossz feltöltését biztosító token\n",
        "- [MASK] Maszkolást biztosító token. pl.: \"Hello I'm a [MASK] model.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__JcBiSuH0iD",
        "outputId": "7e071386-7c5e-4154-9af0-e0e345bdcc12"
      },
      "outputs": [],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "\n",
        "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "print(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyhIAhd3H0iE",
        "outputId": "f0345056-bef4-4734-c2e3-76ce8150f1a4"
      },
      "outputs": [],
      "source": [
        "from tokenizers.trainers import BpeTrainer\n",
        "\n",
        "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "print(trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZ-RLtF-H0iG"
      },
      "outputs": [],
      "source": [
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "tokenizer.pre_tokenizer = Whitespace()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "AjyQtZ7XH0iI",
        "outputId": "f305737a-b378-47ae-e9cf-ca51962fe2c0"
      },
      "outputs": [],
      "source": [
        "tokenizer.train_from_iterator(corpus, trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unjPa0WgH0iJ"
      },
      "outputs": [],
      "source": [
        "tokenizer.save(\"tokenizer-bpe-wiki.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hm2-G6y9H0iL"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer.from_file(\"tokenizer-bpe-wiki.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "9Nw7lkmTH0iN",
        "outputId": "2105245a-5619-47cf-f7e4-250da8955f83"
      },
      "outputs": [],
      "source": [
        "output = tokenizer.encode(\"Hello, y'all! How are you 😁 ?\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23IyXe06H0iO",
        "outputId": "b93a0e53-5b44-4b65-e79b-62522a5957d5"
      },
      "outputs": [],
      "source": [
        "print(output.tokens)\n",
        "print(output.ids)\n",
        "print(output.offsets[9])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLOQavm1H0iO",
        "outputId": "fa177b9c-a4e4-4d52-b458-7d6f0fd1baa3"
      },
      "outputs": [],
      "source": [
        "tokenizer.token_to_id(\"[SEP]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5dcZQmKH0iP"
      },
      "outputs": [],
      "source": [
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "tokenizer.post_processor = TemplateProcessing(\n",
        "    single=\"[CLS] $A [SEP]\",\n",
        "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
        "    special_tokens=[\n",
        "        (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
        "        (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7unTQtzH0iR",
        "outputId": "ad5c26c2-e939-47fc-d5d4-70075b041733"
      },
      "outputs": [],
      "source": [
        "print(output.tokens)\n",
        "output = tokenizer.encode(\"Hello, y'all!\", \"How are you 😁 ?\")\n",
        "print(output.tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPgAEjoaH0iS",
        "outputId": "fc7d1656-2b2d-4b52-ebec-5ca9944b686e"
      },
      "outputs": [],
      "source": [
        "print(output.type_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2gJXlQIH0iT"
      },
      "source": [
        "## Encoding in a batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2a-zYcpH0iV"
      },
      "outputs": [],
      "source": [
        "tokenizer.enable_padding(pad_id=3, pad_token=\"[PAD]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vO3V_yLH0iV",
        "outputId": "d551fd9b-73d8-43fd-9225-f2a0629b8b50"
      },
      "outputs": [],
      "source": [
        "output = tokenizer.encode_batch([\"Hello, y'all!\", \"How are you 😁 ?\"])\n",
        "print(output[0].tokens)\n",
        "print(output[1].tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3M6yvtxH0iW",
        "outputId": "225b8457-c2a7-4af5-a947-80b849f3b1fc"
      },
      "outputs": [],
      "source": [
        "print(output[0].attention_mask)\n",
        "print(output[1].attention_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMNGhG6dH0iX"
      },
      "source": [
        "## Pretrained tokenizer, használata\n",
        "\n",
        "- BERT\n",
        "- WordPiece: Ez a BPE-hez nagyon hasonló részszó-tokenizációs algoritmus, amelyet főként a Google használ olyan modellekben, mint a BERT. Mohó algoritmust használ, amely először hosszú szavakat próbál felépíteni. Ez eltér a BPE-től, amely karakterekből indul ki, és minél nagyobb tokeneket épít. A híres ## előtagot használja a szó részét képező jelzők azonosítására (azaz nem kezdődő szó)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJLF9mPRrL7L"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://huggingface.co/nlpaueb/legal-bert-base-uncased/raw/main/vocab.txt\"\n",
        "response = requests.get(url)\n",
        "\n",
        "with open(\"bert-vocab.txt\", \"w\") as f:\n",
        "  f.write(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVHdul3xH0iY"
      },
      "outputs": [],
      "source": [
        "from tokenizers import BertWordPieceTokenizer\n",
        "\n",
        "tokenizer = BertWordPieceTokenizer(\"bert-vocab.txt\", lowercase=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSt6X5NaH0iY",
        "outputId": "d83d040e-5ea0-4b82-aa30-1f4e53d55c60"
      },
      "outputs": [],
      "source": [
        "output = tokenizer.encode(\"Hello, y'all!\", \"How are you 😁 ?\")\n",
        "print(output.tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YE_IDEOTskxb"
      },
      "source": [
        "### Saját WordPiece építése\n",
        "Ugyanúgy mint a BPE-nél csak használjuk a WordPiece libet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6IP5HEusiA3"
      },
      "outputs": [],
      "source": [
        "from tokenizers.models import WordPiece\n",
        "\n",
        "tokenizerWP = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
        "print(tokenizerWP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yb35ERn2kUdK",
        "outputId": "381fee68-adb7-41fa-a8c5-43410469d82a"
      },
      "outputs": [],
      "source": [
        "from tokenizers.trainers import WordPieceTrainer\n",
        "\n",
        "trainerWP = WordPieceTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
        "print(trainerWP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNmrAO-Gjrij"
      },
      "outputs": [],
      "source": [
        "tokenizerWP.pre_tokenizer = Whitespace()\n",
        "tokenizerWP.train_from_iterator(corpus, trainerWP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBSxLtzGj2KV",
        "outputId": "baf9b39f-8c57-4d3f-fd24-8a1f87851897"
      },
      "outputs": [],
      "source": [
        "output = tokenizerWP.encode(\"Hello, y'all! How are you 😁 ?\")\n",
        "print(output.tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROheze9rtdq4"
      },
      "source": [
        "## Unigram\n",
        "Az Unigram egy részszó tokenizációs algoritmus is, és úgy működik, hogy megpróbálja azonosítani a legjobb részszó tokenek készletet, hogy maximalizálja az adott mondat valószínűségét. Ez abban különbözik a BPE-től, hogy nem determinisztikus, szekvenciálisan alkalmazott szabályok alapján. Ehelyett az Unigram képes lesz többféle tokenizálási módot kiszámítani, miközben kiválasztja a legvalószínűbbet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjAfgDSethc7"
      },
      "outputs": [],
      "source": [
        "from tokenizers.models import Unigram"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('nlp')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 2,
    "vscode": {
      "interpreter": {
        "hash": "d5f66739f1dd32b1c14636a4c66467b6c7b0f6b88b09a96883d2a49b9cebc250"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

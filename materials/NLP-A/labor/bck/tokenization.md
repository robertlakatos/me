---
title: "Tokenization"
collection: teaching
type: "M.Sc course"
permalink: /materials/NLP-A/labor/tokenization
venue: "University of Debrecen, Department of Data Science and Visualization"
date: 2024-02-27
location: "Debrecen, Hungary"
---

# Tokeniz√°ci√≥

- A mondatok szavakra, sz√≥r√©szekre vagy karakterekre t√∂rt√©n≈ë darabol√°sa
- Fajt√°i:
    - Karakter alap√∫
    - Sz√≥, alsz√≥ alap√∫
- C√©l: A feldolgozni k√≠v√°nt adathalmaz szavakra vagy karakterekre t√∂rt√©n≈ë t√∂rdel√©se olyan m√≥don hogy az anal√≠zishez felhaszn√°lt g√©pi tanul√≥ elj√°r√°s a saj√°t sz√≥t√°ra seg√≠ts√©g√©vel azt beazonos√≠tani tudja.
- Trade-off: m√©ret vs hat√©konys√°g

```python
!pip install transformers
!pip install datasets
```

## Karakter alap√∫ tokeniz√°ci√≥

```python
sentence = "I would like to work than machine lerning engineer at Google!".lower()
print(sentence)
```

```python
sentence = sentence.replace(" ","")
print(sentence)
```

```python
chars = [char for char in sentence]
print(chars)
```

```python
chars = list(set(chars))
print(chars)
```

```python
word_to_idx = {chars[i] : i for i in range(len(chars))}
word_to_idx
```

## WordLevel alap√∫ tokeniz√°ci√≥

Ez a ‚Äûklasszikus‚Äù tokeniz√°ci√≥s algoritmus. Egyszer≈±en lek√©pezheti a szavakat az azonos√≠t√≥kra an√©lk√ºl, hogy b√°rmi k√ºl√∂n√∂sebb lenne. Ennek az az el≈ënye, hogy nagyon egyszer≈±en haszn√°lhat√≥ √©s √©rthet≈ë, de a j√≥ lefedetts√©ghez rendk√≠v√ºl nagy sz√≥kincsre van sz√ºks√©g. Ez a modell nem fog k√∂zvetlen√ºl v√°lasztani, egyszer≈±en lek√©pezi a bemeneti tokeneket az azonos√≠t√≥kra.

### NLTK

```python
import nltk
nltk.download('punkt')
```

```python
from nltk.tokenize import word_tokenize

s = '''Good muffins cost $3.88\nin New York.  Please buy me two of them.\n\nThanks.'''
word_tokenize(s)
```

### Hugging Face

```python
from tokenizers.pre_tokenizers import Whitespace

pre_tokenizer = Whitespace()
pre_tokenizer.pre_tokenize_str("Hello! How are you? I'm fine, thank you.")
```

## BPE

Az egyik legn√©pszer≈±bb alsz√≥ tokeniz√°ci√≥s algoritmus. A Byte-Pair-Encoding √∫gy m≈±k√∂dik, hogy karakterekkel kezd≈ëdik, mik√∂zben a leggyakrabban l√°that√≥kat egyes√≠ti, √≠gy √∫j tokeneket hoz l√©tre. Ezut√°n iterat√≠van dolgozik, hogy √∫j tokeneket √©p√≠tsen a korpuszban l√°that√≥ leggyakoribb p√°rokb√≥l. A BPE olyan szavakat tud fel√©p√≠teni, amelyeket soha nem l√°tott t√∂bb r√©szsz√≥ token haszn√°lat√°val, ez√©rt kisebb sz√≥kincsre van sz√ºks√©ge, √©s kisebb az es√©lye annak, hogy ‚Äûunk‚Äù (ismeretlen) tokenjei vannak.

```python
from datasets import load_dataset

dataset = load_dataset("wikitext", "wikitext-103-raw-v1")
dataset
```

```python
corpus = dataset["train"]["text"] + dataset["test"]["text"] + dataset["validation"]["text"]
len(corpus)
```

### Speci√°lis tokenek
- [UNK] ismeretlen token jel√∂l√©se
- [CLS] teljes mondatot reprezent√°l√≥ token
- [SEP] mondat szepar√°tor token
- [PAD] padding token a fix input hossz felt√∂lt√©s√©t biztos√≠t√≥ token
- [MASK] Maszkol√°st biztos√≠t√≥ token. pl.: "Hello I'm a [MASK] model."

```python
from tokenizers import Tokenizer
from tokenizers.models import BPE

tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
print(tokenizer)
```

```python
from tokenizers.trainers import BpeTrainer

trainer = BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])
print(trainer) 
```

```python
from tokenizers.pre_tokenizers import Whitespace

tokenizer.pre_tokenizer = Whitespace()
```

```python
tokenizer.train_from_iterator(corpus, trainer)
```

```python
tokenizer.save("tokenizer-bpe-wiki.json") 
```

```python
tokenizer = Tokenizer.from_file("tokenizer-bpe-wiki.json")
```

```python
output = tokenizer.encode("Hello, y'all! How are you üòÅ ?")
print(output)
```

```python
print(output.tokens)
print(output.ids)
print(output.offsets[9])
```

```python
tokenizer.token_to_id("[SEP]")
```

```python
from tokenizers.processors import TemplateProcessing

tokenizer.post_processor = TemplateProcessing(
    single="[CLS] $A [SEP]",
    pair="[CLS] $A [SEP] $B:1 [SEP]:1",
    special_tokens=[
        ("[CLS]", tokenizer.token_to_id("[CLS]")),
        ("[SEP]", tokenizer.token_to_id("[SEP]")),
    ],
)
```

```python
print(output.tokens)
output = tokenizer.encode("Hello, y'all!", "How are you üòÅ ?")
print(output.tokens)
```

```python
print(output.type_ids)
```

## Encoding in a batch

```python
tokenizer.enable_padding(pad_id=3, pad_token="[PAD]")
```

```python
output = tokenizer.encode_batch(["Hello, y'all!", "How are you üòÅ ?"])
print(output[0].tokens)
print(output[1].tokens)
```

```python
print(output[0].attention_mask)
print(output[1].attention_mask)
```

## Pretrained tokenizer, haszn√°lata

- BERT
- WordPiece: Ez a BPE-hez nagyon hasonl√≥ r√©szsz√≥-tokeniz√°ci√≥s algoritmus, amelyet f≈ëk√©nt a Google haszn√°l olyan modellekben, mint a BERT. Moh√≥ algoritmust haszn√°l, amely el≈ësz√∂r hossz√∫ szavakat pr√≥b√°l fel√©p√≠teni. Ez elt√©r a BPE-t≈ël, amely karakterekb≈ël indul ki, √©s min√©l nagyobb tokeneket √©p√≠t. A h√≠res ## el≈ëtagot haszn√°lja a sz√≥ r√©sz√©t k√©pez≈ë jelz≈ëk azonos√≠t√°s√°ra (azaz nem kezd≈ëd≈ë sz√≥).

```python
import requests

url = "https://huggingface.co/nlpaueb/legal-bert-base-uncased/raw/main/vocab.txt"
response = requests.get(url)

with open("bert-vocab.txt", "w") as f:
  f.write(response.text)
```

```python
from tokenizers import BertWordPieceTokenizer

tokenizer = BertWordPieceTokenizer("bert-vocab.txt", lowercase=True)
```

```python
output = tokenizer.encode("Hello, y'all!", "How are you üòÅ ?")
print(output.tokens)
```

### Saj√°t WordPiece √©p√≠t√©se
Ugyan√∫gy mint a BPE-n√©l csak haszn√°ljuk a WordPiece libet

```python
from tokenizers.models import WordPiece

# To Do
```

## Unigram
Az Unigram egy r√©szsz√≥ tokeniz√°ci√≥s algoritmus is, √©s √∫gy m≈±k√∂dik, hogy megpr√≥b√°lja azonos√≠tani a legjobb r√©szsz√≥ tokenek k√©szletet, hogy maximaliz√°lja az adott mondat val√≥sz√≠n≈±s√©g√©t. Ez abban k√ºl√∂nb√∂zik a BPE-t≈ël, hogy nem determinisztikus, szekvenci√°lisan alkalmazott szab√°lyok alapj√°n. Ehelyett az Unigram k√©pes lesz t√∂bbf√©le tokeniz√°l√°si m√≥dot kisz√°m√≠tani, mik√∂zben kiv√°lasztja a legval√≥sz√≠n≈±bbet.

```python
from tokenizers.models import Unigram

# To Do
```
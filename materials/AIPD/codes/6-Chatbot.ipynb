{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4eb1df6-6e7e-4a84-bd61-5d72091226e8",
   "metadata": {
    "id": "d4eb1df6-6e7e-4a84-bd61-5d72091226e8"
   },
   "source": [
    "# Chatbot - Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78aff64-cba8-490f-a48e-45966b2e68b9",
   "metadata": {
    "id": "a78aff64-cba8-490f-a48e-45966b2e68b9"
   },
   "source": [
    "In this notebook, you'll embark on developing chatbot functionality, crafting an AI bot equipped with the ability to maintain conversation history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504b12a7-0bf2-4974-a6c4-05b5a7b64644",
   "metadata": {
    "id": "504b12a7-0bf2-4974-a6c4-05b5a7b64644"
   },
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eb791e-c828-4593-889b-2c95068ed2ff",
   "metadata": {
    "id": "25eb791e-c828-4593-889b-2c95068ed2ff"
   },
   "source": [
    "Upon completing this notebook, you will have acquired the ability to:\n",
    "\n",
    "- Develop chatbot functionality utilizing our Phi-3 model, endowed with the capability to preserve conversation history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40b540e-4507-4dd7-98a5-426b2b4424ca",
   "metadata": {
    "id": "f40b540e-4507-4dd7-98a5-426b2b4424ca"
   },
   "source": [
    "# <FONT COLOR=\"purple\">Verify that the runtime environment is GPU in Colab!</FONT>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d93ea4a-3529-4a76-9f56-36028d4b31c0",
   "metadata": {
    "id": "3d93ea4a-3529-4a76-9f56-36028d4b31c0"
   },
   "source": [
    "## Install Dependencie(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc54ff77-57fb-4558-a958-5d91ae64909b",
   "metadata": {
    "id": "bc54ff77-57fb-4558-a958-5d91ae64909b"
   },
   "outputs": [],
   "source": [
    "# The 'device_map' paramter requires Accelerate package.\n",
    "# Restart workspace after the install!\n",
    "!pip install accelerate flash_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f19484-acf2-4669-bbca-298c1c512bd7",
   "metadata": {
    "id": "63f19484-acf2-4669-bbca-298c1c512bd7"
   },
   "source": [
    "## Create Microsoft [Phi-3-mini-4k-instruct](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct) Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab62455a-b3bf-41eb-8499-b85dc053c31f",
   "metadata": {
    "id": "ab62455a-b3bf-41eb-8499-b85dc053c31f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TextStreamer\n",
    "\n",
    "# Microsoft Phi-3-mini-4k-instruct model\n",
    "model = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "# The tokenizer is responsible for converting the text into a format understandable by the model.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(model, \n",
    "                                             torch_dtype=torch.float16, \n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=True,\n",
    "                                             attn_implementation=\"eager\")\n",
    "\n",
    "# The task of the streamer object is to ensure that the model's response is continuous. This reduces the waiting time.\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77ceca2-1a25-4ede-8cf8-11f98dcd14e6",
   "metadata": {
    "id": "c77ceca2-1a25-4ede-8cf8-11f98dcd14e6"
   },
   "source": [
    "## Generate Functions\n",
    "\n",
    "In this notebook, we will use the following `generate` function to support our interaction with the LLM.\n",
    "\n",
    "```python\n",
    "# Microsoft Phi-3-mini-4k-instruct default prompt template\n",
    "\n",
    "<|system|>\n",
    "{system}<|end|>\n",
    "<|user|>\n",
    "{question}<|end|>\n",
    "<|assistant|> \n",
    "{response}<|end|>\n",
    "<|user|>\n",
    "{question}<|end|>\n",
    "<|assistant|> \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42343e1f-40d8-4b9a-a56c-53f60774dcd5",
   "metadata": {
    "id": "42343e1f-40d8-4b9a-a56c-53f60774dcd5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(question, system=None, history=[], model=model, max_new_tokens = 512, do_sample=False, temperature=1):\n",
    "    \"\"\"\n",
    "    This function facilitates the generation of text responses leveraging a designated large language model (LLM) pipeline.\n",
    "    It accepts a prompt as input and transmits it to the specified LLM pipeline to produce a textual output.\n",
    "    The function offers comprehensive control over the generative process through the inclusion of configurable parameters and keyword arguments.\n",
    "\n",
    "    - question (str): This parameter holds the user question or any other instruction.\n",
    "    - system (str): This parameter holds contextual information to be provided to the language model for all conversations.\n",
    "    - history (array, opitonal) - This parameter stores the chat history. Each tuple within the list comprises a question and the corresponding assistant response.\n",
    "    - model (object): This object contains the model.\n",
    "    - max_new_tokens (int, optional) — The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n",
    "    - do_sample (bool, optional, defaults to False) — Whether or not to use sampling ; use greedy decoding otherwise.\n",
    "    - temperature (float, optional, defaults to 1.0) — The value used to modulate the next token probabilities.\n",
    "    \"\"\"\n",
    "\n",
    "    if system is None:\n",
    "        system = \"\"\"This is a chat between a user and an artificial intelligence assistant.\n",
    "        The assistant gives helpful, detailed, and polite answers to the user's questions based on the context.\n",
    "        The assistant should also indicate when the answer cannot be found in the context.\"\"\"\n",
    "\n",
    "    prompt = f\"<|system|>\\n{system}<|end|>\\n\"\n",
    "\n",
    "    # Add each example from the history to the prompt\n",
    "    for prev_question, prev_response in history:\n",
    "        prompt += f\"<|user|>{prev_question}<|end|>\\n<|assistant|>{prev_response}<|end|>\\n\"\n",
    "    \n",
    "    # Add the user_message prompt at the end\n",
    "    prompt += f\"<|user|>{question}<|end|>\\n<|assistant|>\"\n",
    "    tokenized_prompt = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    outputs = model.generate(input_ids=tokenized_prompt.input_ids,\n",
    "                             max_new_tokens=max_new_tokens,\n",
    "                             streamer=streamer,\n",
    "                             temperature=1, \n",
    "                             do_sample=do_sample)\n",
    "\n",
    "    # Return the decoded text from outputs\n",
    "    return tokenizer.decode(outputs[0][tokenized_prompt.input_ids.shape[-1]:], skip_special_tokens=True).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17c1228-3280-4798-9558-e19139b77b6c",
   "metadata": {
    "id": "a17c1228-3280-4798-9558-e19139b77b6c"
   },
   "source": [
    "## Chat Assitant Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f00550-4c5f-4d00-aab0-b18dac9dbce3",
   "metadata": {
    "id": "a2f00550-4c5f-4d00-aab0-b18dac9dbce3"
   },
   "outputs": [],
   "source": [
    "class ChatAssitant:\n",
    "    \"\"\"\n",
    "    This is a Chat Assistant interface designed to generate conversational responses utilizing the Phi-3 language model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, system_context=None, tokenizer=None, max_tokens=0):\n",
    "        \"\"\"\n",
    "        Constructor of the Chat Assistant class.\n",
    "\n",
    "        Parameters:\n",
    "        - system_context (str): This parameter holds contextual information to be provided to the language model for all conversations.\n",
    "        - tokenizer (object): The tokenizer used to tokenize the conversation for maintaining the history limit.\n",
    "        - max_tokens (int): The maximum number of tokens allowed in the history.\n",
    "        - history (list of tuples): This parameter stores the chat history. Each tuple within the list comprises a user message and the corresponding agent response.\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_tokens = max_tokens\n",
    "        self.system_context = system_context\n",
    "        self.history = []\n",
    "\n",
    "    def chat(self, user_message):\n",
    "        \"\"\"\n",
    "        This function generates a response from the chatbot in response to the user's message.\n",
    "        This method creates a prompt using the current system context and conversation history.\n",
    "        It then sends this prompt to the language model.\n",
    "        Finally, it stores the new user message and the model's response in the conversation history.\n",
    "\n",
    "        Parameters:\n",
    "        - user_msg (str): The user's input for which the chatbot will generate a response.\n",
    "\n",
    "        Returns:\n",
    "        - str: The generated response from the Chat Assistant.\n",
    "        \"\"\"\n",
    "\n",
    "        prompt = generate(user_message, self.system_context, self.history)\n",
    "        response = generate(prompt)\n",
    "\n",
    "        # Store this interaction in the conversation history\n",
    "        self.history.append((user_message, response))\n",
    "\n",
    "        if self.tokenizer is not None and self.max_tokens > 0:\n",
    "            self._history_trimmer()\n",
    "\n",
    "        return response\n",
    "\n",
    "    def _history_trimmer(self):\n",
    "        \"\"\"\n",
    "        Trims the conversation history to maintain the number of tokens below the specified limit.\n",
    "        \"\"\"\n",
    "        # Combine the conversation history into a unified string.\n",
    "        history_string = ''.join(user + assitant for user, assitant in self.history)\n",
    "        # Compute the total number of tokens in the conversation history.\n",
    "        history_tokens = len(self.tokenizer.encode(history_string))\n",
    "\n",
    "        # While the history exceeds the maximum token limit, remove the oldest items\n",
    "        while history_tokens > self.max_tokens:\n",
    "            # Check history. We need one item at least.\n",
    "            if self.history:\n",
    "                # Pop the oldest item from history.\n",
    "                self.history.pop(0)\n",
    "                # Recalculate the history string\n",
    "                history_string = ''.join(user + assitant for user, assitant in self.history)\n",
    "                history_tokens = len(self.tokenizer.encode(history_string))\n",
    "            else:\n",
    "                # If the conversation history is empty, exit the loop.\n",
    "                break\n",
    "\n",
    "    def reset(self):\n",
    "        # Clear conversation history\n",
    "        self.history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d826b1b-8e3d-4a7a-bfff-9949fa51bf1d",
   "metadata": {
    "id": "0d826b1b-8e3d-4a7a-bfff-9949fa51bf1d"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d710c5b-5ac6-4f34-b0c1-e6518af71615",
   "metadata": {
    "id": "3d710c5b-5ac6-4f34-b0c1-e6518af71615"
   },
   "source": [
    "## Without Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e530c8c9-0156-49c3-9670-b41abd86410e",
   "metadata": {
    "id": "e530c8c9-0156-49c3-9670-b41abd86410e"
   },
   "source": [
    "Let's initiate a conversation with our LLM. We'll set a system context instructing it to behave as a friendly chatbot. Additionally, we'll naively encourage it to remember our name if it's provided during the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bd66e2-8337-4eb9-9f0a-3e49939d0247",
   "metadata": {
    "id": "10bd66e2-8337-4eb9-9f0a-3e49939d0247",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Hello, I'm Peter. It's a pleasure to make your acquaintance!\"\n",
    "\n",
    "_ = generate(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837cd227-0900-498a-9f4d-bcdf0a9e77bc",
   "metadata": {
    "id": "837cd227-0900-498a-9f4d-bcdf0a9e77bc"
   },
   "source": [
    "The model certainly appears to be eager to show that it remembers who we are. Let's see what happens when we actually put its name-retention to the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d0bcd6-11f5-4f33-9a8e-f657ce4a4ca8",
   "metadata": {
    "id": "b2d0bcd6-11f5-4f33-9a8e-f657ce4a4ca8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Could you kindly remind me of my name, please?\"\n",
    "\n",
    "_ = generate(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45217bd0-699e-4126-9e69-f2f4ab22ce38",
   "metadata": {
    "id": "45217bd0-699e-4126-9e69-f2f4ab22ce38"
   },
   "source": [
    "It's not unexpected that the model doesn't recall our name. This is because, despite its presentation, we haven't equipped it with the ability to retain details from previous conversations. The model seems to assert that our name is \"Emily,\" which is evidently incorrect. When models produce responses that are fictional, often with unwavering confidence, we refer to this as hallucination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c9400d-64a8-4107-a441-6cb70004e603",
   "metadata": {
    "id": "51c9400d-64a8-4107-a441-6cb70004e603"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffb9670-9447-43bd-9f42-1c36adc9fe00",
   "metadata": {
    "id": "1ffb9670-9447-43bd-9f42-1c36adc9fe00"
   },
   "source": [
    "## With Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb21775d-758f-4f92-9224-7227156f3243",
   "metadata": {
    "id": "fb21775d-758f-4f92-9224-7227156f3243"
   },
   "source": [
    "To facilitate the creation of a chatbot experience capable of retaining information from past interactions, we'll employ a `ChatAssistant` class (defined earlier). Below, you'll find the ``help`` output extracted from our class definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf3e428-7890-4e39-9f7c-639044c947d4",
   "metadata": {
    "id": "fdf3e428-7890-4e39-9f7c-639044c947d4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "help(ChatAssitant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d28517-0239-450d-a077-ff087cbe8226",
   "metadata": {
    "id": "18d28517-0239-450d-a077-ff087cbe8226"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ecb676-bdf1-4e58-a150-dcfc7e09e2e8",
   "metadata": {
    "id": "51ecb676-bdf1-4e58-a150-dcfc7e09e2e8"
   },
   "source": [
    "Of utmost relevance to our current objective is establishing a **conversation_history** list, which we'll update each time we use the chat method. We'll apply familiar logic from earlier notebooks, notably employing the Phi-3 **prompt template**. This ensures proper formatting of each user/model interaction, which is then added to the prompt for subsequent exchanges.\n",
    "\n",
    "It's apt to describe our process as conducting **few-shot learning**, where the instructive examples consist of the previous interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f807d3b9-00a0-428d-beb4-07e061fcc06d",
   "metadata": {
    "id": "f807d3b9-00a0-428d-beb4-07e061fcc06d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "system = f\"\"\"This is a chat between a user and an artificial intelligence assistant.\n",
    "Always try to keep your answers is very short and concise!\n",
    "Do not explain your answers.\"\"\"\n",
    "\n",
    "assistant = ChatAssitant(system_context=system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94b8d9d-b76a-4df8-9886-eeca66fccaff",
   "metadata": {
    "id": "f94b8d9d-b76a-4df8-9886-eeca66fccaff",
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = assistant.chat(\"Hello, my name is Peter. It's a pleasure to make your acquaintance!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932831fc-9227-4aef-992e-21e2926f24a9",
   "metadata": {
    "id": "932831fc-9227-4aef-992e-21e2926f24a9"
   },
   "source": [
    "So far so good. Let's see now if the model is able to \"recall\" our name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2950da7-9863-4765-8750-da1de864e4fe",
   "metadata": {
    "id": "b2950da7-9863-4765-8750-da1de864e4fe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = assistant.chat(\"Could you tell me what my name is?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb67e3de-2ac6-4aeb-b251-337df87a7846",
   "metadata": {
    "id": "fb67e3de-2ac6-4aeb-b251-337df87a7846"
   },
   "source": [
    "Success! Let's take a look at the model's conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073ad5ac-3e79-40ac-8291-ad811aa0bd6c",
   "metadata": {
    "id": "073ad5ac-3e79-40ac-8291-ad811aa0bd6c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "assistant.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d5d2ef-27aa-4bbe-922b-bd025c82010a",
   "metadata": {
    "id": "45d5d2ef-27aa-4bbe-922b-bd025c82010a"
   },
   "source": [
    "Considering that `history` is added to the beginning of each new prompt, it's logical that the model can generate responses based on past interactions. The `reset` method is designed to clear `history`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729a3ad7-2d20-4cc8-8b96-95254b4e0e51",
   "metadata": {
    "id": "729a3ad7-2d20-4cc8-8b96-95254b4e0e51",
    "tags": []
   },
   "outputs": [],
   "source": [
    "assistant.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543f81f1-aefe-4334-a470-cbca4a521d64",
   "metadata": {
    "id": "543f81f1-aefe-4334-a470-cbca4a521d64",
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = assistant.chat(\"Could you tell me what my name is?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f22e5d0-457d-49e8-b718-72c37a5d8722",
   "metadata": {
    "id": "0f22e5d0-457d-49e8-b718-72c37a5d8722"
   },
   "source": [
    "As expected, the model is unable to \"recall\" details from our previous exchanges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7defa717-6ade-4757-894d-8ddd544f5d1c",
   "metadata": {
    "id": "7defa717-6ade-4757-894d-8ddd544f5d1c"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca0b735-208a-4b3d-be23-17a6a17262e2",
   "metadata": {
    "id": "8ca0b735-208a-4b3d-be23-17a6a17262e2"
   },
   "source": [
    "## Exercise: Task Tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913bdea1-877b-484b-bb7d-39f116f908ec",
   "metadata": {
    "id": "913bdea1-877b-484b-bb7d-39f116f908ec"
   },
   "source": [
    "TODO: Develop an assistant capable of managing your daily tasks. It should have the ability to add and remove items from your list based on your conversation. Additionally, it should accurately remind you of the tasks remaining on your list at any given time.\n",
    "\n",
    "Refer to the solution provided below if you encounter any difficulties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f41c8e-3220-464c-b411-f6598e9b50d6",
   "metadata": {
    "id": "00f41c8e-3220-464c-b411-f6598e9b50d6"
   },
   "source": [
    "### Your Work Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc457e8-33de-4e41-9cb2-d583a7095f8f",
   "metadata": {
    "id": "fbc457e8-33de-4e41-9cb2-d583a7095f8f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01eaf61f-9fe0-47f1-95f5-9902fb336469",
   "metadata": {
    "id": "01eaf61f-9fe0-47f1-95f5-9902fb336469",
    "tags": []
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f49ea64-5bab-4fa1-9a1d-f311b8c9fc32",
   "metadata": {
    "id": "6f49ea64-5bab-4fa1-9a1d-f311b8c9fc32",
    "tags": []
   },
   "outputs": [],
   "source": [
    "system = f\"\"\"This is a chat between a user and an artificial intelligence assistant.\n",
    "Always try to keep your answers is very short and concise!\n",
    "Do not explain your answers.\"\"\"\n",
    "\n",
    "assistant = ChatAssitant(system_context=system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42af759c-4e54-442d-a823-7dcda80c1708",
   "metadata": {
    "id": "42af759c-4e54-442d-a823-7dcda80c1708",
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = assistant.chat(\"Today's agenda includes: having breakfast, lunch, and dinner, going to work, exercising, and cleaning the house.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdac879-04c6-4dc8-bdb6-5a46ad1cbd86",
   "metadata": {
    "id": "efdac879-04c6-4dc8-bdb6-5a46ad1cbd86",
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = assistant.chat(\"Additionally, I plan to spend some time with friends.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70360868-62f1-4029-98c0-f20f9ad80d10",
   "metadata": {
    "id": "70360868-62f1-4029-98c0-f20f9ad80d10",
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = assistant.chat(\"Breakfast and exercise are completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d488ee-64d4-41ee-b59b-aae1b15ea4e5",
   "metadata": {
    "id": "41d488ee-64d4-41ee-b59b-aae1b15ea4e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = assistant.chat(\"Lunch is done. I should call the bike shop sometime today.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1afe3a2-e054-4c89-bd98-9c3b9026b3e9",
   "metadata": {
    "id": "a1afe3a2-e054-4c89-bd98-9c3b9026b3e9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = assistant.chat(\"Work is finished, along with spending time with friends, cleaning the house, and calling the bike shop.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c333c312-dd3b-401f-a6cc-1281f6ba4b91",
   "metadata": {
    "id": "c333c312-dd3b-401f-a6cc-1281f6ba4b91",
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = assistant.chat(\"Dinner is eaten. Now, it's time to head to bed..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef63afa-0bca-4bee-8e91-1be11677cc49",
   "metadata": {
    "id": "bef63afa-0bca-4bee-8e91-1be11677cc49"
   },
   "outputs": [],
   "source": [
    "assistant.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b823025-3bbd-4958-a5ef-acbcca859654",
   "metadata": {
    "id": "3b823025-3bbd-4958-a5ef-acbcca859654",
    "tags": []
   },
   "outputs": [],
   "source": [
    "assistant.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f8b04d-ed2d-49e1-bf86-43d7163a020a",
   "metadata": {
    "id": "f1f8b04d-ed2d-49e1-bf86-43d7163a020a"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a8ae85-9c93-463a-a327-3d93397a412b",
   "metadata": {
    "id": "05a8ae85-9c93-463a-a327-3d93397a412b",
    "tags": []
   },
   "source": [
    "## Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc97760f-ee70-4a39-ade0-9fe63231ec5b",
   "metadata": {
    "id": "fc97760f-ee70-4a39-ade0-9fe63231ec5b"
   },
   "source": [
    "The following key concepts were introduced in this notebook:\n",
    "\n",
    "- **Hallucination:** When a model generates, often with some expressed confidence, untrue or inaccurate responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f6ccc2-246e-4ac9-945f-dbde65f8691d",
   "metadata": {
    "id": "42f6ccc2-246e-4ac9-945f-dbde65f8691d"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd47cc91-8655-4a58-98dc-75b4dfc26cbb",
   "metadata": {
    "id": "dd47cc91-8655-4a58-98dc-75b4dfc26cbb"
   },
   "source": [
    "## Optional Advanced Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f282b3-1d7f-4f49-96e5-e803692d3276",
   "metadata": {
    "id": "e2f282b3-1d7f-4f49-96e5-e803692d3276"
   },
   "source": [
    "Please note: In the upcoming notebook, we will explore the limitations regarding the amount of conversation the model can retain before encountering issues. Bearing this in mind, and prior to suggesting further experimentation, if you observe the model producing only empty responses, proceed to the next section to gain insights into this phenomenon.\n",
    "\n",
    "For those interested in delving deeper into the course material, below are additional open-ended exercises to consider:\n",
    "\n",
    "Develop a Helper Bot: Create a bot designed to offer support to individuals during challenging times. This bot should provide encouragement, praise, and empathy, while also discerning when to respond and when to prompt the user for more information about their situation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06695827-2e3d-49b1-919f-c6c566606d32",
   "metadata": {
    "id": "06695827-2e3d-49b1-919f-c6c566606d32"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013befef-423b-4947-8bf3-3cfe9f28ae49",
   "metadata": {
    "id": "013befef-423b-4947-8bf3-3cfe9f28ae49",
    "tags": []
   },
   "source": [
    "## Restart the Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce3340b-1e90-4485-abfb-bdaab9cc0894",
   "metadata": {
    "id": "8ce3340b-1e90-4485-abfb-bdaab9cc0894",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "get_ipython().kernel.do_shutdown(restart=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bf2014",
   "metadata": {},
   "source": [
    "## <FONT COLOR=\"red\">The notebook is licensed under the Creative [Commons Attribution-NonCommercial-ShareAlike 4.0 International License (CC BY-NC-SA 4.0)](https://creativecommons.org/). This means that you can freely copy, distribute, and modify the notebook by authors ([Balázs Harangi](https://inf.unideb.hu/dr-harangi-balazs), [András Hajdu](https://inf.unideb.hu/munkatars/4250), and [Róbert Lakatos](https://inf.unideb.hu/lakatos-robert-tanarseged)), but not for commercial purposes. Additionally, if you modify the notebook, you must cite them as the original creators and share the modified version under the same terms.\n",
    "</FONT>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "17BSv8vPRQ8Wihts9kr5C9sVD306tVq_2",
     "timestamp": 1716734863588
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python [conda env:admin]",
   "language": "python",
   "name": "conda-env-admin-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

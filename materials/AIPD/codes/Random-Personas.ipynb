{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "706e8c2b-5f1b-4b7b-9ece-b6b341e8b49d",
   "metadata": {
    "id": "706e8c2b-5f1b-4b7b-9ece-b6b341e8b49d",
    "tags": []
   },
   "source": [
    "# Random Personas - Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e22814-a66d-4646-9966-9843a433ce70",
   "metadata": {
    "id": "52e22814-a66d-4646-9966-9843a433ce70"
   },
   "source": [
    "In this module, you'll gain insights into regulating the temperature of the model's generation process, empowering you to influence the level of randomness in its responses. By harnessing temperature, you'll acquire the ability to craft a diverse array of AI personalities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff739fa-f350-4b50-b3ad-d19fbefd4e6f",
   "metadata": {
    "id": "0ff739fa-f350-4b50-b3ad-d19fbefd4e6f"
   },
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daefff31-716d-42f7-8362-26a10145727d",
   "metadata": {
    "id": "daefff31-716d-42f7-8362-26a10145727d"
   },
   "source": [
    "Upon completing this module, you'll achieve the following objectives:\n",
    "- Understand how random **sampling** contributes to non-deterministic responses in Language and Logic Models (LLMs).\n",
    "- Learn to regulate the level of randomness in responses by adjusting the **temperature** parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2481beed-3427-4292-8a1c-a0159ccd76b5",
   "metadata": {
    "id": "2481beed-3427-4292-8a1c-a0159ccd76b5"
   },
   "source": [
    "# <FONT COLOR=\"purple\">Verify that the runtime environment is GPU in Colab!</FONT>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700e61d1-e1db-4f8c-8a2c-cbe5a6e8b5f8",
   "metadata": {
    "id": "700e61d1-e1db-4f8c-8a2c-cbe5a6e8b5f8"
   },
   "source": [
    "## Install Dependencie(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7029e3b6-d6f8-449e-bed1-9aa423d4b6e6",
   "metadata": {
    "id": "7029e3b6-d6f8-449e-bed1-9aa423d4b6e6"
   },
   "outputs": [],
   "source": [
    "# The 'device_map' paramter requires Accelerate package.\n",
    "# Restart workspace after the install!\n",
    "!pip install accelerate flash_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44272604-6cca-4cb4-b19d-2b8f415ebd74",
   "metadata": {
    "id": "44272604-6cca-4cb4-b19d-2b8f415ebd74"
   },
   "source": [
    "## Create Microsoft [Phi-3-mini-4k-instruct](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct) Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c28974-d576-4886-ab0b-061659187313",
   "metadata": {
    "id": "48c28974-d576-4886-ab0b-061659187313",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TextStreamer\n",
    "\n",
    "# Microsoft Phi-3-mini-4k-instruct model\n",
    "model = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "# The tokenizer is responsible for converting the text into a format understandable by the model.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(model, \n",
    "                                             torch_dtype=torch.float16, \n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=True,\n",
    "                                             attn_implementation=\"eager\")\n",
    "\n",
    "# The task of the streamer object is to ensure that the model's response is continuous. This reduces the waiting time.\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76923260-7338-4380-9fd7-d3c7756d651e",
   "metadata": {
    "id": "76923260-7338-4380-9fd7-d3c7756d651e"
   },
   "source": [
    "## Generate Functions\n",
    "\n",
    "In this notebook, we will use the following `generate` function to support our interaction with the LLM.\n",
    "\n",
    "```python\n",
    "# Microsoft Phi-3-mini-4k-instruct default prompt template\n",
    "\n",
    "<|system|>\n",
    "{system}<|end|>\n",
    "<|user|>\n",
    "{question}<|end|>\n",
    "<|assistant|> \n",
    "{response}<|end|>\n",
    "<|user|>\n",
    "{question}<|end|>\n",
    "<|assistant|> \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4949f0ca-0df8-4d88-897e-9ed861ede866",
   "metadata": {
    "id": "4949f0ca-0df8-4d88-897e-9ed861ede866"
   },
   "outputs": [],
   "source": [
    "def generate(question, system=None, history=[], model=model, max_new_tokens = 512, do_sample=False, temperature=1):\n",
    "    \"\"\"\n",
    "    This function facilitates the generation of text responses leveraging a designated large language model (LLM) pipeline.\n",
    "    It accepts a prompt as input and transmits it to the specified LLM pipeline to produce a textual output.\n",
    "    The function offers comprehensive control over the generative process through the inclusion of configurable parameters and keyword arguments.\n",
    "\n",
    "    - question (str): This parameter holds the user question or any other instruction.\n",
    "    - system (str): This parameter holds contextual information to be provided to the language model for all conversations.\n",
    "    - history (array, opitonal) - This parameter stores the chat history. Each tuple within the list comprises a question and the corresponding assistant response.\n",
    "    - model (object): This object contains the model.\n",
    "    - max_new_tokens (int, optional) — The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n",
    "    - do_sample (bool, optional, defaults to False) — Whether or not to use sampling ; use greedy decoding otherwise.\n",
    "    - temperature (float, optional, defaults to 1.0) — The value used to modulate the next token probabilities.\n",
    "    \"\"\"\n",
    "\n",
    "    if system is None:\n",
    "        system = \"\"\"This is a chat between a user and an artificial intelligence assistant.\n",
    "        The assistant gives helpful, detailed, and polite answers to the user's questions based on the context.\n",
    "        The assistant should also indicate when the answer cannot be found in the context.\"\"\"\n",
    "\n",
    "    prompt = f\"<|system|>\\n{system}<|end|>\\n\"\n",
    "\n",
    "    # Add each example from the history to the prompt\n",
    "    for prev_question, prev_response in history:\n",
    "        prompt += f\"<|user|>{prev_question}<|end|>\\n<|assistant|>{prev_response}<|end|>\\n\"\n",
    "    \n",
    "    # Add the user_message prompt at the end\n",
    "    prompt += f\"<|user|>{question}<|end|>\\n<|assistant|>\"\n",
    "    tokenized_prompt = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    outputs = model.generate(input_ids=tokenized_prompt.input_ids,\n",
    "                             max_new_tokens=max_new_tokens,\n",
    "                             streamer=streamer,\n",
    "                             temperature=1, \n",
    "                             do_sample=do_sample)\n",
    "\n",
    "    # Return the decoded text from outputs\n",
    "    return tokenizer.decode(outputs[0][tokenized_prompt.input_ids.shape[-1]:], skip_special_tokens=True).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0710e61-3fa4-45f0-aa6e-455d6ec8740e",
   "metadata": {
    "id": "f0710e61-3fa4-45f0-aa6e-455d6ec8740e"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acd78c2-d700-4a71-baea-b1e4b76ad80c",
   "metadata": {
    "id": "8acd78c2-d700-4a71-baea-b1e4b76ad80c"
   },
   "source": [
    "## Non-random Responses (Default)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a535af-43af-4d04-b8d1-ba2196161302",
   "metadata": {
    "id": "94a535af-43af-4d04-b8d1-ba2196161302"
   },
   "source": [
    "You might have observed in prior modules that the responses from our Phi-3 model, provided we don't modify the prompt, tend to be deterministic. Let's delve into this explicitly.\n",
    "\n",
    "In this exercise, we'll employ our model for another generative task: crafting fictional customer experiences with their Ferrari F40 sports car. To ensure the context is set appropriately, we'll establish the system context and prompt the model to recollect a memorable day with their cars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9ddb17-e619-4947-b0ba-0ade00a8c5ae",
   "metadata": {
    "id": "3d9ddb17-e619-4947-b0ba-0ade00a8c5ae",
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_context = \"\"\"\n",
    "You're a perpetually content customer who delights in sharing personal product experiences.\n",
    "Your focus lies not in technical details, but in the emotions and joy derived from these experiences.\n",
    "You eagerly anticipate others experiencing the same sense of euphoria and happiness as you.\n",
    "Your intention isn't to promote, but to authentically convey heartfelt stories of joy and satisfaction.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"Please reminisce about a memorable day spent with your Ferrari F40 sports car in 150 words or fewer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7983d5e7-ab19-445e-86c2-81fe48cce6d6",
   "metadata": {
    "id": "7983d5e7-ab19-445e-86c2-81fe48cce6d6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = generate(prompt, system_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efbde1d-9ba0-4a27-8091-a185236043dd",
   "metadata": {
    "id": "9efbde1d-9ba0-4a27-8091-a185236043dd"
   },
   "source": [
    "Let's generate a response a few more times using the identical prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faea14c-9bb3-41f0-90db-fda38da3c874",
   "metadata": {
    "id": "3faea14c-9bb3-41f0-90db-fda38da3c874",
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = generate(prompt, system_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f0a1c2-39e5-48c2-9a03-8578ebf6afbd",
   "metadata": {
    "id": "b6f0a1c2-39e5-48c2-9a03-8578ebf6afbd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = generate(prompt, system_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117165bb-fbd6-494d-9f52-8d5f2138e141",
   "metadata": {
    "id": "117165bb-fbd6-494d-9f52-8d5f2138e141"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bdc89d-281b-46f5-8f29-becd0359c0b0",
   "metadata": {
    "id": "d1bdc89d-281b-46f5-8f29-becd0359c0b0"
   },
   "source": [
    "## Sampling and Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a5b4e0-67a3-490f-aff7-1882ba4854f6",
   "metadata": {
    "id": "d2a5b4e0-67a3-490f-aff7-1882ba4854f6"
   },
   "source": [
    "Within the domain of language models, **sampling** constitutes the method by which a model generates text by selecting from the probability distribution of potential next words (technically tokens, although for simplicity, we can regard them as words). When we engage with language models without activating **sampling**, the model operates deterministically, consistently choosing the most probable next word during text generation. This default behavior proves valuable when seeking consistency and precision, yet it can prove restrictive when aiming for diverse and creative responses.\n",
    "\n",
    "In the context of the transformers pipeline utilized to interact with our Phi-3 model, **sampling** remains deactivated by default. To activate **sampling**, we specify do_sample=True when invoking our generate function, thereby directing the model to select words based on a probability distribution. This allows for less probable words to be chosen, potentially resulting in more varied and engaging text.\n",
    "\n",
    "Furthermore, once **sampling** is enabled, we have the option to input specific values for **temperature**, representing the degree of randomness in the response. **Temperature** values range between 0.0 and 1.0, with higher values indicating a greater degree of randomness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71284aa-b9e6-4d14-b29d-c48161791856",
   "metadata": {
    "id": "c71284aa-b9e6-4d14-b29d-c48161791856"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9d9a51-d5ac-4fe5-a736-5155dfa9a08d",
   "metadata": {
    "id": "1d9d9a51-d5ac-4fe5-a736-5155dfa9a08d"
   },
   "source": [
    "## Exercise: Random Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d6d682-07d0-4674-87a9-20cc840ec59d",
   "metadata": {
    "id": "19d6d682-07d0-4674-87a9-20cc840ec59d"
   },
   "source": [
    "TODO: Using the identical **system context** and prompt as previously mentioned, activate sampling `do_sample=True` and adjust the **temperature** to its maximum value `temperature=1.0`. Generate three distinct responses to ensure each one is unique.\n",
    "\n",
    "If you encounter any challenges, refer to the solution provided below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1eb2ab-b768-4a87-aeb3-8e1e468058a8",
   "metadata": {
    "id": "3c1eb2ab-b768-4a87-aeb3-8e1e468058a8"
   },
   "source": [
    "### Your Work Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1929355e-85a0-4af2-9c67-90b6fefd754e",
   "metadata": {
    "id": "1929355e-85a0-4af2-9c67-90b6fefd754e"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8244d459-7fdb-4737-8cd7-f2077de152c9",
   "metadata": {
    "id": "8244d459-7fdb-4737-8cd7-f2077de152c9",
    "tags": []
   },
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b65b2d-fdb6-4c65-8836-e8023cac56a6",
   "metadata": {
    "id": "69b65b2d-fdb6-4c65-8836-e8023cac56a6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = generate(prompt, system_context, do_sample=True, temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1fda59-8421-4abe-8865-1d20daa43723",
   "metadata": {
    "id": "3b1fda59-8421-4abe-8865-1d20daa43723",
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = generate(prompt, system_context, do_sample=True, temperature=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ad85e2-13f3-40fc-91d6-e8106563661c",
   "metadata": {
    "id": "01ad85e2-13f3-40fc-91d6-e8106563661c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = generate(prompt, system_context, do_sample=False, temperature=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e43770-6d77-4eb7-9288-1cafe2cc1d7b",
   "metadata": {
    "id": "f4e43770-6d77-4eb7-9288-1cafe2cc1d7b"
   },
   "source": [
    "### Creativity and Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc45442-ba4d-46ea-bf6b-b2ca4ec24270",
   "metadata": {
    "id": "9dc45442-ba4d-46ea-bf6b-b2ca4ec24270"
   },
   "source": [
    "In conclusion, it's important to note that while random generation, facilitated by adjusting the **temperature**, can be beneficial for producing unique or imaginative outputs, it may not align well with the precision and accuracy of responses. In situations where it's crucial for the model to generate precise and/or factually accurate responses, it's advisable to exercise caution and thoroughly review its outputs when increasing the **temperature** setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d1310c-e654-4236-870c-ac9f397acbd3",
   "metadata": {
    "id": "b3d1310c-e654-4236-870c-ac9f397acbd3"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425d181a-1d5c-4c3a-87a6-fb454c2acf85",
   "metadata": {
    "id": "425d181a-1d5c-4c3a-87a6-fb454c2acf85",
    "tags": []
   },
   "source": [
    "## Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0a945f-30f6-4db1-9eb5-60dcc9d3d1bd",
   "metadata": {
    "id": "bc0a945f-30f6-4db1-9eb5-60dcc9d3d1bd"
   },
   "source": [
    "In this module, we covered the following key concepts:\n",
    "\n",
    "- **Sampling:** This is a fundamental process in text generation where the language model chooses the next word (token) based on a probability distribution across the vocabulary.\n",
    "- **Temperature:** Considered a hyperparameter, temperature governs the degree of randomness in the model's predictions during sampling. Elevating the temperature enhances diversity, resulting in a broader array of responses, while lowering it renders the model's output more predictable and conservative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0e102a-b11e-4591-99bd-ce3ba9d9f725",
   "metadata": {
    "id": "dd0e102a-b11e-4591-99bd-ce3ba9d9f725"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff82d544-edc3-4e3a-86bf-2cd180254aa2",
   "metadata": {
    "id": "ff82d544-edc3-4e3a-86bf-2cd180254aa2"
   },
   "source": [
    "## Optional Advanced Exercises\n",
    "\n",
    "Let's delve into the creation of interacting characters:\n",
    "\n",
    "- **Make Interacting Characters:** Now that you've mastered generating statements from fictional personas, let's explore crafting a system that generates multiple distinct personalities and facilitates interactions among them.\n",
    "- **Make Interacting Characters Play a Game:** Building upon the previous exercise, expand your system to include multiple characters working towards a shared objective defined by a \"game\" of your design. This could entail tasks such as persuading another character to utter a specific word, divulging the location of a hidden treasure, or even engaging in collaborative endeavors where characters must cooperate to achieve a common goal. For an added challenge, consider introducing more than two characters or even organizing them into teams, fostering interactions among multiple players."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f957e189-a45d-4937-a494-8f2d5896c713",
   "metadata": {
    "id": "f957e189-a45d-4937-a494-8f2d5896c713"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b8885f-3035-44bc-9df7-501dcab4b705",
   "metadata": {
    "id": "c3b8885f-3035-44bc-9df7-501dcab4b705",
    "tags": []
   },
   "source": [
    "## Restart the Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8de2be-7716-4c23-a06f-7f8364e62bab",
   "metadata": {
    "id": "8f8de2be-7716-4c23-a06f-7f8364e62bab",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "\n",
    "get_ipython().kernel.do_shutdown(restart=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0c3a80",
   "metadata": {},
   "source": [
    "## <FONT COLOR=\"red\">The notebook is licensed under the Creative [Commons Attribution-NonCommercial-ShareAlike 4.0 International License (CC BY-NC-SA 4.0)](https://creativecommons.org/). This means that you can freely copy, distribute, and modify the notebook by authors ([Balázs Harangi](https://inf.unideb.hu/dr-harangi-balazs), [András Hajdu](https://inf.unideb.hu/munkatars/4250), and [Róbert Lakatos](https://inf.unideb.hu/lakatos-robert-tanarseged)), but not for commercial purposes. Additionally, if you modify the notebook, you must cite them as the original creators and share the modified version under the same terms.\n",
    "</FONT>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1omsHFzTKCSvWEL1kQkfaXB89CXMVGSpu",
     "timestamp": 1716737451465
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python [conda env:admin]",
   "language": "python",
   "name": "conda-env-admin-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

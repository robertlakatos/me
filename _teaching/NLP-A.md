---
title: "Advanced Natural Language Processing"
collection: teaching
type: "M.Sc course"
permalink: /teaching/NLP-A
venue: "University of Debrecen, Department of Data Science and Visualization"
date: 2025-02-09
location: "Debrecen, Hungary"
---

This course delves into advanced concepts of Natural Language Processing (NLP) and Machine Learning (ML) with a strong focus on modern deep learning techniques. It covers foundational topics such as tokenization, text representation, and pipelines, as well as cutting-edge research in large language models (LLMs), transformers, and their applications. The course emphasizes both theoretical understanding and practical implementation, preparing students to tackle real-world NLP challenges, including security, privacy, and human-centered design. During the semester, students will also have the opportunity to test and train these architectures on real data using cloud-based services [(Google Collab)](https://colab.google/).

======

## [Email address of the Teacher](mailto:lakatos.robert@inf.unideb.hu)

## Consultation

- Monday / Hétfő - 14:00 - 15:30 - IK 107 (in the classroom / teremben)
- Monday / Hétfő - 15:30 - 16:00 - IK I128 (in the office / irodában)
- Monday / Hétfő - 16:00 - 17:30 - IK 204 (in the classroom / teremben)
- Monday / Hétfő - 17:30 - 18:00 - IK I128 (in the office / irodában)
- Monday / Hétfő - 18:00 - 19:30 - IK 204 (in the classroom / teremben)

- Tuesday / Kedd - 14:00 - 15:30 - IK 132 (in the classroom / teremben)
- Tuesday / Kedd - 15:30 - 16:00 - IK I128 (in the office / irodában)
- Tuesday / Kedd - 16:00 - 17:30 - IK TEOKJ II. em. 109 (in the classroom / teremben)
- Tuesday / Kedd - 17:30 - 18:00 - IK I128 (in the office / irodában)
- Tuesday / Kedd - 18:00 - 19:30 - IK TEOKJ II. em. 106 (in the classroom / teremben)

## [Attendance sheet](https://forms.cloud.microsoft/e/Ek0iSB0fjC?origin=lprLink)

## [Attendance sheet status](https://unidebhu-my.sharepoint.com/:x:/g/personal/lakatos_robert_inf_unideb_hu/IQCCnu6AtHk1TpCV4NsYwGVqASFjuU1MguAOZNI0A8oSHlY?e=B3IZ3O)

## Requirements

- Attendance sheet: Fewer absences than allowed. Active participation in classes.
- Create a working application, solve a real problem, and present it as a video using the solutions and models learned in class.
     - It must be uploaded to Github and shared.
     - Maximum length of video is 5-10 minutes.
     - In the video, each creator must present their own contribution. (for 3-8 minutes)
     - The application must be shown in action at the end of the video. (for 1-2 minutes)
     - Video size must be 50 MB or less.
     - The group members (students) have to send one Jupyter notebook (ipynb) file with Python code that contains all the project code.
- Organizing into teams (2-4 people) or working individually.
- If the creator(s) uses a service based on a generative language model to complete the task, they must attach the prompt log to the completed project as additional material.
- It is not certain that the team members receive a uniform grade, but they get grades proportionate to the task they have completed in the project.
- **Submission deadline: 2025.05.24**
- [**Submission form**]()

## Lecture

- I.    [Tokenization](../materials/NLP-A/lectures/lesson_2)
- I.    [Text representation I.](../materials/NLP-A/lectures/lesson_3)
- I.    [Text representation II.](../materials/NLP-A/lectures/lesson_4)
- II.   [Large language models I. fancy-rnn](https://robertlakatos.github.io/me/materials/NLP-A/lectures/fancy-rnn.pdf)
- II.   [Large language models I. CNN-TreeRNN](https://robertlakatos.github.io/me/materials/NLP-A/lectures/CNN-TreeRNN.pdf)

## Labor

### Basics

- I.    [Tokenization](https://colab.research.google.com/drive/1Z9XA9Ik9ofb_hk61mukq-P1X9JEpv1m4#scrollTo=gtFbC4F-H0h7)
- II.   [Embedded vectors](https://colab.research.google.com/drive/1RRurVdRdYblE9ShU8kwzpmkToAW67iIG#scrollTo=tsfToJX1g3gW)

## Submitted

- [2025 Spring](../materials/NLP-A/submitted/2025-1)

## Usefull Links

- [Huggingface](https://huggingface.co/)
- [LlamaIndex](https://docs.llamaindex.ai/en/stable/)
- [Keras](https://keras.io/)
- [Tensorflow](https://www.tensorflow.org/)
- [Pytorch](https://pytorch.org/)
- [Pyton](https://www.python.org/)
- [Google Colab](https://colab.google/)

## Recommended Literatures and Courses

1.  [Jurafsky, Daniel, and James H. Martin. "Speech and language processing (draft)." Chapter A: Hidden Markov Models (Draft of September 11, 2018). Retrieved March 19 (2018): 2019.](https://ms.b-ok.xyz/book/3560643/4a6ab2)
2.  [Eisenstein, Jacob. "Introduction to natural language processing." MIT press, 2019.](https://mitpress.mit.edu/9780262042840/introduction-to-natural-language-processing/)
3.  [Goldberg, Yoav. "A primer on neural network models for natural language processing." Journal of Artificial Intelligence Research 57 (2016): 345-420.](https://arxiv.org/pdf/1510.00726.pdf)
4.  [Francois Chollet. "Deep Learning with Python"](https://www.amazon.com/Deep-Learning-Python-Francois-Chollet/dp/1617294438)
5.  [Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/chapter0/1?fw=pt)
6.  [MIT Introduction to Deep Learning](http://introtodeeplearning.com/)
7.  [Visual Guide to Transformer Neural Networks - (Episode 1)](⁠https://www.youtube.com/watch?v=dichIcUZfOw)
8.  [Visual Guide to Transformer Neural Networks - (Episode 2)](⁠https://www.youtube.com/watch?v=mMa2PmYJlCo)
9.  [Visual Guide to Transformer Neural Networks - (Episode 3)](⁠https://www.youtube.com/watch?v=gJ9kaJsE78k)
10. [Stanford CS 224N / Ling 280  —  Natural Language Processing](https://web.stanford.edu/class/cs224n/)

# Usefull Publications

[1] [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)

[2] [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)

[3] [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)

[4] [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)

[5] [Global Vectors for Node Representations](https://arxiv.org/pdf/1902.11004.pdf)
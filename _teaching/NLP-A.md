---
title: "Advanced Natural Language Processing"
collection: teaching
type: "M.Sc course"
permalink: /teaching/NLP-A
venue: "University of Debrecen, Department of Data Science and Visualization"
date: 2024-02-27
location: "Debrecen, Hungary"
---

Within the framework of the subject, students will learn about the basics of natural language text processing (NLP). In addition, they also gain practical experience while solving various tasks. Main topics: logistic regression, naive Bayes model, PCA, n-gram models, Word2Vec, classical and recurrent neural networks. Furthermore, during the completion of the subject, students can gain insight into current, modern neural architectures. During the semester, students will also have the opportunity to test and train these architectures on real data using cloud-based services [(Google Collab)](https://colab.google/).

======

## Requirements

- Fewer absences than allowed. Active participation in classes.
- Organizing into teams (3-4 people).
- Creating a working application and presenting it in the form of a video using the solutions and models learned in class.
     - It must be uploaded to Github and shared.
     - Maximum length of video is 5-10 minutes.
     - In the video, each creator must present their own contribution. (for 3-8 minutes)
     - At the end of the video, the application must be shown in action. (for 1-2 minutes)
- The team members do not receive a uniform ticket, but get the ticket in proportion to the task they have completed in the project.
- Method of submission: [By email](mailto:lakatos.robert@inf.unideb.hu)
- Submission deadline: ****

## Lecture

- I.    [Aperitive](https://robertlakatos.github.io/me/materials/NLP-A/lectures/L01_anlp.pdf)
- II.   [Tokenization](../materials/NLP-A/lectures/lesson_2)
- III.  [Text representation I.](../materials/NLP-A/lectures/lesson_3)
- IV.   [Text representation II.](../materials/NLP-A/lectures/lesson_4)
- V.    [Large language models](https://robertlakatos.github.io/me/materials/NLP-A/lectures/L05_anlp.pdf)
- VI.   [Transformer](https://robertlakatos.github.io/me/materials/NLP-A/lectures/L06_anlp.pdf)
- VII.  [Pretrain (BERT, GPT)](https://robertlakatos.github.io/me/materials/NLP-A/lectures/L07_anlp.pdf)
- VIII. [Instruction Finetuning, and RLHF](https://robertlakatos.github.io/me/materials/NLP-A/lectures/L09_anlp.pdf)
- IX.   [Efficient Adaptation](https://robertlakatos.github.io/me/materials/NLP-A/lectures/L09_anlp.pdf)
- X.    [Question Answering](https://robertlakatos.github.io/me/materials/NLP-A/lectures/L10_anlp.pdf)
- XI.   [Security & Privacy of LLMs](https://robertlakatos.github.io/me/materials/NLP-A/lectures/L11_anlp.pdf)
- XII.  [Evaluation](https://robertlakatos.github.io/me/materials/NLP-A/lectures/L12_anlp.pdf)
- XIII. [Natural Language to Code Generation](https://robertlakatos.github.io/me/materials/NLP-A/lectures/L13_anlp.pdf)
- XIV.  [Human-Centered NLPF](https://robertlakatos.github.io/me/materials/NLP-A/lectures/L14_anlp.pdf)
- XV.   [Hardware-aware Algorithms for Sequence Modeling](https://robertlakatos.github.io/me/materials/NLP-A/lectures/L15_anlp.pdf)
- XVI.  [Open problems and discussion](https://robertlakatos.github.io/me/materials/NLP-A/lectures/L16_anlp.pdf)

## Labor

- I.    [Python Basics](../materials/NLP-A/python)
- I.    [Numpy and Matplotlib](./materials/NLP-A/numpy-and-matplotlib)
- I.    [Pandas Intro](../materials/NLP-A/pandas)
- II.   [Pipeline](../materials/NLP-A/pipeline)
- III.  [Train](../materials/NLP-A/train)
- IV.   [Tokenization](../materials/NLP-A/tokenization)
- V.    [Embedding](../materials/NLP-A/embedding)
- VI.   [Pos Embedding](../materials/NLP-A/pos-embedding)
- VII.  [Attention I.](../materials/NLP-A/attention-I)
- VIII. [Attention II.](../materials/NLP-A/attention-II)
- IX.   [Feed Foward Layer](../materials/NLP-A/feed-foward)
- X.    [Training in Transformer Architecture](../materials/NLP-A/train-transformer)
- XI.   [Generative Pre-trained Transformer (GPT)](../materials/NLP-A/gpt)
- XII.  [Recommender System](../materials/NLP-A/recommender-system)
- XIII. [Topic Modelling](../materials/NLP-A/topic-modelling)
- XIV.  [Multimodal](../materials/NLP-A/multimodal)
- XIV.  [Generativ AI - Chat modell](../materials/NLP-A/gaichat)

## Submitted

- [2023 Autumn](../materials/NLP-A/submitted/2023-2)

## Usefull Links

- [Huggingface](https://huggingface.co/)
- [Keras](https://keras.io/)
- [Tensorflow](https://www.tensorflow.org/)
- [Pytorch](https://pytorch.org/)
- [Pyton](https://www.python.org/)
- [Google Colab](https://colab.google/)

## Recommended Literatures and Courses

1. [Jurafsky, Daniel, and James H. Martin. "Speech and language processing (draft)." Chapter A: Hidden Markov Models (Draft of September 11, 2018). Retrieved March 19 (2018): 2019.](https://ms.b-ok.xyz/book/3560643/4a6ab2)
2. [Eisenstein, Jacob. "Introduction to natural language processing." MIT press, 2019.](https://mitpress.mit.edu/9780262042840/introduction-to-natural-language-processing/)
3. [Goldberg, Yoav. "A primer on neural network models for natural language processing." Journal of Artificial Intelligence Research 57 (2016): 345-420.](https://arxiv.org/pdf/1510.00726.pdf)
4. [Francois Chollet. "Deep Learning with Python"](https://www.amazon.com/Deep-Learning-Python-Francois-Chollet/dp/1617294438)
5. [Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/chapter0/1?fw=pt)
6. [MIT Introduction to Deep Learning](http://introtodeeplearning.com/)

## Key Words

1. Tokenization 
    - Byte-Pair Encoding (BPE)
    - Byte-level BPE
    - WordLevel
    - WordPiece
    - Unigram
    - SentencePiece
2. Embbeding
    - Skip-Gram
    - CBOW
    - GLOVE
    - Word2Vec
3. Position Embedding
4. (Multi-Head) Attention
5. Neural Network (Feed Foward layer)
6. Normalization
7. Transoformer
8. Pre-Trained
9. Large Language Model
10. NLP Tasks
    - Summarization
    - Translate
    - Generation
    - Q&A
    - Named Entity Recognition
    - Sentiment analysis
11. Multimodal architectures
12. Huggingface
13. Keras
14. Tensorflow
15. Pytorch
16. Python
17. Pipline
18. Notebook
19. Google Colab

# Usefull Publications

[1] [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)

[2] [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)

[3] [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)

[4] [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)

[5] [Global Vectors for Node Representations](https://arxiv.org/pdf/1902.11004.pdf)
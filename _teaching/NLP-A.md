---
title: "Advanced Natural Language Processing"
collection: teaching
type: "M.Sc course"
permalink: /teaching/NLP-A
venue: "University of Debrecen, Department of Data Science and Visualization"
date: 2024-09-04
location: "Debrecen, Hungary"
---

This course delves into advanced concepts of Natural Language Processing (NLP) and Machine Learning (ML) with a strong focus on modern deep learning techniques. It covers foundational topics such as tokenization, text representation, and pipelines, as well as cutting-edge research in large language models (LLMs), transformers, and their applications. The course emphasizes both theoretical understanding and practical implementation, preparing students to tackle real-world NLP challenges, including security, privacy, and human-centered design. During the semester, students will also have the opportunity to test and train these architectures on real data using cloud-based services [(Google Collab)](https://colab.google/).

======

## Requirements

- Attendance sheet: Fewer absences than allowed. Active participation in classes.
- Create a working application, solve a real problem, and present it as a video using the solutions and models learned in class.
     - It must be uploaded to Github and shared.
     - Maximum length of video is 5-10 minutes.
     - In the video, each creator must present their own contribution. (for 3-8 minutes)
     - The application must be shown in action at the end of the video. (for 1-2 minutes)
- Organizing into teams (2-4 people) or working individually.
- If the creator(s) uses a service based on a generative language model to complete the task, they must attach the prompt log to the completed project as additional material.
- It is not certain that the team members receive a uniform grade, but they get grades proportionate to the task they have completed in the project.
- **Submission deadline: 2025.05.31**
- [**Submission form**](https://forms.office.com/e/kLRhQ8ZZj6?origin=lprLink)



## Lecture

- I.    [Tokenization](../materials/NLP-A/lectures/lesson_2)
- I.    [Text representation I.](../materials/NLP-A/lectures/lesson_3)
- I.    [Text representation II.](../materials/NLP-A/lectures/lesson_4)
- II.   [Large language models I. fancy-rnn](https://robertlakatos.github.io/me/materials/NLP-A/lectures/fancy-rnn.pdf)
- II.   [Large language models I. CNN-TreeRNN](https://robertlakatos.github.io/me/materials/NLP-A/lectures/CNN-TreeRNN.pdf)
- III.  [Large language models II. Basic](https://robertlakatos.github.io/me/materials/NLP-A/lectures/rnnlm.pdf)
- IV.   [Large language models II. Transformer](https://robertlakatos.github.io/me/materials/NLP-A/lectures/transformers.pdf)
- V.    [Pretrain](https://robertlakatos.github.io/me/materials/NLP-A/lectures/pretraining-updated.pdf)
- V.    [Question Answering](https://robertlakatos.github.io/me/materials/NLP-A/lectures/QA.pdf)
- VI.   [Post-training](https://robertlakatos.github.io/me/materials/NLP-A/lectures/instruction-tuning-rlhf.pdf)
- VI.   [Promting RLHF](https://robertlakatos.github.io/me/materials/NLP-A/lectures/prompting-rlhf.pdf)
- VII.  [Life After DPO](https://robertlakatos.github.io/me/materials/NLP-A/lectures/life-after-dpo-lambert.pdf)
- VII.  [Training](https://robertlakatos.github.io/me/materials/NLP-A/lectures/training.pdf)
- VIII. [Efficient Adaptation](https://robertlakatos.github.io/me/materials/NLP-A/lectures/adaptation.pdf)
- IX.   [Hardware-aware Algorithms for Sequence Modeling](https://robertlakatos.github.io/me/materials/NLP-A/lectures/hardware-aware.pdf)
- X.    [Evaluation](https://robertlakatos.github.io/me/materials/NLP-A/lectures/evaluation.pdf)
- X.    [Natural Language to Code Generation](https://robertlakatos.github.io/me/materials/NLP-A/lectures/code-generation-pengcheng-yin.pdf)
- X.    [Security & Privacy of LLMs](https://robertlakatos.github.io/me/materials/NLP-A/lectures/security-and-privacy.pdf)
- XI.   [Human-Centered NLPF](https://robertlakatos.github.io/me/materials/NLP-A/lectures/human-centered-nlp.pdf)
- XI.   [Speech](https://robertlakatos.github.io/me/materials/NLP-A/lectures/speech-bci.pdf)
- XII.  [Agents](https://robertlakatos.github.io/me/materials/NLP-A/lectures/agents.pdf)
- XII.  [Linguistics Philosophy](https://robertlakatos.github.io/me/materials/NLP-A/lectures/linguistics-philosophy.pdf)
- XIII. [Open problems and discussion](https://robertlakatos.github.io/me/materials/NLP-A/lectures/open-problems.pdf)
- IVX.  [State of the art](https://robertlakatos.github.io/me/materials/NLP-A/lectures/state-of-the-art.pdf)

## Labor

- N.    [Python Basics](../materials/NLP-I/labor/N-python)
- N.    [Numpy and Matplotlib](../materials/NLP-I/labor/N-numpy-and-matplotlib)
- N.    [Pandas Intro](../materials/NLP-I/labor/N-pandas)
- I.    [Problem Identification and Discovery](../materials/NLP-A/labor/problem)
- II.   [Pipeline](../materials/NLP-A/labor/pipeline)
- III.  [Vector Store](../materials/NLP-A/labor/vector-store)
- IV.   [Filter and Cluster](../materials/NLP-A/labor/filter-cluster)
- V.    [Agent](../materials/NLP-A/labor/agent)
- VI.   [Tokenization](../materials/NLP-A/labor/tokenization)
- VII.  [Embedding](../materials/NLP-A/labor/embedding)
- VIII. [Recurent Neural Network](../materials/NLP-A/labor/recurent-neural-network)
- IX.   [BERT and LoRA](../materials/NLP-A/labor/BERT)
- X.    [Code Generation](../materials/NLP-A/labor/code-generation)
- XI.   [Recommmender System](../materials/NLP-A/labor/recommmender-system)
- XII.  [Transformer](../materials/NLP-A/labor/transformer)
- XII.  [Transformer](../materials/NLP-A/labor/transformer)
- XIII. [Total LABOR]


## Submitted

- [2024 Autumn](../materials/NLP-I/submitted/2024-2)
- [2023 Autumn](../materials/NLP-I/submitted/2023-2)

## Usefull Links

- [Huggingface](https://huggingface.co/)
- [LlamaIndex](https://docs.llamaindex.ai/en/stable/)
- [Keras](https://keras.io/)
- [Tensorflow](https://www.tensorflow.org/)
- [Pytorch](https://pytorch.org/)
- [Pyton](https://www.python.org/)
- [Google Colab](https://colab.google/)

## Recommended Literatures and Courses

1. [Jurafsky, Daniel, and James H. Martin. "Speech and language processing (draft)." Chapter A: Hidden Markov Models (Draft of September 11, 2018). Retrieved March 19 (2018): 2019.](https://ms.b-ok.xyz/book/3560643/4a6ab2)
2. [Eisenstein, Jacob. "Introduction to natural language processing." MIT press, 2019.](https://mitpress.mit.edu/9780262042840/introduction-to-natural-language-processing/)
3. [Goldberg, Yoav. "A primer on neural network models for natural language processing." Journal of Artificial Intelligence Research 57 (2016): 345-420.](https://arxiv.org/pdf/1510.00726.pdf)
4. [Francois Chollet. "Deep Learning with Python"](https://www.amazon.com/Deep-Learning-Python-Francois-Chollet/dp/1617294438)
5. [Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/chapter0/1?fw=pt)
6. [MIT Introduction to Deep Learning](http://introtodeeplearning.com/)
7. [Visual Guide to Transformer Neural Networks - (Episode 1)](⁠https://www.youtube.com/watch?v=dichIcUZfOw)
8. [Visual Guide to Transformer Neural Networks - (Episode 2)](⁠https://www.youtube.com/watch?v=mMa2PmYJlCo)
9. [Visual Guide to Transformer Neural Networks - (Episode 3)](⁠https://www.youtube.com/watch?v=gJ9kaJsE78k)

## Key Words

1. Tokenization 
    - Byte-Pair Encoding (BPE)
    - Byte-level BPE
    - WordLevel
    - WordPiece
    - Unigram
    - SentencePiece
2. Embbeding
    - Skip-Gram
    - CBOW
    - GLOVE
    - Word2Vec
3. Position Embedding
4. (Multi-Head) Attention
5. Neural Network (Feed Foward layer)
6. Normalization
7. Transoformer
8. Pre-Trained
9. Large Language Model
10. NLP Tasks
    - Summarization
    - Translate
    - Generation
    - Q&A
    - Named Entity Recognition
    - Sentiment analysis
11. Multimodal architectures
12. Huggingface
13. Keras
14. Tensorflow
15. Pytorch
16. Python
17. Pipline
18. Notebook
19. Google Colab

# Usefull Publications

[1] [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)

[2] [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)

[3] [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)

[4] [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)

[5] [Global Vectors for Node Representations](https://arxiv.org/pdf/1902.11004.pdf)